\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}

\include{paper}

%include paper.fmt
%format +|+ = "\mathbin{\ddagger}"
%format &+ = "\mathbin{\cup}"
%format &* = "\mathbin{\cap}"

%format _a = "\mathit{a}"
%format _b = "\mathit{b}"
%format _e = "\mathit{e}"
%format _s = "\mathit{s}"
%format _l' = "\mathit{l}^\prime{}"
%format _b' = "\mathit{b}^\prime{}"

%format let_ = "\mathsf{let}"
%format case_ = "\mathsf{case}"

%format Lambda a b = \a -> b
%format Fun a b = a \? b
%format Con a b = a \? b
%format App a b = a \? b
%format Var a = a
%format Case a b = case a of b
%format Let a b c = let a = b in c

\newcommand{\simp}[2]{\vspace{-7mm} #2 & (#1) \\}
\newenvironment{simplify}
    {\noindent
     \begin{flushright}
     \begin{tabular}{p{6.5cm}r}
    }
    {\end{tabular}
     \vspace{-7mm}
     \end{flushright}
    }

\begin{comment}
% for paper haskell
\begin{code}
f = undefined
\end{code}
\begin{code}
primNeqInt, primEqInt :: Int -> Int -> Bool
\end{code}
\end{comment}

\begin{document}

\conferenceinfo{ICFP '08}{date, City.} %
\copyrightyear{2008} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Loosing functions without gaining data}
\subtitle{ -- a new method for defunctionalisation}

\authorinfo{Neil Mitchell\titlenote{The first author is supported by an EPSRC PhD studentship}}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~ndm/}}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~colin/}}

\maketitle

\begin{abstract}
We describe an automated transformation which takes a higher-order program, and a produces an equivalent program with fewer functional values. Our method has a number of advantages over Reynold's style defunctionalization -- it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. While our method cannot always succeed in removing all functional values, in practice it is remarkably successful. We have evaluated our method on programs from the nofib benchmark, where only 3 out of 49 remain higher-order.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, transformation

\keywords
Haskell, first-order reduction, firstification, defunctionalization, functional programming

\section{Introduction}

Higher-order functions are well used and much loved within functional programming languages, such as Haskell \cite{haskell}. Their use is ubiquitous -- even a simple numeric literal in Haskell typically expands into the application of a function taking a tuple containing many functional values. This operation occurs as part of the dictionary transformation for the implementation of type classes \cite{wadler:type_classes}. While having functions as first-class values leads to more concise code, it often complicates analysis methods, such as checking pattern-match safety \cite{me:catch_icfp} and termination checking \cite{sereni:higher_order_termination}.

\begin{example}
\label{ex:incList}
\begin{code}
incList :: [Int] -> [Int]
incList = map (+1)

map :: (alpha -> beta) -> [alpha] -> [beta]
map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

Consider the above definition of |incList|, which contains many higher-order features. The function |(+1)| is passed as a functional argument to |map|. The |incList| value is a functional value, bound to a function taking a list to a list. The use of first-class functions has lead to short code, but we could equally have written:

\begin{code}
incList :: [Int] -> [Int]
incList []      = []
incList (x:xs)  = x+1 : incList xs
\end{code}

While the second variant of |incList| is much longer, it may also be more amenable to certain types of analysis -- and indeed may also perform faster at runtime. The transformation pass presented in this paper performs the above transformation automatically.
\end{example}

Our first-order reduction method processes the whole program to remove functional values, without changing the semantics of the program. This idea is not new, as far back as 1972 Reynold's gave a solution, known as Reynold's style defunctionalization \cite{reynolds:defunc}. Unfortunately, Reynold's method effectively introduces a mini-interpreter within the resultant program, which causes problems for analysis tools. Our method instead tries to produce a program closer to what a human may have written, if not allowed the benefit of functional values.

Our method has been implemented in Haskell, and operates over the Core language from the York Haskell Compiler \cite{me:yhc_core}. We have used our transformation within the Catch analysis tool, which checks for potential pattern-match errors in Haskell. In addition, we have made our defunctionalization method available as a library on Hackage\footnote{\url{http://hackage.haskell.org/cgi-bin/hackage-scripts/package/firstify}} -- and hope others from the termination checking community may be able to make use of it.

\subsection{Contributions}

Our paper makes a number of contributions:

\begin{itemize}
\item We define a method to remove most functional values from a program. Unlike previous work our method does not introduce new types, and can deal well with complexities such as type classes, continuations and monads.
\item Our method makes use of standard transformation steps, but the key is in their combination. In particular, we describe what restrictions are required, to ensure better defunctionalization, and to guarantee termination.
\item We have implemented our method, and present results for much of the nofib benchmark suite. These results enable us to measure the effectiveness of our method.
\end{itemize}

\subsection{Roadmap}

This paper begins with a definition of our Core language \S\ref{sec:core}, including what we consider to be a first-order program. Next we present an overview of our method \S\ref{sec:overview}, followed by a more detailed account \S\ref{sec:detailed}, along with a number of examples in \S\ref{sec:examples}. We classify where functional values may remain in a resultant program \S\ref{sec:completeness} and show how to modify our method to guarantee termination \S\ref{sec:termination}. Finally we give results \S\ref{sec:results}, review related work \S\ref{sec:related} and conclude \S\ref{sec:conclusion}.

\section{Our Core Language}
\label{sec:core}

\begin{figure}
\ignore\begin{code}
func = f vs_ expr

expr  =  Lambda vs_ x    -- lambda abstraction
      |  Fun f xs_       -- function application
      |  Con c xs_       -- constructor application
      |  App x xs_       -- general application
      |  Var v           -- variable
      |  Case x alts_    -- case expression
      |  Let v x y       -- let expression

alt = c vs_ -> x
\end{code}
\begin{comment}
% for paper haskell
\begin{code}
data Expr = Lambda [String] Expr
          | Fun String [Expr]
          | Con String [Expr]
          | App Expr [Expr]
          | Var String
          | Case Expr [Alt]
          | Let String Expr Expr

data Alt = Alt String [String] Expr

arity :: String -> Int
body :: String -> Expr
args :: String -> [String]
rhs :: Alt -> Expr
\end{code}
\end{comment}

We let |v| range over variables, |x| over expressions, |f| over function names and |c| over constructors.
\caption{Core Language.}
\label{fig:core}
\end{figure}

Our Core language is presented in Figure \ref{fig:core} -- and has a number of differences from a standard Core language. A function in our Core language consists of a name, a list of arguments, and a body expression. The arity of a function is the number of arguments associated with it.

The variable, case, let, application and lambda expressions are much as they would be in any Core language. The constructor expression consists of a constructor and a list of expressions, exactly matching the arity of the constructor -- any constructors not given enough arguments can be wrapped within a lambda expression. The function expression consists of a function and a list of argument expressions -- a number which may be less, equal to, or more than the arity of the function. If a function is given too few arguments, we refer to it as \textit{partially-applied}, matching the arity is \textit{fully-applied} and more than the arity is \textit{over-applied}. We use the meta functions |arity f|, |body f| and |args f| to denote the arguments, body and arity of |f|. We use the function |rhs| to extract the expression on the right of a case alternative.

A program can be said to be higher-order if at runtime the program generates and manipulates functional values. Functional values can only be created in two ways: (1) a lambda expression; (2) a partially-applied function application. We make the following definition:

\paragraph{Definition: First-Order} A program which contains no lambda expressions and no partially-applied functions is first-order.

In Example \ref{ex:incList}, our program is higher-order because of the under-saturation of both |map| and |(+)|. In the defunctionalized version, the program is first-order.

We do not permit primitives in our language, but explain how to extend our method to deal with them in \S\ref{sec:primitives}. We assume a root function, named |main|. We assume that if |main| takes any arguments, they are all first-order values, and that |main| evaluates to a first-order value. Again, we will explain how to loosen these restrictions in Example \ref{ex:root_function_functional}.


\section{Our First-Order Reduction Method}
\label{sec:overview}

Our method works by combining three separate and well-known transformations. Additionally we use general simplification rules, most of which may be found in any optimising compiler \cite{spj:transformation}.

\begin{description}
\item[Arity Raising:] A function is arity raised if the body of the function is a lambda expression. In this situation, the variables within the lambda expression can be added as arguments to the function.
\item[Inlining:] Inlining is a standard technique in optimising compilers \cite{spj:inlining}, and has been studied in depth.
\item[Specialisation:] Specialisation is another standard technique, used to remove type classes \cite{jones:dictionary_free} and more recently to specialise functions to a given constructor \cite{spj:specconstr}.
\end{description}

Each transformation has the possibility of removing some functional values, but the key contribution of this paper is \textit{how they can be used together}. Each transformation on its own is correctness preserving, and none introduce any additional data types. Each can also be implemented upon a typed Core language, although we have not done so.

We proceed by first giving a brief flavour of how each operation can be used to remove some functional values. We then discuss each step in detail in \S\ref{sec:detailed}, including how they can be combined. We leave the issue of termination until \S\ref{sec:termination}.

\subsection{Simplification}

The simplification step serves to group all the short and simple transformations that most optimising compilers apply. Some of these steps have the ability to remove functional values, while others simply ensure a normal form for future transformations.

\begin{example}
\begin{code}
one = (\x -> x) 1
\end{code}

The simplification rules transform this function to:

\begin{code}
one = let x = 1 in x
\end{code}
\end{example}

Other rules within the program do not eliminate lambda expressions, but manipulate them -- putting them in a form other stages can remove.

\begin{example}
\begin{code}
even =  let one = 1
        in \x -> not (odd x)
\end{code}

Here the let expression is surrounding the lambda expression. We can lift the lambda expression out of the let expression.

\begin{code}
even = \x ->  let one = 1
              in not (odd x)
\end{code}

In general this transformation may cause duplicate computation to be performed, an issue we return to in \S\ref{sec:sharing}.
\end{example}


\subsection{Arity Raising}

The arity raising transformation is an attempt to increase the arity of a function. It relies heavily on the simplification stage.

\begin{example}
\begin{code}
even = \x -> not (odd x)
\end{code}

Here the arity raising transformation changes the body of the function from being a lambda, and lifts the argument to the lambda into an argument of the function -- increasing the arity.

\begin{code}
even x = not (odd x)
\end{code}
\end{example}


\subsection{Inlining}

We use inlining to remove functions which return data constructors containing functional values.

\begin{example}
\begin{code}
f = case  eqInt of
          (a,b) -> a 1 2

eqInt = (primEqInt, primNeqInt)
\end{code}

In the above example the |eqInt| function evaluates to a tuple, where both |primEqInt| and |primNeqInt| are functional values. We can start to remove these functional values by inlining the function |eqInt|:

\begin{code}
f = case  (primEqInt, primNeqInt) of
          (a,b) -> a 1 2
\end{code}

We can now simplify the program into a first-order variant, using a rule dealing with a case scrutinising a known constructor. This rule is applied by the simplification stage.

\begin{code}
f = primEqInt 1 2
\end{code}
\end{example}

\subsection{Specialisation}

Specialisation works by replacing a function application with a specialised variant, where some of the arguments are passed at compile time.

\begin{example}
\begin{code}
notList xs = map not xs
\end{code}

Here the |map| function takes a functional value as its first argument. In the case of |notList| this functional value is |not|. We can create a variant of |map| specialised to |not| as its first argument:

\begin{code}
map_not x = case  x of
                  []    -> []
                  y:ys  -> not y : map_not ys

notList xs = map_not xs
\end{code}

The recursive call in |map| is replaced by a recursive call to the specialised variant. We have now eliminated all functional values.
\end{example}

\subsection{Goals}
\label{sec:goals}

Our goal is to combine the transformations we have  presented to remove as many functional values as possible. For example, the initial |incList| example requires simplification, arity raising and specialisation. We have defined a number of goals, in order of priority. Some of the earlier goals mean that later goals are unachievable -- in these cases we will merely do the best we can.

\paragraph{Correct and terminating.} Like all transformation methods, both correctness and termination are essential to make the transformation useful. By making use of three established transformations, correctness is relatively easy to show. The issue of termination is much harder -- both inlining and specialisation can be applied in ways that will diverge. In \S\ref{sec:termination} we develop a set of criteria to ensure termination.

\paragraph{Correspondence to the original program.} Since our transformation is designed to be performed before analysis, it is important that the results of the analysis can be presented in terms of the original program. We provide a method for transforming expressions in the resultant program into equivalent expressions in the original program.

\paragraph{Introduce no data types.} Reynold's method introduces a new data type that serves as a representation of functions, then embeds an interpreter for this data type into the program. A natural desire would be to eliminate the higher-order aspects of a program, \textit{without} introducing any new data types. By composing our transformation out of existing transformations, none of which introduce data types, we can easily ensure that our resultant transformation does not introduce data types. Unfortunately, such a transformation is not possible.

Given a program, we can remove all data types by encoding them as functions, as described in \citet{naylor:reduceron}. If we then had a transformation which made the program first-order \textit{without} introducing any data types, we would end up with a program without data or closures, which is incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

We cannot both remove all functional values and meet this goal. We choose to not introduce any data types, and accept that some functional values may remain in the resultant programs. If a totally first-order program is required, Reynold's method can be always be applied after our transformation.

\paragraph{Remove all functional values.} We have already indicated that the removal of all functional values is an impossible goal within our previous constraints. However, we will attempt to remove as many functional values as possible. In \S\ref{sec:completeness} we classify where functional values may appear in the resultant programs.

\paragraph{Preserve the type information.} Given an original program with explicit types, we desire that the resultant program can still be assigned a type. Similar passes to our stages are implemented in a type preserving manner inside the GHC compiler \cite{ghc}, so we believe this task should be relatively easy. We note that Reynold's method is not so amenable to type preservation.

\paragraph{Preserve the space/sharing behaviour of the program.} In the expression |let y = f x in y + y|, according to the rules of lazy evaluation, |f x| will be evaluated at most once. We could inline the let binding to give |f x + f x|, but this expression evaluates |f x| twice. Where possible, we will avoid changing the sharing of the program. Our goals are primarily for analysis of the resultant code, not to compile and execute the result. Because we are not interested in performance, we permit the loss of sharing in computations, if to do so will remove functional values.

\paragraph{Other Desirable Qualities.} Previous defunctionalization attempts have shown a concern about code size increase \cite{chin:higher_order_removal}. A smaller resultant program would be desirable, but not at the cost of clarity. The speed of the implementation is not of much importance to us, provided it performs in a respectable time to permit proper evaluation.


\section{Method in Detail}
\label{sec:detailed}

\begin{comment}
% paper haskell
\begin{code}
data Prog = Prog deriving Eq
simplify,arity,inline,specialise :: Prog -> Prog
\end{code}
\end{comment}

Our method proceeds in four steps, simplification (|simplify|), arity raising (|arity|), inlining (|inline|) and specialisation (|specialise|). These steps are combined using a fixed point operator.

For the implementation, we have implemented our steps within a monadic framework to deal with issues such as obtaining unique free variables and tracking termination constraints. To simplify the presentation, we have ignored these issues -- while these are tedious engineering concerns, they do not effect the underlying algorithm.

Our method is written as:

\begin{code}
firstify = simplify +|+ arity +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operator, defined as:

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g = fix (g . fix f)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x' then x else fix f x'
    where x' = f x
\end{code}

The expression |f +||+ g| applies the |f| to an input until it reaches a fixed point, then applies |g|. If the |g| changes the value, then |f| is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Idempotent in each function] After the operation has completed, applying either |f| or |g| will not change the value.

\begin{code}
propIdempotent f g x = let r = (+|+) f g x in f r == r && g r == r
\end{code}

\item[Idempotent] The operation as a whole is idempotent.

\begin{code}
propIdemp2 f g x = let r = (+|+) f g x in r == (+|+) f g r
\end{code}

\item[Function choice] If both |f| and |g| could be applied, |f| will be chosen.

\item[Function ordering] The function |f| will have reached a fixed point before the function |g| is applied. If a postcondition of |f| implies a precondition of |g|, then we can guarantee |g|'s precondition will always be met.
\end{description}

These properties allow us to combine the individual steps with less regard to their interaction. The function ordering allows us to overlap the application sites of two stages, but prefer one stage over another. The first two properties ensure that after the method terminates no further transformation will be possible.

The operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((simplify +|+ arity) +|+ inline) +|+ specialise
\end{code}

Within this chain we guarantee that the end result will be idempotent with respect to any of the functions, and before any function is invoked, all those to the left of it will be idempotent.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent on its own, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent in all but the first case, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited.

We describe each of the stages in the algorithm separately. In all subsequent stages, we assume that all the simplification rules have been applied.


\subsection{Simplification}

\begin{figure}
\begin{simplify}

\simp{app-app}{
\ignore\begin{code}
(x xs_) ys_
    => x xs_ ys_
\end{code}}

\simp{fun-app}{
\ignore\begin{code}
(f xs_) ys_
    => f xs_ ys_
\end{code}}

\simp{lam-app}{
\ignore\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\simp{let-app}{
\ignore\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\simp{case-app}{
\ignore\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\simp{case-con}{
\ignore\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\simp{case-let}{
\ignore\begin{code}
case (let v = x in y) of alts_
    => let v = x in (case y of alts_)
\end{code}}

\simp{case-case}{
\ignore\begin{code}
case (case x of {... ; c vs_ -> y ; ...) of alts_
    => case x of {... ; c vs_ -> case y of alts_ ; ...}
\end{code}}

\simp{case-lam}{
\ignore\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \z -> case x of {... ; c vs_ -> (\v -> y) z ; ...} z
\end{code}}

\simp{eta}{
\ignore\begin{code}
f xs_
    => \v -> f xs_ v
    where arity f > length xs_
\end{code}}

\end{simplify}
\caption{Standard Core simplification rules.}
\label{fig:simplify}
\end{figure}


\begin{figure}
\begin{simplify}

\simp{bind-lam}{
\ignore\begin{code}
let v = (\w -> x) in y
    => y[v / \w -> x]
\end{code}}

\simp{bind-box}{
\ignore\begin{code}
let v = x in y
    => y[v / x]
    where x {-" \text{ is a boxed lambda (see \S\ref{sec:inlining})} "-}
\end{code}}

\simp{let-lam}{
\ignore\begin{code}
let v = x in \w -> y
    => \w -> let v = x in y
\end{code}}

\end{simplify}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The simplification stage has the goal of moving lambda expressions upwards, and introducing lambdas for partially applied functions. This stage makes use of standard simplification rules given in Figure \ref{fig:simplify}, which are found in most simplifiers, such as \cite{spj:transformation}. We also make use of additional rules which deal specifically with lambda expressions, given in Figure \ref{fig:lambda_simplify}. All of the simplification rules are correct individually. The rules are applied at any subexpression, as long as any rule matches.

\subsubsection{Lambda Introduction}

The (eta) rule inserts lambdas where possible, using $\eta$-expansion. The input program from the Yhc compiler has been lambda lifted \cite{lambda_lift}, replacing lambda expressions with partial application. This rule removes all occurrences of partial-application, replacing them with explicit lambda expressions. The second stage moves these lambda expressions around, attempting to lift them to the top level.

\begin{example}
\begin{code}
even = (.) not odd
\end{code}

Here the functions |(.)|, |not| and |odd| are all partially applied. Lambda expressions can be inserted to saturate these applications.

\begin{code}
even = \x -> (.) (\y -> not y) (\z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is applied to a number of arguments equal to its arity. This transformation will enable the arity raising transformation, resulting in:

\begin{code}
even x = (.) (\y -> not y) (\z -> odd z) x
\end{code}
\end{example}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. This step trades one form of functional value for another form, but has the advantage of making functional values more explicit, and permitting arity raising.

\subsubsection{Lambda Movement}
\label{sec:sharing}

The rule (case-lam) lifts a lambda out from within a case alternative to outside the case value. The (bind-lam) rule inlines a lambda bound in a let expression. The (bind-box) rule will be discussed as part of the inlining stage, see \S\label{sec:inlining}. The (let-lam) rule can be responsible for a reduction in sharing:

\begin{example}
\begin{code}
f x = let i = expensive x
      in \j -> i + j

main xs = map (f 1) xs
\end{code}

In the above example, |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x j = let i = expensive x
        in i + j
\end{code}

Now |expensive| will be recomputed for every element in |xs| -- potentially a very severe speed penalty.
\end{example}

The loss of sharing is not purely theoretical. To take one example, the Uniplate library \cite{me:uniplate} makes use of a let within a lambda to keep a scoreboard which is computed once. The (let-lam) rule would make the scoreboard mechanism a severe performance penalty. As we focus on functional value removal at the expense of sharing, we perform these rules when necessary.


\subsection{Arity Raising}

The arity raising step is simple:

\ignore\begin{code}
function vs_ = \v -> x
    => function vs_ v = x
\end{code}

Given a body which is a lambda expression, the arguments to the lambda expression can be lifted into the arguments for the function. This transformation requires the body to be a lambda expression, so additional transformations are necessary to introduce and move lambda expressions. If a function has its arity increased, it is likely that this will cause previously fully-applied uses to become partially-applied, causing the (eta) simplification rule to fire.


\subsection{Inlining}
\label{sec:inlining}

We use inlining to remove functional values which are stored within a data value -- for example |Just (\x -> x)|. We refer to functional values inside data values as \textit{boxed lambdas}, because they are contained within a data value. If a boxed lambda is bound in a let expression, we substitute the let binding, using the (bind-box) rule from Figure \ref{fig:lambda_simplify}. We only inline a function if two conditions both hold: (1) the functions body is a boxed lambda; (2) the function application occurs within a case scrutinee.

\begin{figure}
\begin{code}
isBox (Con c xs_     )  =  any isLambda xs_ ||  any isBox xs_
isBox (Let v x y     )  =  isBox y
isBox (Case x alts_  )  =  any (isBox . rhs) alts_
isBox (Fun f xs_     )  =  isBox (body f)
isBox _                 =  False

isLambda (Lambda vs_ x)  = True
isLambda _               = False
\end{code}
\caption{The |isBox| function, to test if an expression is a boxed lambda.}
\label{fig:boxed_lambda}
\end{figure}

We define an expression to be a boxed lambda if the |isBox| check in Figure \label{fig:boxed_lambda} returns true. The |isBox| as presented may not terminate, but by simply keeping a list of followed functions, we can assume the result is false in any duplicate call. This modification does not decrease the number of true results.

\begin{example}
The following expressions are boxed lambdas:

\ignore\begin{code}
[\x -> x]
(Just [\x -> x])
(let y = 1 in [\x -> x])
[Nothing, Just (\x -> x)]
\end{code}

The following examples are not boxed lambdas:

\ignore\begin{code}
\x -> id x
[id (\x -> x)]
id [\x -> x]
\end{code}

In the first negative example there is no constructor, so the first condition does not apply. In the second there is a constructor, but the lambda expression is not an argument to the constructor. In the third there is a lambda expression as an argument to a constructor, but that subexpression is an argument to a function application.

Note that the function final expression evaluates to a boxed lambda, but that this information is hidden by the |id| function. We do not analyse within the |id| function, but assume that specialisation will deal with any boxed lambdas passed to functions.
\end{example}

Our inline rule can now be specified as:

\begin{simplify}

\simp{case-boxed}{
\ignore\begin{code}
case (f xs_) alts_
    => case (let args f = xs_ in body f) alts_
    where isBox (body f)
\end{code}}

\end{simplify}

The |inline| rule deals with situations where functions evaluate to a data structure containing functional values. This situation occurs regularly with the standard dictionary implementation, but only occasionally in other situations. The inline rule does not actually remove functional values, but can bring their use and creation closer together, and thus helps them be removed.

As with the simplification stage, there may be some loss of sharing if the function being inlined has an arity of 0, known as constant applicative form (CAF). A Haskell implementation will compute these functions once, and reuse their value as necessary. If these functions are inlined, this sharing will be lost.

\subsection{Specialisation (|specialise|)}

The original Catch tool \cite{me:catch_tfp} uses specialisation to remove higher-order functions. For each application of a function to functional arguments, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applied where function arguments are lambda expressions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have functional arguments, and generate templates, omitting first-order components.
\item For each template, generate an associated function, specialised to the template.
\item For each subexpression matching a template, replace it with the generated function.
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The specialisation first finds the application of |map| within |main|, and generates the template \ignore|map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates the name |map_id| for the template, and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code has no functional values within it.
\end{example}

The specialisation stage is the only one which introduces new function names. In order to recover an expression in the input language, it is sufficient to replace all generated functions with their associated template, supplying all necessary variables.

\subsubsection{Generating Templates}

A template is generated if an expression is an application to a top-level function, whose arguments include a sub-expression which is either a lambda expression or a boxed lambda. The template includes all sub-expressions whose removal would lead to higher-order elements, or which have free variables from a bound variable.

\begin{example}
\ignore\begin{code}
id (\x -> x)             => id (\x -> x)
id (Just (\x -> x))      => id (Just(\x -> x))
id (Just (\x -> x + 3))  => id (Just (\x -> x + _))
id (Just (\x -> x + y))  => id (Just (\x -> x + _))
\end{code}

In all examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, the |3| and |y| are not dependent on variables bound within the lambda, and are left as unspecified.  The |Just| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. Each |_| within the template is assigned a free variable, as an argument to the new function, then the body is produced by unfolding the outer function symbol in the template once.

\begin{example}
\label{ex:map_id}
Following the \ignore|map (\x -> x) _| template from above, we can generate |v_1| as the unique free variable for the single |_| placeholder, and |map_id| as the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

In the next step, we unfold the definition of map once:

\begin{code}
map_id v_1 = let  f   = \x -> x
                  xs  = v_1
             in   case  xs of
                        []    -> []
                        y:ys  -> f y : map f ys
\end{code}

Now the generation of the specialised variant is complete. To give an idea of how the final function is calculated, after the simplification rules from Figure \ref{fig:lambda_simplify}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{example}

\subsubsection{Using Templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an undecided element is passed as an argument. Continuing with the generated code from Example \ref{ex:map_id}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We have now eliminated all the functional values from within this operation.

\subsection{Extending the method to allow primitive functions}
\label{sec:primitives}

Primitive functions are like normal functions, but do not have an associated body, and therefore cannot be examined or inlined. We can make two simple changes to support primitives. Firstly, we define that a primitive application is not a boxed lambda. Secondly, we restrict specialisation so that if a function to be specialised is actually a primitive, no template is generated. The reason for this restriction is that the generation of code associated with a template requires a one-step unfolding of the function, something which cannot occur with a primitive.

\section{Examples}
\label{sec:examples}

We now give a series of examples. We have particularly focused on where our method leaves residual functional values, or where a small alteration to the method would harm the effectiveness.

\begin{examplename}{Dead Code}
\begin{code}
nothing = Nothing

main = case  nothing of
             Nothing  -> 1
             Just f   -> f (\x -> x)
\end{code}

In |main| the |Nothing| branch will always be taken. The |Just| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. Our method cannot remove this lambda.
\end{examplename}

\begin{examplename}{Undefined Values}
\label{ex:undefined_values}
\begin{code}
bottom = bottom

main = bottom (\y -> y)
\end{code}

In |main| a lambda is applied to |undefined|, so the program will non-terminate before the lambda is evaluated. The |bottom| function could also have been implemented as a call to the |error| primitive, with similar results.
\end{examplename}

\begin{examplename}{Root function returning functional values}
\label{ex:root_function_functional}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor, i.e. a boxed lambda. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from 0 to 1. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type \ignore|IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> One alpha)
data One alpha = One alpha
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to \ignore|One alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{examplename}

\begin{examplename}{Root function taking functional values}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are transforming, we cannot remove the functional value.
\end{examplename}

\begin{examplename}{Primitives}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type \ignore|alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Some primitives not only permit functional values, but actually \textit{require} them. The |primCatch| function within the Yhc standard libraries implements the Haskell exception handling function |catch|. The type of |primCatch| is \ignore|alpha -> (IOError -> alpha) -> alpha|, taking an exception handler as one of the arguments.
\end{examplename}

\begin{examplename}{Functional Lists}
\label{ex:functional_lists}
Sometimes lambda expressions are used to build up lists which can have elements concatenated onto the end. Using Hughes lists \cite{hughes:lists}, we can define:

\begin{code}
nil = id
snoc x xs = \ys -> xs (x:ys)
list xs = xs []
\end{code}

This list representation provides |nil| similarly to |[]| in standard lists, but instead of providing a |(:)| or ``cons'' operation, it provides |snoc| which adds a single element on to the end of the list. The function |list| is provided to create a standard list. We are unable to defunctionalize such a construction, as it stores unbounded information within closures. We have seen such constructions in both the |lines| function of the HsColour program, and the |sort| function of Yhc.

However, there is an alternative implementation of these functions:

\begin{code}
nil = []
snoc = (:)
list = reverse
\end{code}

We have benchmarked these operations in a variety of settings and the list based version appears to use approximately 75\% of the memory, and 65\% of the time. We suggest that people using continuations for |snoc| move instead to a list type.
\end{examplename}


\begin{examplename}{Inlining Boxed Lambdas}
\label{ex:inlining_boxed_lambdas}
Our original program inlined boxed lambdas everywhere they occurred. This makes the detection of boxed lambdas much simpler, and does not require looking into external functions. However, it is unable to cope with certain programs:

\begin{code}
main xs = app (gen xs)
app = map ($ 1)
gen = map (const x)
\end{code}

The function |gen| returns a boxed lambda. However, the code is recursive, so if it was inlined repeatedly, it would not terminate. After deciding to restrict the inlining of gen, we are still left with lambdas. However, by first specialising |app| with respect to |gen|, we are able to remove the functional values. The removal of the lambda expressions has effectively lead to a deforesting between the |gen| producer of |(:)| nodes, and the |app| consumer -- eliminating the intermediate functional values in the same step.
\end{examplename}



\section{Classification of Completeness}
\label{sec:completeness}

Our method would be complete if it removed all lambda expressions from a program. Completeness is not obtainable given the termination restriction (see \S\ref{sec:goals}). We have two causes of incompleteness: those that come from the fundamental algorithm, and those that are added by the termination criteria in \S\ref{sec:termination}. Here we explore causes arising from our algorithm. We wish to classify where a lambda expression may reside in a program after the application of our firstification method.

To examine where lambda expressions may occur, we model our Core expression language as a context free grammar. We define the start symbol |_s|, and require that the bodies of all functions in our language be defined by the grammar. First we map all expressions onto symbols in our grammar:

\ignore\begin{code}
\vs_ -> x         => lam x
f xs_             => fun (body f) xs_
c xs_             => con xs_
x ys_             => app x ys_
v                 => var
case x of alts_   => case_ x (map rhs alts_)
let v = x in y    => let_ x y
\end{code}

We have abstracted away variable names and patterns in expressions such as case, let and lambda. Some expressions, such as |app|, have multiple arguments. We have abstracted this away, and use \ignore|(app _a _b)| to denote that the first argument to an application is define by |_a|, and that all subsequent arguments are defined by |_b|.

In the case of function application, the first argument represents the possible expressions of the body of the function, and the second expression represents the arguments applied to the function. We compose grammars using \ignore|&+|, \ignore|&*| and \ignore|-| as grammar subtraction.

First we start out with the complete grammar:

% _ = L f C a v c l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app _s _s &+ var &+
       case_ _s _s &+ let_ _s _s
\end{code}

We know our original program is type safe. This means that the scrutinee of a case may not evaluate to a functional value, and therefore we can discard the |lam| production from the first argument of |case_|. Likewise, we know that all constructor expressions are saturated, so will evaluate to a data value, and cannot be the first argument of an application. Modifying our grammar to take account of this, we have:

% _ = L f C a(!C, _) v c(!L, _) l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app (_s - con _s) _s &+ var &+
       case_ (_s - lam _s) _s &+ let_ _s _s
\end{code}

Our simplification rules given in Figure \ref{fig:simplify} are applied until a fixed point is found, meaning that the left hand side of any rule cannot occur in the output. Accounting for these restrictions in the grammar we are left with:

% _ = L f C a(v, _) v c(f a v, _) l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
       case_ (fun _s _s &+ app var _s &+ var) _s &+ let_ _s _s
\end{code}

Next we apply the lambda rules from Figure \ref{fig:lambda_simplify}. As \ignore|(_s - lam _s)| is a common production we have factored it out as |_l'|.

% _ = L f C a(v, _) v c(f a v, !L) l(!L, !L)
\ignore\begin{code}
_l'  =  _s - lam _s
_s   =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
        case_(fun _s _s &+ app var _s &+ var) _l' &+ let_ _l' _l'
\end{code}

We can now use the results of arity raising, which ensures that every function's root may not be a lambda expression.

% _ = L f(!L, _) C a(v, _) v c(f a v, !L) l(!L, !L)
\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun _l' _e &+ app var _e &+ var) _l' &+ let_ _l' _l'
\end{code}

We now wish to work with lambda boxes, as defined by the function |isBox|, from Figure \ref{fig:boxed_lambda}. Within our grammar rules it is not possible to define that an expression is a lambda box. The first expression defined as a lambda box is a constructor where one of the arguments is a lambda, and the other arguments are unrestricted. Using our grammar, we have merged all the arguments to a constructor, and cannot define the property that a term is a lambda box in our grammar. We can however define the property that something is \textit{not} a lambda box, which we define with the production:

%
\ignore\begin{code}
_b'  =  lam _e &+ fun _b' _e &+ con (_b' - lam _e) &+ app _e _e &+ var &+
        case_ _e _b' &+ let_ _e _b'
\end{code}

Now if an expression meets |_e|, but not |_b'|, then we know it is a lambda box. We can now make use of the inlining rule and the (bind-box) rules to write:

\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') _e &+ app var _e &+ var) _l' &+ let_ (_l' &* _b') _l'
\end{code}

Now we can make use of the specialise rule, which removes all lambdas and boxed lambdas from the arguments of function applications:

\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' (_l' &* _b') &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') (_l' &* _b') &+ app var _e &+ var) _l' &+
      let_ (_l' &* _b') _l'
\end{code}

Having applied all the rules, we now classify what the parent expressions of a lambda may be. A lambda may not be the root of a function body, because |lam| is not a production in |_s|. If we restrict our attention only to expressions which permit a lambda expression as a direct child, and add explicit \ignore|lam _e| productions where we can, we are left with:

\ignore\begin{code}
_e =  lam (lam _e &+ _e) &+ con (lam _e &+ _e) &+ app var (lam _e &+ _e) &+
      -- productions not permitting a lambda as a direct child
\end{code}

A lambda may therefore occur as the child of a lambda expression, as an argument to a constructor, or as an argument to an application. However, a constructor containing a lambda is a lambda box, and therefore is not permitted anywhere |_b'| is intersected with the expression. We can now classify where a lambda box may occur, denoting all expressions as either |_b| to denote that a box may occur, or |_b'| to denote that it may not:

\ignore\begin{code}
lam _b &+ fun _b _b' &+ con _b &+ app _b' _b &+ case_ _b' _b &+ let _b' _b
\end{code}

Therefore a constructor containing a lambda may only occur at the root of a function (since |_s| permits boxed lambdas), or in one of the positions outlined above. Assuming no unreachable functions, either the root function of the program must evaluate to a boxed lambda, or there must be an expression which is not an unboxed lambda containing a boxed lambda. Of the above productions, most are boxed lambdas by definition, so the only non-boxed lambdas containing a boxed lambda are:

\ignore\begin{code}
lam _b &+ app _b' _b
\end{code}

Therefore the root function may be a boxed lambda, with a lambda inside the box. Or a lambda or boxed lambda may occur as inside a lambda, or as the second argument to an application. Given this situation, we can be sure that the variable does not bind to a lambda, as the lambda cannot escape. This leads us to the following situations:

\ignore\begin{code}
case v of
    Just w -> w (\x -> x)
    Nothing -> 1

bottom (\x -> x)
    where bottom = bottom
\end{code}

In the first case there must be no corresponding |Just| value containing a functional value, so the first branch must be dead code. In the second example, a non-terminating function the first argument of an application, and contains a lambda as an argument to it. Provided the |main| function is first-order, and the termination criteria have not applied, if a program has any residual lambda expressions, there must be at least one in one of these two situations.


\section{Proof of Termination}
\label{sec:termination}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.

\subsection{Termination of Simplification}
\label{sec:termination_simplification}

In order to check the termination of the simplifier we have used the AProVE system \cite{aprove}, to model our rules as term rewrites, and check termination thereon. A simple encoding of our rules is given in Appendix A, which checks basic termination. We have proven termination using both this simple formulation, which considers all constructors to have one alternative of exactly arity one, and a more complex encoding. In both cases, the system is able to report success.

The encoding of the (bind-box) and (bind-lam) rules is excluded. Given these rules, there are non terminating sequences. For example:

\ignore\begin{code}
(\x -> x x) (\x -> x x)
   => -- (lam-app) rule
let x = \x -> x x in x x
   => -- (bind-lam) rule
(\x -> x x) (\x -> x x)
\end{code}

These type of expressions are a problem for GHC, and can cause the compiler to non-terminate if encoded as data structures \cite{spj:inlining}. Other transformation systems \cite{chin:higher_order_removal} are able to make use of type annotations to ensure these reductions terminate. In practice the above situation does not occur, but in order to guarantee termination, we only perform $n$ (bind-lam) or (bind-box) rules upon an expression. If the expression is altered by other stages, we reset the count. Currently we have set $n$ to 1000, and have never had this limit reached.

\subsection{Termination of Arity Raising}

\todo{What makes arity raising terminate. Does it terminate?}

\subsection{Termination of Inlining}

One standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions, such as Example \ref{ex:inlining_boxed_lambdas}. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = case  f of
          One _ -> One (\x -> x)
\end{code}

The |f| inside the case is a candidate for inlining:

\ignore\begin{code}
case f of One _ -> One (\x -> x)
    => -- inlining rule
case (case f of One _ -> One (\x -> x)) of One _ -> One (\x -> x)
    => -- (case-case) rule
case f of One _ -> case One (\x -> x) of One _ -> One (\x -> x)
    => -- (case-con) rule
case f of One _ -> One (\x -> x)
\end{code}

This expression could cause non-termination.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. Any future attempts to inline |f| within this function would be disallowed, although |f| could still be inlined within other function bodies. This termination criteria is sufficient, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}
\label{sec:termination_specialisation}

The specialisation method, left unrestricted, does not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. Specialisation would generate an infinite number of specialisations of |f|.
\end{example}

\begin{figure}
\[\frac{s \unlhd t_i \text{ for some } |i|}{s \unlhd \sigma(t_1,\ldots,t_n)} \]

\[\frac{\sigma_1 \sim \sigma_2,
        s_1 \unlhd t_1, \ldots , s_n \unlhd t_n}
       {\sigma_1 (s_1,\ldots,s_n) \unlhd \sigma_2 (t_1,\ldots,t_n)} \]
\caption{Homeomorphic embedding relation.}
\label{fig:homeomorphic}
\end{figure}

To ensure we only specialise a finite number of times we use a homeomorphic embedding \cite{leuschel:homeomorphic}, given in Figure \ref{fig:homeomorphic}. The homeomorphic embedding has been used for termination of Supercompilation \cite{sorensen:supercompilation}, which makes use of some similar transformations. The homeomorphic embedding $\unlhd$ is a well-binary relation, meaning there are no infinite admissible sequences. A sequence $s_1,s_2 \ldots$ is admissible if there are no $i < j$ such that $s_i \unlhd s_j$. This property only holds provided all elements are expressions over a finite alphabet.

For each function, we associate a set $S$, of expressions. After generating a template $t$, we only specialise with that template if $\forall s \in S \bullet \neg(s \unlhd t)$. If $S$ is already admissible, then the sequence $S$ with $t$ added at the end is still admissible. After specialising with a template we add that template to the set $S$ associated with that expression. When we create a new function based on a template, we copy the $S$ associated with the function in which the specialisation is performed.

One of the conditions for termination of homeomorphic embedding is that there is only a finite alphabet. To ensure this condition, we consider all variables and literals to be equal. However, this is not sufficient. During the process of specialisation we create new functions, and these new functions are new symbols in our language. Because the homeomorphic embedding is being used to ensure that infinite functions are not created, we cannot assume this property when proving it. Instead we only use function names from the original input program. Every template has a correspondence with an expression in the original program. We perform the homeomorphic embedding after transforming all templates into their original equivalent expression.

Using homeomorphic embedding on the previous example, we would generate the specialised variant of |f (Value head)|. Upon attempting to generate the specialised variant |f (Wrap (Value head))| we would abort, with an embedding.

While homeomorphic embedding is sufficient to obtain defunctionalization in most simple examples, there do exist examples where it terminates prematurely.

\begin{example}
\begin{code}
main y = f (\x -> x) y
f x y = fst (x, f x y) y
\end{code}

Here we first generate a specialised variant of |f (\x -> x) y|.  If we call the specialised variant |f'|, we have:

\begin{code}
f' y = fst (\x -> x, f' y) y
\end{code}

Note that the recursive call to |f| has also been specialised. We now attempt to generate a specialised variant of |fst|, using the template |fst (\x -> x, f' y) y|. Unfortunately, this template is an embedding of the template we used for |f'|, and we do not specialise. If we permit specialisation, we reduce to:

\begin{code}
f' y = fst' y y
fst' y_1 y_2 = y_2
\end{code}

Note that the homeomorphic embedding has prevented us for making a first-order variant of the program.
\end{example}

While the above example may look slightly obscure, it occurs commonly with the standard translation of dictionaries. Often, classes have default methods, which call other methods in the same class. These recursive class calls often pass dictionaries, embedding the original caller -- even though no recursion actually happens.

To alleviate this problem, instead of storing one set $S$, we instead store a sequence of sets, $S_1$ through $S_n$ -- where $n$ is a small positive number, constant for the duration of the program. Instead of adding to the set $S$, we now add to the lowest set $S_i$ where adding the element will not violate the admissible sequence. Each of the sets $S_i$ is still finite, and there are a finite number ($n$) of them, so termination is maintained.

By default the firstification program uses 8 as the number of sets. In the results table given in \S\ref{sec:results}, we have given the minimum possible value of $n$ to remove all lambda expressions within each program. The disadvantage of a higher $n$ is that a program will unroll infinite recursion more times.

\subsection{Termination as a Whole}

We call a function |f| finite if it only applies a finite number of times, given a starting program. We call a function |f| terminating if |fix f| terminates. Given |f +||+ g|, if both |f| and |g| are finite then |f +||+ g| is finite. If one if finite, and the other is terminating, then the entire production is terminating.

We have shown that arity raising, inlining and specialisation are finite. The remaining step, simplification, is terminating. Therefore our entire program is terminating.

\section{Results}
\label{sec:results}

\begin{table}
\caption{Results of defunctionalization on the nofib suite.}
\label{tab:results}

\smallskip

\textbf{Name} is the name of the program;
\textbf{Bound} is the numeric bound use for the termination (see \S\ref{sec:termination_specialisation});
\textbf{HO Create} the number of lambda expressions and under-applied functions, first in the input program and then in the output program;
\textbf{HO Use} the number of application expressions and over-applied functions;
\textbf{Size} the change in the program size.

\smallskip\smallskip

\begin{tabular*}{\linewidth}{lrrrrrrlll}
\textbf{Name} & \textbf{Bound} & \multicolumn{2}{c}{\textbf{HO Create}} & \multicolumn{2}{c}{\textbf{HO Use}} & \textbf{Size} \\
\vspace{-1ex} \\
bernouilli      & 4 & 240 & 0 & 190 & 2 & -32\% \\
digits-of-e1    & 4 & 217 & 0 & 153 & 2 & -35\% \\
digits-of-e2    & 5 & 236 & 0 & 198 & 3 & -32\% \\
exp3\_8         & 4 & 232 & 0 & 154 & 2 & -39\% \\
gen\_regexps    & 7 & 116 & 0 &  69 & 0 & -31\% \\
integrate       & 4 & 348 & 0 & 358 & 2 & -38\% \\
paraffins       & 4 & 360 & 0 & 351 & 2 & -53\% \\
primes          & 4 & 217 & 0 & 148 & 2 & -38\% \\
queens          & 4 & 217 & 0 & 146 & 2 & -38\% \\
rfib            & 4 & 338 & 0 & 355 & 2 & -31\% \\
tak             & 4 & 212 & 0 & 145 & 4 & -40\% \\
wheel-sieve1    & 4 & 224 & 0 & 151 & 2 & -35\% \\
wheel-sieve2    & 4 & 224 & 0 & 162 & 2 & -36\% \\
x2n1            & 4 & 345 & 0 & 385 & 2 & -57\% \\
\multicolumn{8}{l} \ldots{}plus 35 tests from the spectral suite\ldots{} \\
Minimum         & 2 & 60 & 0 & 46 & 0 & -78\% \\
Maximum         & 14 & 437 & 1 & 449 & 100 & 15\% \\
Average         & 4.8 & 237 & 0.06 & 202 & 3.6 & -33\% \\
\hline
\end{tabular*}
\end{table}

We have tested out method with programs drawn from the nofib benchmark \cite{nofib}, and the results are given in Table \ref{tab:results}. We have tested with all 14 tests from the imaginary section (each shown in the table), and 35 out of the 47 tests in the spectral section. The remaining 12 tests in the spectral section do not compile using the Yhc compiler, mainly due to missing or incomplete libraries.

Most analysis operations work on an abstracted view of the program. For example, |readFile| in Yhc is implemented in terms of file handles and pointer operations -- while an analysis tool would probably give an abstraction for the |readFile| function. For the purposes of defunctionalization, we have worked on unmodified Yhc libraries, including all the low-level detail. Higher-order functions occur in many places, including:

\begin{itemize}
\item Type classes create dictionaries which are implemented as tuples of functions.
\item The monadic bind operation is higher-order.
\item The IO data type is implemented as a function.
\item The Haskell |Show| type class uses continuation passing style extensively.
\item List comprehensions in Yhc are desugared to continuations. There are other translations which require less functional value manipulations \cite{wadler:list_comprehensions,coutts:stream_fusion}.
\end{itemize}

One torture test we have used during testing is \ignore|show (1 :: Double)|. The underlying implementation of |show| on |Double| makes use of arrays, which are implemented within the |IO| monad. Out of the tests, only 3 programs have residual functional values within them.

The integer program passes functional values to the primitive |seq|, similar to Example \ref{ex:functional_lists}, using the following function:

\begin{code}
seqlist [] = return ()
seqlist (x:xs) = x `seq` seqlist xs
\end{code}

This function is invoked with the IO monad, so the |return ()| expression is a functional value. It is impossible to remove this functional value without having access to the implementation of the |seq| primitive.

The pretty and constraints programs both apply a functional value to an expression that evaluates to |undefined|, similar to Example \ref{ex:undefined_values}. The case in pretty comes from the fragment:

\begin{comment}
% for paper haskell
\begin{code}
data PrettyRep = PrettyRep
\end{code}
\end{comment}
\begin{code}
type Pretty = Int -> Bool -> PrettyRep

ppBesides     :: [Pretty] -> Pretty
ppBesides = foldr1 f nil
\end{code}

Here |ppBesides| may evaluate to |undefined| if the input list to |foldr1| is |[]|. The |undefined| value will be of type |Pretty|, and will be given further arguments, which can be functions. In reality, the code ensures that the input list is never |[]|, so the program will never fail with this error.

While very few programs have functional values, a substantial number make use of applications to a non-function/non-constructor, and use over-application of functions. In most cases these result from supplying |error| calls with additional arguments, typically related to the desugaring of |do| notation and pattern matching within Yhc. We do not consider the use of a small number of higher-order applications to be a problem.

The termination bound used on the programs varies from 2 to 11 within the sample programs. If we exclude the integer program, which is complicated by the primitive operations on functional values, the highest bound is 8. Most programs have a termination bound of 4. There is no relation to the size of a program and the termination bound.

On average the size of the resultant program is smaller by 33\%. We measured program size by taking the number of nodes in the abstract syntax tree, and the number of functions. We took the average of the two measures, which were typically very close to each other. Before counting the size we remove all functions which simply call on to other functions passing the same arguments, for example:

\begin{code}
f x y = g x y
\end{code}

Given this, we would replace all invocations of |f| with |g|. The reason for the decrease in program size is due to dictionaries holding references to unnecessary code.

\section{Related Work}
\label{sec:related}

Reynolds style defunctionalization \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Reynolds method works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\ignore\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f y : map f ys
\end{code}

\noindent Now all calls to |map head| are replaced by \ignore|map Head|.
\end{example}

Reynold's method works on all programs, defunctionalized code is still type safe, but type checking would require a dependently typed language. Since the original work, people have proposed variants of Reynold's method that are type safe in the simply typed lambda calculus \cite{bell:type_driven_defunctionalization}, and within a polymorphic type system \cite{pottier:polymorhpic_typed_defunctionaization}.

We are unaware of any simple method for extending Reynold's style defunctionalization to primitives, without changes the primitives to be aware of the \ignore|Function| data type. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Reynold's method has been used as a tool in program calculation \cite{danvy:defunctionalization_at_work,hutton:calculating_an_exceptional_machine}, often as a mechanism for removing introduced continuations. Another use of Reynold's method is for optimisation \cite{grin,jhc}, allowing flow control information to be recovered without the complexity of higher-order transformation.

The closest work to ours is \citet{chin:higher_order_removal}, which itself is similar to that of Nelan \cite{nelan:firstification}. They define a higher-order removal method, with similar goals of removing functional values from a program. Their work shares some of the simplification rules, the arity raising and function specialisation. Despite these commonalities, there are great differences in how their method is structured.

\begin{itemize}
\item Their method makes use of the types of expressions, information that must be maintained and extended to work with additional type systems.
\item Their method does not have an inline step, or any notion of boxed lambdas. Functional values within constructors are ignored, and the authors suggest the use of deforestation \cite{wadler:deforestation} to help remove them. Deforestation is not tailored to this usage, and therefore transforms the program more than necessary, and fails to eliminate many functional values.
\item Their specialisation step does not work with boxed lambdas to functional values.
\item To ensure termination of the specialisation step, they never specialise a recursive function which does have any functional arguments passed identically in all recursive calls. This restriction satisfies higher-order functions such as |map|, but fails in many other cases.
\item By splitting our the simplification rules, our other stages are comparatively simple to theirs.
\end{itemize}

The limitations of their work are entirely understandable, given the nature of programming at the time their work was written. While nowadays people use monads, IO continuations and type classes as a matter of course, these features were still experimental when their work was done. Our work can be seen as a successor to theirs, achieving similar goals but with a greater complexity of input program. Our work achieves most of the aims set out in their future work section.

We have tried their examples, and can confirm that all of them are successfully handled by our system. None of the nofib programs could be reduced by their transformations. Some of their observations and extensions apply equally to our work, for example they suggest possible methods of removing accumulating functions such as in Example \ref{ex:functional_lists}.

The specialisation and inlining steps are taken from existing program optimisers, as is the termination strategy of homeomorphic embedding. A lot of program optimisers remove some higher-order functions, such as partial evaluation \cite{jones:partial_evaluation} and supercompilation \cite{supercompilation}. We have certainly benefited from ideas in both these areas in developing our algorithms. Our initial attempt at removing functional values involved modifying a supercompiler \cite{me:supero}. Unfortunately this approach has a number of disadvantages. Firstly, the optimiser is not attempting to preserve correspondence to the original program, so will optimise all aspects of the program equally, instead of focusing on the higher-order elements. The other more serious problem was that the results were poor -- given the restricted setting of only removing functional values, the techniques can be pushed much further.

\section{Conclusions and Future Work}
\label{sec:conclusion}

Higher order functions are very useful to the programmer, but may pose difficulties for certain types of analysis. Using the method we have described, it is possible to remove most functional values from most programs. Our method has already found practical use within the Catch tool, and we hope it can be of benefit to others.

The implementation presented in this paper is designed for clarity, not speed. By simply tracking which functions have changed, a massive performance could be obtained. The use of a numeric termination bound in the homeomorphic embedding is regrettable, but practically motivated. We need further research to determine if such a numeric bound is necessary, or if other measures could be used, and what the bound should be.

Many analysis methods, in fields such as strictness analysis and termination analysis, start out first-order and are gradually extended to work in a higher-order language. Instead of modifying each analysis method, we hope that instead we can transform the functional values away, enabling more analysis methods to work on a greater range of programs.

\section*{Appendix A}

The following script can be supplied to the AProVe system \cite{aprove} to check termination of our simplification rules, as described in \S\ref{sec:termination_simplification}.

\begin{verbatim}
[x,y,z]
app(lam(x),y)    -> let(y,x)
app(case(x,y),z) -> case(x,app(y,z))
app(let(x,y),z)  -> let(x,app(y,z))
case(let(x,y),z) -> let(x,case(y,z))
case(con(x),y)   -> let(x,y)
case(x,lam(y))   -> lam(case(x,app(lam(y),var)))
let(lam(x),y)    -> lam(let(x,y))
\end{verbatim}


\bibliographystyle{plainnat}

\bibliography



\end{document}
