\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}

\include{paper}
%include paper.fmt

\begin{document}

\conferenceinfo{ICFP '08}{date, City.} %
\copyrightyear{2008} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Lambdas Begone!}
\subtitle{ -- first order reduction for a Core language}

\authorinfo{Neil Mitchell\titlenote{The first author is supported by an EPSRC PhD studentship}}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~ndm/}}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~colin/}}

\maketitle

\begin{abstract}
We describe an automated transformation that takes a higher-order program, and a produces an equivalent program with most functional values removed. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method has a number of advantages over Reynold's style defunctionalisation -- it is type safe within the Hindley-Milner framework, and is more amenable to analysis. While our method cannot always succeed in removing all functional values, in practice is usually manages to.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, transformation

\keywords
Haskell, first-order reduction, defunctionalisation, functional programming

\section{Introduction}

Higher-order functions are well used and much loved within functional programming languages such as Haskell \cite{haskell}. Their use is ubiquitous -- even a simple numeric literal in Haskell typically expands into a the application of a function taking a tuple containing many functional values -- as part of the dictionary transformation for the implementation of type classes. While having functions as first-class values leads to more concise code, it often complicates analysis methods, such as checking pattern-match safety \cite{catch} and termination checking \cite{termination_checking}.

\begin{example}
\begin{code}
incList :: Int -> Int
incList = map (+1)
\end{code}

Consider the above definition of |incList|, containing many higher-order features. The function |(+1)| is passed as a functional argument to |map|. The |incList| value is a functional value, bound to a function taking a list to a list. The use of first-class functions has lead to short code, but we could equally have written:

\begin{code}
incList :: Int -> Int
incList []      = []
incList (x:xs)  = x+1 : incList xs
\end{code}

While the second variant of |incList| is much longer, it may also be more amenable to certain types of analysis -- and indeed may also perform faster at runtime. We aim to automatically transform from a program with functional values, to one without.
\end{example}

Our first-order reduction method processes the program as a whole to remove the functional values, without changing the semantics of the program. This idea is not new, indeed as far back as 19** Reynold's gave a solution, Reynold's style defunctionalisation. Unfortunately, Reynold's method effectively introduces a mini-interpreter within the resultant program, which causes problems for analysis tools. Our method instead tries to produce a program closer to what a human may have written, if not allowed the benefit of functional values.

Our method has been implemented in Haskell, and operates over the Core language from the York Haskell Compiler (Yhc). We have used our transformation within the Catch analysis tool, which checks for potential pattern-match errors in Haskell. In addition, we have made our method available as a library on Hackage -- and hope others from the termination checking community may be able to make use of it.

Our goals are primarily for analysis of the resultant code, not to compile and execute the result. Because we are not interested in performance, we permit the loss of sharing in computations, if to do so will remove functional values.

This paper starts with a definition of our Core language \S?, including what we consider to be a higher-order function. We then review Reynold's style defunctionalisation \S?. Next we present out method in \S?, motivated by a series of examples. Next we classify where out method may fail to remove higher order functions. We then prove that our method is correct in \S?, and terminating in \S?. Finally we give results \S?, review related work \S? and conclude \S?.

\section{Our Core Language}

\begin{figure}
\begin{code}
func = f vs_ expr

expr  =  \vs_ -> x
      |  f xs_
      |  c xs_
      |  x xs_
      |  v
      |  case x of alts_
      |  let v = x in y

alt = c vs_ -> x
\end{code}

We let |v| range over variables, |x| over expressions, |f| over function names and |c| over constructors.
\caption{Core Language.}
\label{fig:core}
\end{figure}

Our Core language is presented in Figure \ref{fig:core} -- and has a number of differences from a standard Core language. A function in our Core language consists of a name, a list of arguments, and a body expression. The arity of a function is the number of arguments associated with it.

The variable, case, let, application and lambda expressions are much as they would be in any Core language. The constructor expression consists of a constructor and a list of expressions, exactly matching the arity of the constructor -- any constructors not given enough arguments can be wrapped within a lambda expression. The function expression consists of a function and a list of expressions -- a number which may be more, equal to, or less than the arity of the function.

A program can be said to be higher-order if at runtime the program generates and manipulates functional values.

\begin{example}
\label{ex:ho_elements}
The following two functions can be thought of as having functional elements:

\begin{code}
even = (.) not odd

map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

The |even| function passes |not| and |odd| to the function |(.)|, both of which are functional values. In addition, the |(.)| function, when applied to only two arguments, returns a functional value. The definition of |map| does not create any functional values, but applies the variable |f| to an argument, suggesting that |f| is a functional value.
\end{example}

We classify higher-order elements as either \textit{creating} or \textit{applying} functional values.

\subsection{Creating Functional Values}

The most obvious way to create a functional value in our Core language is with an explicit lambda expression. The other way is to \textit{curry} or \textit{partially apply} a function, by passing fewer arguments than the arity of the function.

\begin{example}
\begin{code}
example1 = (\x -> x) 42

example2 xs = map id xs
\end{code}

Here |example1| contains an explicit lambda, which is a functional value. In |example2| the function |id| has an arity of 1, but is not given any arguments, and hence is a functional value.
\end{example}

It is possible for a program to have either one of these syntactic conditions, but at runtime, not create any functional values. Consider the following examples:

\begin{code}
example1 = case  True of
                 True   -> 1
                 False  -> (\x -> x) 2

example2 = x (\y -> y)
    where x = x

example3 = (error "bang") (\x -> x)
\end{code}

In |example1| the |True| branch will always be taken due to the known case scrutinee. The |False| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. In |example2| and |example3| the a lambda is applied to |undefined|, so the program will terminate before the lambda is evaluated. These situations can all be thought of as having functional elements in dead code.


\subsection{Applying Functional Values}

There are two ways to make use of functional values. The first is to apply an argument to an expression which is not a constructor or a top-level function. The second is to give a function more arguments than its associated arity.

\begin{example}
\begin{code}
example1 f = f 42

example2 = even 42
\end{code}

In |example1| the argument |f| is applied to the value 42, suggesting that |f| is functional. In |example2|, |even| is applied to one argument. Using the definition of |even| from Example \ref{ex:ho_elements}, with arity 0, this causes |even| to be given more arguments than its arity.
\end{example}

As before, we can construct an example where a functional value is applied, but which never occurs at runtime due to dead code. Another way of constructing an apparent application of a functional value is using |undefined| or a call to |error|.

\begin{example}
\begin{code}
example1 = case  Nothing of
                 Nothing  ->  1
                 Just x   ->  x 1

example2 = error "bang" 42

example3 = x 42
    where x = x
\end{code}

In |example1| the value |x| is taken from a data constructor, when we know no value will ever match. In |example2| the |error| primitive is being given two arguments, even though at first glance it appears to only take one. The is possible because the type of error is |String -> alpha|, where |alpha| is completely unconstrained and can be anything, including a functional type. In |example2| the value |42| is being passed to |x|. At runtime |x| will not terminate, and semantically reduces to |undefined|.
\end{example}

\subsection{First-Order Core}
\label{sec:first_order_restrictions}

We define a program to be first-order if it contains no expressions of the two types identified as creating functional values. In order to allow the property to be calculated statically, we ignore the issue of dead code.

There exist programs, or fragments of code, which cannot be reduced to first-order. We present several examples in turn, explaining why first-order reduction is not possible.

\begin{example}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from zero to one. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type |IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> _E alpha)
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to |_E alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{example}

\begin{example}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are specialising, we cannot change its interface.
\end{example}

\begin{example}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type |alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Some primitives not only permit functional values, but actually \textit{require} them. The |primCatch| function within the Yhc standard libraries implements the Haskell exception handling function |catch|. Tye type of |primCatch| is |alpha -> (IOError -> alpha) -> alpha|, taking an exception handler as one of the arguments.
\end{example}

In all these examples, a functional value must be created as it is required by an interface external to the program under transformation -- either a primitive or the root function. If none of the above cases occurs, then it is always possible to remove all function values from a Core program.

In the following sections we will assume that the root function returns a first-order value, and takes no parameters. We also assume there are no primitive functions. We first present Reynold's method for removing higher-order functions, followed by our method.

\section{Reynolds style defunctionalization}

Reynolds style defunctionalization \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Defunctionalization works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f a : map f as
\end{code}

\noindent Now all calls to |map head| are replaced by |map Head|.
\end{example}

This method naturally extends to partial application. To take a more complicated example, where higher-order functions are being used to store information:

\begin{example}
\begin{code}
type Map = String -> Int

new :: Map
new _ = 0

get :: String -> Map -> Int
get key mp = mp key

add :: String -> Int -> Map -> Map
add key val mp s = if s == key then val else get key mp

test = get "foo" (add "bar" 4 (add "baz" 2 new))
\end{code}

\noindent The above code creates a functional map, which uses a higher-order function to store a mapping from |String| to |Int|. The |add| function inserts a new key/value pair into the map. This is transformed with defunctionalization to:

\begin{code}
data Function  =  New
               |  Add3 String Int Function

apply  New                 x = new x
apply  (Add3 y_1 y_2 y_3)  x = add y_1 y_2 y_3 x

new _ = 0

get key mp = apply mp key

add key val mp s = if s == key then val else get key mp

test = get "foo" (Add3 "bar" 4 (Add3 "baz" 2 New))
\end{code}

Here we use the constructor |Add3| to represent the |add| function with three arguments pre-applied. Note that the |Function| data type now serves to store a linked-list of the values with |New| serving a similar role to |[]|, and |Add3| storing one key/value pair along with the remainder of the list.
\end{example}

We are unaware of any simple method for extending Reynold's style defunctionalisation to primitives, without changes the primitives to be aware of the |Function| data type. Defunctionalized code is still type safe, but type checking would require a dependently typed language. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Reynold's method has been used as a tool in program calculation \cite{graham_hutton_calculating_an_exceptional_machine, another_one, another_one}, often as a mechanism for removing introduced continuations. Another use of Reynold's method is for optimisation \cite{grin,jhc}, allowing flow control information to be recovered without the complexity of higher-order transformation.


\section{Our First-Order Reduction Method}

Reynold's method introduces a new data type that serves as a representation of functions, then embeds an interpreter for this data type. A natural desire would be to eliminate the higher-order aspects of a program, \textit{without} introducing any new data structures. Unfortunately, such a transformation is not possible.

Given a program, we can remove all data structures by Church encoding \cite{church_encode}. If we then had a transformation which made the program first-order \textit{without} introducing any data, we would end up with a program without data or closures, which is therefore incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

Before describing our method, we first give a list of desirable features of our algorithm -- in priority order. Often one criteria implies that future goals cannot be met.

\begin{enumerate}
\item Our method must be correct and terminating.
\item We do not wish to introduce any additional data values.
\item We would like to remove as many functional values as possible.
\item We would like to preserve the sharing behaviour of the program.
\end{enumerate}

Our reduction method proceeds in four steps, which are combined using a fixed point operator. Because our method terminates and does not introduce any data structures, it is necessarily incomplete -- but Reynold's method can be used afterwards if necessary. Our method can be trivially extended to work in a setting where primitives exist, and we explain how to do so.

For the implementation, available at ..., we have implemented our steps within a monadic framework to deal with issues such as obtaining unique free variables and tracking termination constraints. To simplify the presentation, we have ignored these issues -- while these are tedious engineering issues, they do not effect the algorithm.

Our method is written as:

\begin{code}
firstify = lambdas +|+ simplify +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operator, defined as:

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g x  = fix (fix x . y)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x' then x else fix f x'
    where x' = f x
\end{code}

The |(+||+)| operator applies the first argument until it reaches a fixed point, then applies the second argument. If the second argument changes the value, the first argument is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Idempotent in each function] After the operation has completed, applying either |f| or |g| will not change the value.

\begin{code}
forall f g x `o` let r = (+|+) f g x in f r == r && g r == r
\end{code}

\item[Idempotent] The operation as a whole is idempotent.

\begin{code}
forall f g x `o` let r = (+|+) f g x in r == (+|+) f g r
\end{code}

\item[Function ordering] The function |f| will have reached a fixed point before the function |g| is applied. If a postcondition of |f| implies a precondition of |g|, then we can guarantee |g|'s precondition will always be met. If both |f| and |g| could be applied, |f| will be applied first.
\end{description}

The final property allows us to overlap the application sites between the two arguments, but guarantee the first will always be chosen. The other two properties ensure that when the operation finishes there will be no further sites where application could occur.

The operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((lambdas +|+ simplify) +|+ inline) +|+ specialise
\end{code}

Within this chain we guarantee that the end result will be idempotent with respect to any of the functions, and before any function is invoked, all those to the left of it will be idempotent.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent on its own, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent in all but the first case, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited.

We now describe each of the stages in the algorithm separately.

\subsection{Lambda Insertion (|lambdas|)}

The first stage removes all occurrences of partial application, replacing them with explicit lambda expressions.

\begin{examplerevisit}{\ref{ex:ho_elements}}
\begin{code}
even = \x -> (.) (\y -> not y) (z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is applied to a number of arguments equal to its arity.
\end{examplerevisit}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. This step trades one form of functional value creation for another form, but has the advantage of making functional values more explicit.

In general, the insertion of a lambda and application is not semantics preserving:

\begin{code}
x => \v -> x v
\end{code}

Consider for instance |undefined `seq` 1|, which evaluates to |undefined|. Applying the lambda introduction rule we can replace the code with |(\v -> undefined v) `seq` 1|, which evaluates to |1|. Similarly, we could replace |1| with |\v -> 1 v|, which is neither semantics preserving nor type safe. However, a closely allied operation that is both semantics preserving and type safe is:

\begin{code}
\v -> x => \v' -> (\v -> x) v'
\end{code}

The lambda insertion phase can be seen as first transforming the function |f| using the lambda insertion rule, then inlining the outer lambda into the application site. These operations are semantics preserving, and standard within the field of program transformation.

\subsection{Simplification (|simplify|)}

\newcommand{\simp}[2]{\vspace{-7mm} #2 & (#1) \\}
\newenvironment{simplify}
    {\begin{flushright}
     \begin{tabular}{p{6.5cm}r}
    }
    {\end{tabular}
     \vspace{-7mm}
     \end{flushright}
    }

\begin{figure}
\begin{simplify}

\simp{app-app}{
\begin{code}
(x xs_) ys_
    => x xs_ ys_
\end{code}}

\simp{lam-lam}{
\begin{code}
\vs_ -> (\ws_ -> x)
    => \vs_ ws_ -> x
\end{code}}

\simp{lam-app}{
\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\simp{case-con}{
\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\simp{case-app}{
\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\simp{let-app}{
\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\end{simplify}
\caption{Standard Core simplification rules.}
\label{fig:simplify}
\end{figure}


\begin{figure}
\begin{simplify}

\simp{case-lam}{
\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \w -> case x of {... ; c vs_ -> \v -> y ; ...} w
\end{code}}

\simp{bind-lam}{
\begin{code}
let v = \w -> x in y
    => y[v / \w -> x]
\end{code}}


\simp{let-lam}{
\begin{code}
let v = x in \w -> y
    => \w -> let v = x in y
\end{code}}

\end{simplify}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The second stage attempts to move lambda's upwards until they form part of the arity of a function. This stage makes use of the general simplification rules (Figure \ref{fig:simplify}), along with some additional rules which deal with lambda expressions, given in Figure \ref{fig:lambda_simplify}.

We also require the additional simplification rule:

\begin{code}
f vs_ (\ws_ -> x) = f (vs_ ws_) x
\end{code}

In this particular instance, the introduction of lambda expressions changes the arity of |even| from 0 to 1, possibly requiring applications of |even| to become partially applied, causing |lambdas| to be invoked.

The rule (case-lam) lifts a lambda out from within a case alternative to outside the case value. The (bind-lam) rule inlines a lambda bound in a let expression. The (let-lam) rule is the one responsible for the largest loss of sharing, promoting a lambda expression outside a let, although (case-lam) can be used in a similar manner to reduce the sharing.

\begin{example}
\begin{code}
f x = let i = expensive x
      in \j -> i + j

main xs = map (f 1) xs
\end{code}

In the above example, |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x j = let i = expensive x
        in i + j
\end{code}

Now |expensive| will be recomputed for every element in |xs| -- potentially a very severe speed penalty.
\end{example}

The loss of sharing is not purely theoretical, the Uniplate library makes use of a let within a lambda (see \S\ref{sec:optimise_playdata}) to keep a scoreboard. The (let-lam) rule would make the scoreboard mechanism a severe performance penalty.

\subsection{Inlining (|inline|)}

After inserting additional lambda expressions, and performing simplification, there may still be residual lambda expressions which are not at the top level of a function. The simplification rules will promote lambda expressions up above let expressions and case expressions. With the additional rules, the only residual lambda expressions will be as arguments to either a constructor, or a function. The inline stage deals with lambda expressions which are residual to a constructor at the top level, the specialise stage deals with all remaining lambda expressions.

\begin{example}
Given a program making use of type class dictionaries, as detailed in \S\ref{sec:dictionary_transformation}, we may end up with the following code:

\begin{code}
eqInt = (\x y -> primEqInt x y, \x y -> primNeqInt x y)
(==) d x y = d x y

main = (==) eqInt 1 2
\end{code}

Here the lambda expression including |primEqInt| is an argument to the constructor |(,)| in the top-level of |eqInt|. The inline stage will inline the |eqInt| function anywhere it occurs, resulting in:

\begin{code}
(==) (a,b) x y = a x y

main = (==) (\x y -> primEqInt x y, \x y -> primNeqInt x y) 1 2
\end{code}

The lambda expressions are still present, but hopefully can be removed using other techniques.
\end{example}

The general rule is to inline all functions containing a subexpression |alpha|, where: |alpha| is a constructor application with one argument which is a lambda expression; and |alpha| is not a subexpression of any argument to a function application.

\begin{example}
\begin{code}
yes0 = [\x -> x]
yes1 = Maybe [\x -> x]
yes2 = let y = 1 in [\x -> x]
no0 = \x -> id x
no1 = [id (\x -> x)]
no2 = id [\x -> x]
\end{code}

In this example, we would inline the |yes| functions, but none of the others. In |no0| there is no constructor, so the first condition does not apply. In |no1| there is a constructor, but the lambda expression is not an argument to the constructor. In |no2| there is a lambda expression as an argument to a constructor, but that subexpression is an argument to a function application.
\end{example}

The |inline| rule deals with situations where functions evaluate to a data structure containing functional values. This situation occurs regularly with the standard dictionary implementation, but rarely in other situations. The inline rule does not actually remove functional values, but can bring their use and creation closer together, and thus helps them be removed.

There may be some loss of sharing with the inline rule, if a CAF is inlined.

\subsection{Specialisation (|special|)}

The original Catch tool \cite{me:catch_tfp} uses specialisation to remove higher-order functions. For each application of a function to functional arguments, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applied where function arguments are partially applied functions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have functional arguments, and generate templates, omitting first-order components.
\item For each template, generate an associated function, specialised to the template.
\item For each subexpression matching a template, replace it with the associated function.
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The specialisation first finds the application of |map| within |main|, and generates the template |map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates the name |map_id| for the template, and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code has no functional values within it.
\end{example}

\subsubsection{Generating Templates}

A template is generated if an expression is an application to a top-level function, whose arguments include a sub-expression which is a lambda expression. The template includes all sub-expressions whose removal would lead to higher-order elements, or which have free variables from a bound variable.

\begin{example}
\begin{code}
id (\x -> x)              => id (\x -> x)
id (Maybe (\x -> x))      => id (Maybe (\x -> x))
id (Maybe (\x -> x + 3))  => id (Maybe (\x -> x + _))
id (Maybe (\x -> x + y))  => id (Maybe (\x -> x + _))
\end{code}

In all three examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, the |3| and |y| are not dependent on variables bound within the lambda, and are left as unspecified.  The |Maybe| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. Each |_| within the template is assigned a free variable, as an argument to the new function, then the body is produced by unfolding the outer function symbol in the template once.

\begin{example}
\label{ex:map_id}
Following the |map (\x -> x) _| template from above, we can generate |v_1| as the unique free variable for the single |_| placeholder, and |map_id| as the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

In the next step, we unfold the definition of map once:

\begin{code}
map_id v_1 = let  f   = \x -> x
                  xs  = v_1
             in   case  xs of
                        []    -> []
                        y:ys  -> f y : map f ys
\end{code}

Now the generation of the specialised variant is complete. To give an idea of how the final function is calculated, after the simplification rules introduced in Figure \ref{fig:lambda_simplify}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{example}

\subsubsection{Using Templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an undecided element is passed as an argument. Continuing with the generated code from Example \ref{ex:map_id}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We have now eliminated all the functional values from within this operation.


\subsubsection{Extending to Primitives}

The specialisation is the only section that requires extending when working within a language containing primitives. The only restriction is that if a function to be specialised is actually a primitive, no template should be generated. The reason for this is because the generation of code associated with a template requires a one-step unfolding of the function, something which cannot occur with a primitive.

\section{Proof of Completeness}

\todo{This proof sucks -- even if you remove simplify, the inlining of lambdas ensures you non-terminate. Or rather, the inlining rule assumes there is a context to inline into, which is not the case if you miss simplifications and promote them to the root.}

The algorithm we have presented is complete, in the sense that if it terminates, there will be no remaining lambda expressions within the program, other than those identified bellow. We have already shown that it is impossible to have both completeness and not introduce data structures (which our algorithm does not), therefore it follows that our algorithm is \textit{not} terminating. We shall return to the issue of termination in \S\ref{sec:firstify_terminate}, but assuming that our algorithm does terminate, it is possible to show that there are no remaining functional values.

We originally identified three causes of incompleteness when converting a program to first-order, detailed in \S\ref{sec:first_order_restrictions}. Two of those can be removed by restricting the root function to take no functional arguments and return a first-order value. The only remaining necessary cause is passing a lambda expression to a primitive function. Our method introduces one final instance of incompleteness -- passing a lambda to an expression that evaluates to |undefined|, caused by either a call to the |error| primitive, or by non-termination.

We can prove that, other than the restrictions introduced above, there are no resultant functional values created. We assume for the purposes of this proof that all functions are non-primitive.

\newenvironment{lemma}[1]
    {\paragraph{Lemma:} #1 \textbf{Proof:} }
    {\hfill$\Box$}

\begin{lemma}{There is no partial application in the resultant code.}
After our algorithm terminates, the resultant code must be idempotent with respect to each of the four stages presented. The |lambdas| stage ensures there will be no partial application by inserting explicit lambda expressions.
\end{lemma}

\begin{lemma}{The root expression of each top-level function is not a lambda.}
If a top-level function is bound to a lambda expression, then the arity of the function is increased and the lambda is no longer the root expression. Another way of expressing this property is that all lambda expressions must have a parent expression.
\end{lemma}

\begin{lemma}{The parent of a lambda expression must be an application, applied to a variable.}
In all other cases, one of the rules will transform the expression. To enumerate the possible parent expressions:

\begin{description}
\item[lambda abstraction:] the desugaring rule for lambda abstractions would combine them to one lambda abstraction.
\item[let binding:] the (let-lam) and (bind-lam) rules applied as part of |simplify| ensure these expressions are removed.
\item[case expression:] the (case-lam) rule will remove any lambda abstractions from the alternatives of a case expression. The static typing in the original Core language ensures that a lambda cannot be the scrutinee of a case expression.
\item[application:] the (lam-app) rule ensures that the lambda abstraction must be one of the arguments to the application. We now enumerate all the possible applied values within the application:
    \begin{description}
    \item[constructor:] Either the expression is the child of a function application, in which case the |special| stage will remove it, or it is not, in which case the |inline| stage will remove it.
    \item[function:] The |special| stage will specialise the function with respect to the lambda abstraction.
    \item[application:] The application desugaring rules will combine the application with the current one, removing this situation.
    \item[let binding:] The (let-app) rule applies.
    \item[case expression:] The (case-app) rule applies.
    \end{description}

    The only possibility not covered is that there is an application whose applied expression is a variable.
\end{description}
\end{lemma}

\begin{lemma}{All applications of a variable to a lambda expression evaluate to |undefined|.}
Given an expression |v (\v' -> x)|, the variable |v| must evaluate to |undefined|. It is clear from the type rules of our Core language that the variable |v| must either evaluate to |undefined|, or to a lambda expression. If it evaluates to a lambda expression, there must be a lambda expression within the program, since we are assuming no primitive functions or functional arguments to the root function. If such a lambda exists, it must be itself under an application to a variable, since all other possibilities have been eliminated. Given many such enclosing |v| variables, we must evaluate one first. The first one we evaluate cannot evaluate to a lambda expression, as all lambda expressions must result from first evaluating a variable, and since this is the first variable to be evaluated, that is not possible. The only other case is that the variable evaluates to |undefined|.
\end{lemma}


\section{Proof of Correctness}

To prove the correctness of the whole argument it is sufficient to show the correctness of each step. The simplification rules are all standard within lazy Core languages. The inlining and specialisation stages are also based on well accepted transformations. the only interesting case, from a correctness perspective, is lambda insertion.



\section{Proof of Termination}
\label{sec:firstify_terminate}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.


\subsection{Termination of Inlining}

The standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = (\x -> x) : f
\end{code}

The expression bound to |f| will repeatedly grow in size as inlining is applied, and will remain a candidate for inlining.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. The resultant code would be:

\begin{code}
f = (\x -> x) : (\x -> x) : f
\end{code}

Any future attempts to inline |f| within this function would be disallowed, although |f| could be inlined within other functions. This termination criteria is sound, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}

The termination of specialisation can be achieved by using a homeomorphic embedding \ref{sec:homeomorphic}.

The termination of specialisation is more tricky than inlining. We permit the inlining

A natural extension of specialisation is to take the fixed point, eliminating unsaturated expressions in generated functions. Unfortunately, such an algorithm would not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. We would generate an infinite number of specialisations of |f|.
\end{example}

One simple way to prevent such non-termination is to have a bound on the number of specialisations. Another approach is to use a homeomorphic embedding. All functions relate to some original expression in the Core language, if the expression to be generated was a homeomorphic embedding of an already specialised expression, we can stop.

Using homeomorphic embedding on the previous example, we would generate the following specialised variants of |f (Value head)| and |f (Wrap (Value head))|. Upon attempting to generate the specialised variant |f (Wrap (Wrap (Value head)))| we would abort, with an embedding of |f (Wrap (Value head))|.

\subsection{Termination as a Whole}






\section{Results}

Our preferred method for higher-order function removal is to apply specialisation and inlining interleaved. We have tried our method on the nofib suite, and have the following results.

\section{Conclusions}

We have developed a method for removing functional values from a program. Our method has good results, and has found practical use within the Catch tool.



\bibliographystyle{plainnat}

\bibliography



\end{document}
