\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}

\include{paper}

%include paper.fmt
%format +|+ = "\mathbin{\ddagger}"
%format &+ = "\mathbin{\cup}"
%format &* = "\mathbin{\cap}"

%format _a = "\mathit{a}"
%format _b = "\mathit{b}"
%format _e = "\mathit{e}"
%format _s = "\mathit{s}"
%format _l' = "\mathit{l}^\prime{}"
%format _b' = "\mathit{b}^\prime{}"

%format let_ = "\mathsf{let}"
%format case_ = "\mathsf{case}"

%format Lambda a b = \a -> b
%format Fun a b = a \? b
%format Con a b = a \? b
%format App a b = a \? b
%format Var a = a
%format Case a b = case a of b
%format Let a b c = let a = b in c

\newcommand{\simp}[2]{\vspace{-7mm} #2 & (#1) \\}
\newenvironment{simplify}
    {\noindent
     \begin{flushright}
     \begin{tabular}{p{6.5cm}r}
    }
    {\end{tabular}
     \vspace{-7mm}
     \end{flushright}
    }


\begin{document}

\conferenceinfo{ICFP '08}{date, City.} %
\copyrightyear{2008} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Loosing functions without gaining data}
\subtitle{ -- a new method for defunctionalisation}

\authorinfo{Neil Mitchell\titlenote{The first author is supported by an EPSRC PhD studentship}}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~ndm/}}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~colin/}}

\maketitle

\begin{abstract}
We describe an automated transformation which takes a higher-order program, and a produces an equivalent program with most functional values removed. Our method has a number of advantages over Reynold's style defunctionalisation -- it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. While our method cannot always succeed in removing all functional values, in practice it usually manages to. We have evaluated our method on programs from the nofib benchmark, where only 3 out of 49 remain higher-order.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, transformation

\keywords
Haskell, first-order reduction, firstification, defunctionalisation, functional programming

\section{Introduction}

Higher-order functions are well used and much loved within functional programming languages such as Haskell \cite{haskell}. Their use is ubiquitous -- even a simple numeric literal in Haskell typically expands into a the application of a function taking a tuple containing many functional values -- as part of the dictionary transformation for the implementation of type classes \cite{wadler:type_classes}. While having functions as first-class values leads to more concise code, it often complicates analysis methods, such as checking pattern-match safety \cite{me:catch_icfp} and termination checking \cite{sereni:higher_order_termination}.

\begin{example}
\label{ex:incList}
\begin{code}
incList :: Int -> Int
incList = map (+1)

map :: (alpha -> beta) -> [alpha] -> [beta]
map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

Consider the above definition of |incList|, containing many higher-order features. The function |(+1)| is passed as a functional argument to |map|. The |incList| value is a functional value, bound to a function taking a list to a list. The use of first-class functions has lead to short code, but we could equally have written:

\begin{code}
incList :: Int -> Int
incList []      = []
incList (x:xs)  = x+1 : incList xs
\end{code}

While the second variant of |incList| is much longer, it may also be more amenable to certain types of analysis -- and indeed may also perform faster at runtime. The transformation pass presented in this paper performs the above transformation automatically.
\end{example}

Our first-order reduction method processes the program as a whole to remove the functional values, without changing the semantics of the program. This idea is not new, indeed as far back as 1972 Reynold's gave a solution, known as Reynold's style defunctionalisation \cite{reynolds:defunc}. Unfortunately, Reynold's method effectively introduces a mini-interpreter within the resultant program, which causes problems for analysis tools. Our method instead tries to produce a program closer to what a human may have written, if not allowed the benefit of functional values.

Our method has been implemented in Haskell, and operates over the Core language from the York Haskell Compiler (Yhc). We have used our transformation within the Catch analysis tool, which checks for potential pattern-match errors in Haskell. In addition, we have made our method available as a library on Hackage\footnote{\url{http://hackage.haskell.org/cgi-bin/hackage-scripts/package/firstify}} -- and hope others from the termination checking community may be able to make use of it.

\subsection{Contribution}

\todo{Strengthen}

Our paper makes a number of contributions:

\begin{enumerate}
\item We define three transformation steps to remove functional values.
\item We show how to apply these transformation steps, in particular to maximise the removal of functional values.
\item We define termination criteria to ensure termination of our method.
\item We present results from the nofib suite, including discussions of the limits of our transformation mechanism.
\end{enumerate}

\subsection{Roadmap}

This paper starts with a definition of our Core language \S\ref{sec:core}, including what we consider to be a higher-order function. Next we present an overview of our method \S\ref{sec:overview}, followed by a more detailed account \S\ref{sec:detailed}, along with a number of examples in \S\ref{sec:examples}. We classify where functional values may remain in a resultant program \S\ref{sec:completeness} and show how to modify our method to make it terminating \S\ref{sec:termination}. Finally we give results \S\ref{sec:results}, review related work \S\ref{sec:related} and conclude \S\ref{sec:conclusion}.

\section{Our Core Language}
\label{sec:core}

\begin{figure}
\begin{code}
func = f vs_ expr

expr  =  Lambda vs_ x    -- lambda abstraction
      |  Fun f xs_       -- function application
      |  Con c xs_       -- constructor application
      |  App x xs_       -- general application
      |  Var v           -- variable
      |  Case x alts_    -- case expression
      |  Let v x y       -- let expression

alt = c vs_ -> x
\end{code}

We let |v| range over variables, |x| over expressions, |f| over function names and |c| over constructors.
\caption{Core Language.}
\label{fig:core}
\end{figure}

Our Core language is presented in Figure \ref{fig:core} -- and has a number of differences from a standard Core language. A function in our Core language consists of a name, a list of arguments, and a body expression. The arity of a function is the number of arguments associated with it.

The variable, case, let, application and lambda expressions are much as they would be in any Core language. The constructor expression consists of a constructor and a list of expressions, exactly matching the arity of the constructor -- any constructors not given enough arguments can be wrapped within a lambda expression. The function expression consists of a function and a list of argument expressions -- a number which may be more, equal to, or less than the arity of the function. If a function is given too few arguments, we refer to it as \textit{partially-applied}, the right number is \textit{fully-applied} and too many is \textit{over-applied}.

\paragraph{Definition: First-Order} A program can be said to be higher-order if at runtime the program generates and manipulates functional values. We define a program to be first-order if we can guarantee that at runtime it will create no functional values. Functional values can only be created in two ways: (1) a lambda expression; (2) an under-saturated function application. We define a first-order program to be one containing neither of these constructs.

In Example \ref{ex:incList}, our program is higher-order because of the under-saturation of both |map| and |(+)|. In the reduced version, the program is first-order.

We do not permit primitives in our language, but explain how to extend our method to them in \S\ref{sec:primitives}. We assume a root function, named |main|. We assume that if |main| takes any arguments, they are all first-order values, and that |main| evaluates to a first-order value. Again, we will explain how to loosen these restrictions in \S\ref{sec:primitives}.

We use the meta functions |arity f|, |body f| and |args f| to denote the arguments, body and arity of f. We use the function |rhs| to extract the expression on the right of a case branch.


\section{Our First-Order Reduction Method}
\label{sec:overview}

Our method works by combining three separate and well-known transformations, along with general simplification rules including many from standard Haskell compilers \cite{spj:transformation}.

\begin{description}
\item[Arity Raising:] A function is arity raised if by local transformations the arity of a function can be changed, without changing its semantics.
\item[Inlining:] Inlining is a standard technique in optimising compilers \cite{spj:inlining}, and has been studied at length.
\item[Specialisation:] Specialisation is another standard technique, used to remove type classes \cite{jones:dictionary_free} and more recently to specialisation functions to a given constructor \cite{spj:specconstr}.
\end{description}

Each transformation has the possibility of removing some functional values, but \textit{the key contribution of this paper is how they can be used together}, to remove functional values. Each transformation on its own is correctness preserving, and none introduce any additional data types. Each can also be performed in a typed Core language, although we have not done so.

We proceed by first giving a brief flavour of how each operation can be used to remove some functional values. We then discuss in detail how each step can be specified in \S\ref{sec:detailed}, including how they can be combined, and what restrictions are necessary for termination in \S\ref{sec:termination}.

\subsection{Simplification}

The simplification step serves to group all the short and simple transformations that most optimising compilers apply, typically as part of a simplification stage. Some of these steps have the ability to remove functional values, while others simply ensure a normal form for future transformations.

\begin{example}
\begin{code}
one = (\x -> x) 1
\end{code}

The simplification rules transform this function to:

\begin{code}
one = let x = 1 in x
\end{code}
\end{example}

Other rules within the program do not eliminate lambda expressions, but manipulate them -- putting them in a form other stages can remove.

\begin{example}
\begin{code}
even =  let one = 1
        in \x -> not (odd x)
\end{code}

Here the let expression is surrounding the lambda expression. We can lift the lambda expression out of the let expression.

\begin{code}
even = \x ->  let one = 1
              in not (odd x)
\end{code}

In general this transformation may cause duplicate computation to be performed, an issue we return to in \S\ref{sec:sharing}.
\end{example}


\subsection{Arity Raising}

The arity raising transformation is an attempt to increase the arity of a function, also known as eta expansion. It relies heavily on the simplification stage.

\begin{example}
\begin{code}
even = \x -> not (odd x)
\end{code}

Here the arity raising transformation changes the body of the function from being a lambda, and lifts the lambda arguments into the explicit arity of the function.

\begin{code}
even x = not (odd x)
\end{code}
\end{example}


\subsection{Inlining}

If a function returns a data constructor containing functional values, one way to remove these functional values is to use inlining.

\begin{example}
\begin{code}
f = case  eqInt of
          (a,b) -> a 1 2

eqInt = (primEqInt, primNeqInt)
\end{code}

In the above example the |eqInt| function evaluates to a tuple, where both |primEqInt| and |primNeqInt| are functional values. We can start to remove these elements by inlining the function |eqInt|:

\begin{code}
f = case  (primEqInt, primNeqInt) of
          (a,b) -> a 1 2
\end{code}

Next if we use the rule that the case scrutinee is a known constructor, we can reduce the program to a first-order variant. This rule is applied by the simplification stage.

\begin{code}
f = case  (primEqInt, primNeqInt) of
          (a,b) -> a 1 2
\end{code}
\end{example}

\subsection{Specialisation}

Specialisation works by replacing a function with a more specialised variant with some of its arguments passed at compile time.

\begin{example}
\begin{code}
notList xs = map not xs
\end{code}

Here the |map| function takes a functional value as its first argument. In the case of |notList| this functional value is |not|. We can create a variant of |map| specialised to |not| as its first argument:

\begin{code}
map_not x = case  x of
                  []    -> []
                  y:ys  -> not y : map_not ys

notList xs = map_not xs
\end{code}

The recursive call in |map| is replaced by a recursive call to the specialised variant. We have now eliminated all functional values.
\end{example}

\subsection{Goals}
\label{sec:goals}

Our goal is to combine the transformations we have  presented to remove as many functional values as possible. For example, the initial |incList| example required simplification, arity raising and specialisation. We have defined a number of goals, in order of priority. Some of the earlier goals mean that later goals are unacheivable -- and we will merely do the best we can.

\paragraph{Correct and terminating.} Like all transformation methods, both correctness and termination are important to make the transformation useful. By making use of three established transformations, correctness is relatively easy to show. The issue of termination is much harder -- both inlining and specialisation can be applied in ways that will diverge. To ensure termination we have to restrict where these two methods can be applied, using termination criteria. We develop a set of termination criteria, along with a proof that they are sufficient, in \S\ref{sec:termination}.

\paragraph{Correspondence to the original program.} Since our transformation designed to be performed before analysis, it is important that the results of the analysis can be presented in terms of the original program. We provide a method for transforming expressions in the resultant program into equivalent expressions in the original program.

\paragraph{Introduce no data types.} Reynold's method introduces a new data type that serves as a representation of functions, then embeds an interpreter for this data type. A natural desire would be to eliminate the higher-order aspects of a program, \textit{without} introducing any new data structures. Unfortunately, such a transformation is not possible.

Given a program, we can remove all data structures by encoding them as functions, as described in \citet{naylor:reduceron}. If we then had a transformation which made the program first-order \textit{without} introducing any data, we would end up with a program without data or closures, which is therefore incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

It is therefore clear that we will not be able to remove all functional values, and meet this goal. We choose therefore to not be complete. If a totally first-order program is required, Reynold's method can be always be applied to the result of our transformation.

By composing our transformation out of three existing transformations, none of which introduce data structures, we can easily ensure that our resultant transformation does not.

\paragraph{Remove all functional values.} We have already indicated that the removal of all functional values is an impossible goal within our previous constraints. However, we will attempt to remove as many functional values as possible. In \S\ref{sec:completeness} we classify where functional values may appear in the resultant programs.

\paragraph{Preserve the type information.} Given an original program with explicit types, we desire that the resultant program can still be assigned a type. Similar passes to our stages are implemented in a type preserving manner inside the GHC compiler \cite{ghc}, so we believe this task should be relatively easy. We note that Reynold's method is not so amenable to type preservation.

\paragraph{Preserve the space/sharing behaviour of the program.} In the expression |let y = f x in y + y|, according to the rules of lazy evaluation, |f x| will be evaluated at most once. We could inline the let binding to give |f x + f x|, but this expression evaluates |f x| twice. Where possible, we will avoid changing the sharing of the program. Our goals are primarily for analysis of the resultant code, not to compile and execute the result. Because we are not interested in performance, we permit the loss of sharing in computations, if to do so will remove functional values.

\paragraph{Other Desirable Qualities.} Previous defunctionalization attempts have shown a concern about code size increase \cite{chin:higher_order_removal}. A smaller resultant program would be desirable, but not at the cost of clarity. The speed of the implementation is not of much importance to us, provided it performs in a respectable time to permit proper evaluation.


\section{Method in Detail}
\label{sec:detailed}

Our method proceeds in four steps, simplification (|simplify|), arity raising (|arity|), inlining (|inline|) and specialisation (|specialise|). These steps are combined using a fixed point operator.

For the implementation, we have implemented our steps within a monadic framework to deal with issues such as obtaining unique free variables and tracking termination constraints. To simplify the presentation, we have ignored these issues -- while these are tedious engineering issues, they do not effect the algorithm.

Our method is written as:

\begin{code}
firstify = simplify +|+ arity +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operator, defined as:

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g = fix (g . fix f)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x' then x else fix f x'
    where x' = f x
\end{code}

The |(+||+)| operator applies the first argument until it reaches a fixed point, then applies the second argument. If the second argument changes the value, the first argument is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Idempotent in each function] After the operation has completed, applying either |f| or |g| will not change the value.

\begin{code}
forall f g x `o` let r = (+|+) f g x in f r == r && g r == r
\end{code}

\item[Idempotent] The operation as a whole is idempotent.

\begin{code}
forall f g x `o` let r = (+|+) f g x in r == (+|+) f g r
\end{code}

\item[Function choice] If both |f| and |g| could be applied, |f| will be chosen.

\item[Function ordering] The function |f| will have reached a fixed point before the function |g| is applied. If a postcondition of |f| implies a precondition of |g|, then we can guarantee |g|'s precondition will always be met.
\end{description}

The final property allows us to overlap the application sites between the two arguments, but guarantee the first will always be chosen. The other two properties ensure that when the operation finishes there will be no further sites where application could occur.

The operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((simplify +|+ arity) +|+ inline) +|+ specialise
\end{code}

Within this chain we guarantee that the end result will be idempotent with respect to any of the functions, and before any function is invoked, all those to the left of it will be idempotent.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent on its own, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent in all but the first case, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited.

We describe each of the stages in the algorithm separately. In all subsequent stages, we assume that all the simplification rules have been applied.


\subsection{Simplification}

\begin{figure}
\begin{simplify}

\simp{app-app}{
\begin{code}
(x xs_) ys_
    => x xs_ ys_
\end{code}}

\simp{fun-app}{
\begin{code}
(f xs_) ys_
    => f xs_ ys_
\end{code}}

\simp{lam-app}{
\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\simp{let-app}{
\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\simp{case-app}{
\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\simp{case-con}{
\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\simp{case-let}{
\begin{code}
case (let v = x in y) of alts_
    => let v = x in (case y of alts_)
\end{code}}

\simp{case-case}{
\begin{code}
case (case x of {... ; c vs_ -> y ; ...) of alts_
    => case x of {... ; c vs_ -> case y of alts_ ; ...}
\end{code}}

\simp{case-lam}{
\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \z -> case x of {... ; c vs_ -> (\v -> y) z ; ...} z
\end{code}}

\simp{eta}{
\begin{code}
f xs_
    => \v -> f xs_ v
    where arity f > length xs_
\end{code}}

\end{simplify}
\caption{Standard Core simplification rules.}
\label{fig:simplify}
\end{figure}


\begin{figure}
\begin{simplify}

\simp{bind-lam}{
\begin{code}
let v = (\w -> x) in y
    => y[v / \w -> x]
\end{code}}

\simp{bind-box}{
\begin{code}
let v = x in y
    => y[v / x]
    where x {-" \text{ is a boxed lambda (see \S\ref{sec:inlining})} "-}
\end{code}}

\simp{let-lam}{
\begin{code}
let v = x in \w -> y
    => \w -> let v = x in y
\end{code}}

\end{simplify}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The simplification stage has the goal of moving lambdas upwards, and introducing lambdas for partially applied functions. This stage makes use of standard simplification rules given in Figure \ref{fig:simplify}, which are found in most simplifiers, such as \cite{spj:transformation}. We also make use of additional rules which deal with lambda expressions, given in Figure \ref{fig:lambda_simplify}.

\subsubsection{Lambda Introduction}

The (eta) rule inserts lambdas where possible, using $\eta$-expansion. The original code from the Yhc compiler has been lambda lifted \cite{lambda_lift}, so contains no explicit lambda expressions. This rule removes all occurrences of partial application, replacing them with explicit lambda expressions. The second stage moves these lambda expressions around, attempting to lift them to the top level.

\begin{example}
\begin{code}
even = (.) not odd
\end{code}

Here the functions |(.)|, |not| and |odd| are all partially applied. Lambda expressions can be inserted to saturate these applications.

\begin{code}
even = \x -> (.) (\y -> not y) (z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is applied to a number of arguments equal to its arity. This transformation will enable the arity raising transformation, resulting in:

\begin{code}
even x = (.) (\y -> not y) (z -> odd z) x
\end{code}
\end{example}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. This step trades one form of functional value for another form, but has the advantage of making functional values more explicit, and permitting arity raising.

In general, the insertion of a lambda and application is not semantics preserving:

\begin{code}
x => \v -> x v
\end{code}

Consider for instance |undefined `seq` 1|, which evaluates to |undefined|. Applying the lambda introduction rule we can replace the code with |(\v -> undefined v) `seq` 1|, which evaluates to |1|. Similarly, we could replace |1| with |\v -> 1 v|, which is neither semantics preserving nor type safe. However, a closely allied operation that is both semantics preserving and type safe is:

\begin{code}
\v -> x => \v' -> (\v -> x) v'
\end{code}

The lambda insertion phase can be seen as first transforming the function |f| using the lambda insertion rule, then inlining the outer lambda into the application site. These operations are semantics preserving, and standard within the field of program transformation.


\subsubsection{Lambda Movement}
\label{sec:sharing}

The rule (case-lam) lifts a lambda out from within a case alternative to outside the case value. The (bind-lam) rule inlines a lambda bound in a let expression. The (bind-box) rule will be discussed as part of the inlining stage, see \S\label{sec:inlining}. The (let-lam) rule is the one responsible for the largest loss of sharing, promoting a lambda expression outside a let.

\begin{example}
\begin{code}
f x = let i = expensive x
      in \j -> i + j

main xs = map (f 1) xs
\end{code}

In the above example, |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x j = let i = expensive x
        in i + j
\end{code}

Now |expensive| will be recomputed for every element in |xs| -- potentially a very severe speed penalty.
\end{example}

The loss of sharing is not purely theoretical. To take one example, the Uniplate library \cite{me:uniplate} makes use of a let within a lambda to keep a scoreboard which is computed once. The (let-lam) rule would make the scoreboard mechanism a severe performance penalty.

As we focus on functional value removal at the expense of sharing, we perform these operations when necessary.


\subsection{Arity Raising}

The arity raising step is simple:

\begin{code}
function vs_ = \v -> x
    => function vs_ v = x
\end{code}

Given a body which is a lambda expression, the arguments to the lambda expression can be lifted into the arguments for the function. This transformation requires the body to be a lambda expression, so additional transformations are necessary to introduce and move lambda expressions. If a function has its arity increased, it is likely that this will cause applications of it to become partially saturated, requiring lambda expressions to be reinserted, using the (eta) simplification rule.

In this particular instance, the introduction of lambda expressions changes the arity of |even| from 0 to 1, possibly requiring applications of |even| to become partially applied, causing |lambdas| to be invoked.


\subsection{Inlining}
\label{sec:inlining}

We use inlining to remove functional values which are stored within a data structure -- for example |Just (\x -> id x)|. We refer to functional values inside data structures as \textit{boxed lambdas}, because they have been placed inside the value. If a boxed lambda is bound in a let expression, we substitute the let binding, using the (bind-box) rule from Figure \ref{fig:lambda_simplify}. We only inline a function if two conditions both hold: (1) the function evaluates to a boxed lambda; (2) the function application occurs within a case scrutinee.

\begin{figure}
\begin{code}
isBox (Con c xs_     )  =   any isLambda xs_ ||  any isBox xs_
isBox (Let v x y     )  =   isBox y
isBox (Case x alts_  )  =   any (isBox . rhs) alts_
isBox (Fun f xs_     )  =   isBox (body f)
\end{code}
\caption{The |isBox| function, to test if an expression is a boxed lambda.}
\label{fig:boxed_lambda}
\end{figure}

We define an expression to be a boxed lambda if the |isBox| check returns true. The |isBox| as written may not terminate, however we can ensure it does by keeping a list of followed functions, and assuming False for each one we have already seen. This does not decrease the number of true results, but turns non-termination into False.

\begin{example}
The following expressions are boxed lambdas:

\begin{code}
[\x -> x]
(Just [\x -> x])
(let y = 1 in [\x -> x])
[Nothing, Just (\x -> x)]
\end{code}

The following examples are not boxed lambdas:

\begin{code}
\x -> id x
[id (\x -> x)]
id [\x -> x]
\end{code}

In the first negative example there is no constructor, so the first condition does not apply. In the second there is a constructor, but the lambda expression is not an argument to the constructor. In the third there is a lambda expression as an argument to a constructor, but that subexpression is an argument to a function application.

Note that the function final expression evaluates to a boxed lambda, but that this information is hidden by the |id| function. We do not analyse within the |id| function, but assume that specialisation will deal with any boxed lambdas passed to functions.
\end{example}

We inline if the expression is a boxed lambda, more precisely, we make the following substitution:

\begin{simplify}

\simp{case-boxed}{
\begin{code}
case (f xs_) alts_
    => case (let args f = xs_ in body f) alts_
    where isBox (body f)
\end{code}}

\end{simplify}

The |inline| rule deals with situations where functions evaluate to a data structure containing functional values. This situation occurs regularly with the standard dictionary implementation, but only occasionally in other situations. The inline rule does not actually remove functional values, but can bring their use and creation closer together, and thus helps them be removed.

As with the simplification stage, there may be some loss of sharing if the function being inlined has an arity of 0, and is therefore in constant applicative form (CAF). A Haskell implementation will compute these functions once, and reuse their value repeatedly. If these functions are inlined, they will be recomputed.

\subsection{Specialisation (|special|)}

The original Catch tool \cite{me:catch_tfp} uses specialisation to remove higher-order functions. For each application of a function to functional arguments, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applied where function arguments are partially applied functions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have functional arguments, and generate templates, omitting first-order components.
\item For each template, generate an associated function, specialised to the template.
\item For each subexpression matching a template, replace it with the associated function.
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The specialisation first finds the application of |map| within |main|, and generates the template |map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates the name |map_id| for the template, and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code has no functional values within it.
\end{example}

\subsubsection{Generating Templates}

A template is generated if an expression is an application to a top-level function, whose arguments include a sub-expression which is either a lambda expression or a boxed lambda. The template includes all sub-expressions whose removal would lead to higher-order elements, or which have free variables from a bound variable.

\begin{example}
\begin{code}
id (\x -> x)              => id (\x -> x)
id (Maybe (\x -> x))      => id (Maybe (\x -> x))
id (Maybe (\x -> x + 3))  => id (Maybe (\x -> x + _))
id (Maybe (\x -> x + y))  => id (Maybe (\x -> x + _))
\end{code}

In all three examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, the |3| and |y| are not dependent on variables bound within the lambda, and are left as unspecified.  The |Maybe| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. Each |_| within the template is assigned a free variable, as an argument to the new function, then the body is produced by unfolding the outer function symbol in the template once.

\begin{example}
\label{ex:map_id}
Following the |map (\x -> x) _| template from above, we can generate |v_1| as the unique free variable for the single |_| placeholder, and |map_id| as the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

In the next step, we unfold the definition of map once:

\begin{code}
map_id v_1 = let  f   = \x -> x
                  xs  = v_1
             in   case  xs of
                        []    -> []
                        y:ys  -> f y : map f ys
\end{code}

Now the generation of the specialised variant is complete. To give an idea of how the final function is calculated, after the simplification rules introduced in Figure \ref{fig:lambda_simplify}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{example}

\subsubsection{Using Templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an undecided element is passed as an argument. Continuing with the generated code from Example \ref{ex:map_id}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We have now eliminated all the functional values from within this operation.

\subsection{Extending the method to allow primitive functions}
\label{sec:primitives}

The problem that primitives introduce is that they have a body which cannot be viewed by the program, and therefore cannot be examined or inlined. We can make two simple changes to support primitives. Firstly, we define that a primitive application is not a boxed lambda. Secondly, we restrict specialisation so that if a function to be specialised is actually a primitive, no template is generated. The reason for this is because the generation of code associated with a template requires a one-step unfolding of the function, something which cannot occur with a primitive.

\section{Examples}
\label{sec:examples}

We now show a series of examples, showing our method. We have particularly focused on where our leaves residual functional values, or where a small alteration to the method would harm the results.

\begin{examplename}{Dead Code}
\begin{code}
nothing = Nothing

example = case  nothing of
                Nothing  -> 1
                Just f   -> f (\x -> x)
\end{code}

In |example| the |Nothing| branch will always be taken due to the known case scrutinee. The |Just| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. Our method cannot remove this lambda.
\end{examplename}

\begin{examplename}{Undefined Values}
\label{ex:undefined_values}
\begin{code}
bottom = bottom

example = bottom (\y -> y)
\end{code}

In |example| a lambda is applied to |undefined|, so the program will non-terminate before the lambda is evaluated. The |bottom| function could also have been implemented as a call to the |error| primitive, with similar results.
\end{examplename}

\begin{examplename}{Root function returning functional values}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from zero to one. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type |IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> _E alpha)
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to |_E alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{examplename}

\begin{examplename}{Root function taking functional values}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are specialising, we cannot change its interface.
\end{examplename}

\begin{examplename}{Primitives}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type |alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Some primitives not only permit functional values, but actually \textit{require} them. The |primCatch| function within the Yhc standard libraries implements the Haskell exception handling function |catch|. Tye type of |primCatch| is |alpha -> (IOError -> alpha) -> alpha|, taking an exception handler as one of the arguments.
\end{examplename}

\begin{examplename}{Functional Lists}
\label{ex:functional_lists}
Sometimes lambda expressions are used to build up lists which can have elements concatenated onto the end. Using Hughes lists \cite{hughes:lists}, we can define:

\begin{code}
nil = id
snoc x xs = \ys -> xs (x:ys)
list xs = xs []
\end{code}

This list representation provides |nil| similarly to |[]| in standard lists, but instead of providing a |(:)| or ``cons'' operation, it provides |snoc| which adds a single element on to the end of the list. The function |list| is provided to create an original list. We are unable to first order reduce such a construction, as it stores the values within the lambda, requiring an infinite amount of lambdas. We have seen such constructions in both the |lines| function of the HsColour program, and the |sort| function of Yhc.

However, there is an alternative implementation of these functions:

\begin{code}
nil = []
snoc = (:)
list = reverse
\end{code}

We have benchmarked these operations in a variety of settings and the list based version appears to use approximately 75\% of the memory, and 65\% of the time. We suggest that people using continuations for |snoc| move instead to a list type.
\end{examplename}


\begin{examplename}{Inlining Boxed Lambdas}
\label{ex:inlining_boxed_lambdas}
Our original program inlined boxed lambdas everywhere they occured. This makes the detection of boxed lambdas much simpler, and does not require looking into external functions. However, it is unable to cope with certain functions:

\begin{code}
main xs = app (gen xs)
app = map ($ 1)
gen = map (const x)
\end{code}

Consider the above code. The function |gen| returns a boxed lambda. However, the code is recursive, so if it was inlined repeatedly, it would not terminate. After deciding to restrict the inlining of gen, we are still left with lambdas. However, by first specialising |app| with respect to |gen|, we are able to remove the functional values. The removal of the lambda expressions has effectively lead to a deforesting between the |gen| producer of cons cells, and the |app| consumer -- eliminating the intermediate lambda expressions in the same step.
\end{examplename}



\section{Classification of Completeness}
\label{sec:completeness}

Our method would be complete if it removed all lambda expressions from a program. Completeness is not obtainable given the termination restriction (see \S\ref{sec:goals}). We have two causes of incompleteness: those that come from the fundamental algorithm, and those that are added by any termination criteria in \S\ref{sec:termination}. Here we explore causes arising from our algorithm. We wish to classify where a lambda expression may reside in a program after the application of our firstification method.

To examine where lambda expressions may occur, we model our Core expression language as a context free grammar. We define the start symbol |_s|, and require that the bodies of all functions in our language be defined by the grammar. First we map all expressions onto symbols in our grammar:

\begin{code}
\vs_ -> x         => lam x
f xs_             => fun (body f) xs_
c xs_             => con xs_
x ys_             => app x ys_
v                 => var
case x of alts_   => case_ x (map snd alts_)
let v = x in y    => let_ x y
\end{code}

We have abstracted away variable names and patterns in expressions such as case, let and lambda. Some expressions, such as |app|, have multiple arguments. We have abstracted this away, and use |(app _a _b)| to denote that the first argument to an application is define by |_a|, and that all subsequent arguments are defined by |_b|.

In the case of function application, the first argument represents the possible expressions of the body of the function, and the second expression represents the arguments applied to the function. We compose grammars using |&+|, |&*| and |-| as grammar subtraction.

First we start out with the complete grammar:

% _ = L f C a v c l
\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app _s _s &+ var &+
       case_ _s _s &+ let_ _s _s
\end{code}

We know our original program is type safe. This means that the scrutinee of a case may not evaluate to a functional value, and therefore we can discard the |lam| production from the first argument of |case_|. Likewise, we know that all constructor expressions are saturated, so will evaluate to a data value, and cannot be the first argument of an application. Modifying our grammar to take account of this, we have:

% _ = L f C a(!C, _) v c(!L, _) l
\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app (_s - con _s) _s &+ var &+
       case_ (_s - lam _s) _s &+ let_ _s _s
\end{code}

Our simplification rules given in Figure \ref{fig:simplify} are applied until a fixed point is found, meaning that the left hand side of any rule cannot occur in the output. Accounting for these restrictions in the grammar we are left with:

% _ = L f C a(v, _) v c(f a v, _) l
\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
       case_ (fun _s _s &+ app var _s &+ var) _s &+ let_ _s _s
\end{code}

Next we apply the lambda rules from Figure \ref{fig:lambda_simplify}. As |(_s - lam _s)| is a common production we have factored it out as |l'|.

% _ = L f C a(v, _) v c(f a v, !L) l(!L, !L)
\begin{code}
n_l  =  _s - lam _s
_s   =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
        case_(fun _s _s &+ app var _s &+ var) _l' &+ let_ _l' _l'
\end{code}

We can now use the results of arity raising, which ensures that every function's root may not be a lambda expression.

% _ = L f(!L, _) C a(v, _) v c(f a v, !L) l(!L, !L)
\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun _l' _e &+ app var _e &+ var) _l' &+ let_ _l' _l'
\end{code}

We now wish to work with lambda boxes, as defined by the function |isBox|, from Figure \ref{fig:boxed_lambda}. Within our grammar rules it is not possible to define that an expression is a lambda box. The first expression defined as a lambda box is a constructor where one of the arguments is a lambda, and the other arguments are unrestricted. Using our grammar, we have merged all the arguments to a constructor, and cannot define the property that a term is a lambda box in our grammar. We can however define the property that something is \textit{not} a lambda box, which we define with the production:

%
\begin{code}
_b'  =  lam _e &+ fun _b' _e &+ con (_b' - lam _e) &+ app _e _e &+ var &+
        case_ _e _b' &+ let_ _e _b'
\end{code}

Now if an expression meets |_e|, but not |_b'|, then we know it is a lambda box. We can now make use of the inlining rule and the (bind-box) rules to write:

\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') _e &+ app var _e &+ var) _l' &+ let_ (_l' &* _b') _l'
\end{code}

Now we can make use of the specialise rule, which removes all lambdas and boxed lambdas from the arguments of function applications:

\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' (_l' &* _b') &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') (_l' &* _b') &+ app var _e &+ var) _l' &+ let_ (_l' &* _b') _l'
\end{code}

Having applied all the rules, we now classify what the parent expressions of a lambda may be. A lambda may not be the root of a function body, because |lam| is not a production in |_s|. If we restrict our attention only to expressions which permit a lambda expression as a direct child, and add explicit |lam _e| productions where we can, we are left with:

\begin{code}
_e =  lam (lam _e &+ _e) &+ con (lam _e &+ _e) &+ app var (lam _e &+ _e) &+
      -- productions not permitting a lambda as a direct child
\end{code}

A lambda may therefore occur as the child of a lambda expression, as an argument to a constructor, or as an argument to an application. However, a constructor containing a lambda is a lambda box, and therefore is not permitted anywhere |_b'| is intersected with the expression. We can now classify where a lambda box may occur, denoting all expressions as either |_b| to denote that a box may occur, or |_b'| to denote that it may not:

\begin{code}
lam _b &+ fun _b _b' &+ con _b &+ app _b' _b &+ case_ _b' _b &+ let _b' _b
\end{code}

Therefore a constructor containing a lambda may only occur at the root of a function (since |_s| permits boxed lambdas), or in one of the positions outlined above. Assuming no unreachable functions, either the root function of the program must evaluate to a boxed lambda, or there must be an expression which is not an unboxed lambda containing a boxed lambda. Of the above productions, most are boxed lambdas by definition, so the only non-boxed lambdas containing a boxed lambda are:

\begin{code}
lam _b &+ app _b' _b
\end{code}

Therefore the root function may be a boxed lambda, with a lambda inside the box. Or a lambda or boxed lambda may occur as inside a lambda, or as the second argument to an application. Given this situation, we can be sure that the variable does not bind to a lambda, as the lambda cannot escape. This leads us to the following situations:

\begin{code}
case v of
    Just w -> w (\x -> x)
    Nothing -> 1

bottom (\x -> x)
    where bottom = bottom

main f = f (\x -> x)
\end{code}

In the first case there must be no corresponding |Just| value containing a functional value, so the first branch must be dead code. In the second example, a non-terminating function the first argument of an application, and contains a lambda as an argument to it.


\section{Proof of Termination}
\label{sec:termination}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.

\subsection{Termination of Simplification}

In order to check the termination of the simplifier we have used the AProVE system \cite{aprove}, to model our rules as term rewrites, and check termination thereon. A simple encoding of our rules is given in Appendix A, which checks basic termination. We have proven termination using both this simple formulation, which considers all constructors to have one alternative of exactly arity one, and a more complex encoding. In both cases, the system is able to report success.

The encoding of the (bind-box) and (bind-lam) rules is excluded. Given these rules, there are non terminating sequences. For example:

\begin{code}
(\x -> x x) (\x -> x x)
   => -- (lam-app) rule
let x = \x -> x x in x x
   => -- (bind-lam) rule
(\x -> x x) (\x -> x x)
\end{code}

These type of expressions are a problem for GHC, and can cause the compiler to non-terminate if encoded as data structures \cite{spj:inlining}. However, in practice, we have been unable to take a type correct Haskell program and encode it to trigger an infinite sequence in the simplification stage. In order to ensure this cannot happen, we only perform $n$ (bind-lam) or (bind-box) rules upon an expression. If the expression is altered by other stages, we reset the count. Currently we have set $n$ to 1000, and have never had this limit reached.

\subsection{Termination of Arity Raising}

Todo.

\subsection{Termination of Inlining}

The standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions, such as Example \ref{ex:inlining_boxed_lambdas}. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = case  f of
          Box _ -> Box (\x -> x)
\end{code}

The |f| inside the case is a candidate for inlining:

\begin{code}
case f of Box _ -> Box (\x -> x)
    => -- inlining rule
case (case f of Box _ -> Box (\x -> x)) of Box _ -> Box (\x -> x)
    => -- (case-case) rule
case f of Box _ -> case Box (\x -> x) of Box _ -> Box (\x -> x)
    => -- (case-con) rule
case f of Box _ -> Box (\x -> x)
\end{code}

This expression could cause non-termination.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. Any future attempts to inline |f| within this function would be disallowed, although |f| could be inlined within other functions. This termination criteria is sound, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}
\label{sec:termination_specialisation}

The specialisation method, left unrestricted, does not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. We would generate an infinite number of specialisations of |f|.
\end{example}

\begin{figure}
\[\frac{s \unlhd t_i \text{ for some } |i|}{s \unlhd \sigma(t_1,\ldots,t_n)} \]

\[\frac{\sigma_1 \sim \sigma_2,
        s_1 \unlhd t_1, \ldots , s_n \unlhd t_n}
       {\sigma_1 (s_1,\ldots,s_n) \unlhd \sigma_2 (t_1,\ldots,t_n)} \]
\caption{Homeomorphic embedding relation.}
\label{fig:homeomorphic}
\end{figure}

To ensure we only specialise a finite number of times we use a homeomorphic embedding \cite{leuschel:homeomorphic}, given in Figure \ref{fig:homeomorphic}. The homeomorphic embedding has been used for termination of Supercompilation \cite{sorensen:supercompilation}. The homeomorphic embedding $\unlhd$ is a well-binary relation, meaning there are no infinite admissible sequences. A sequence $s_1,s_2 \ldots$ is admissible if there are no $i < j$ such that $s_i \unlhd s_j$. This property only holds provided all elements are expressions over a finite alphabet.

For each function, we associate a set $S$, of expressions. After generating a template $t$, we only specialise with that template if $\forall s \in S \bullet \neg(s \unlhd t)$. If $S$ is already admissible, then the sequence $S$ with $t$ added at the end is still admissible. After specialising with a template we add that template to the set $S$ associated with that expression. When we create a new function based on a template, we copy the $S$ associated with the function in which the specialisation is performed.

One of the conditions for termination of homeomorphic embedding is that there is only a finite alphabet. To help ensure this condition, we consider all variables and literals to be equal. However, this is not sufficient. During the process of specialisation we create new functions, and these new functions are new symbols in our language. Because the homemorphic embedding is being used to ensure that infinite functions are not created, we cannot assume this property when proving it. Instead we only refer to functions in the original input program. Every generated function has a correspondence with an expression in the original program. Before performing the homeomorphic embedding we transform all templates into their original equivalent, then template upon them.

Using homeomorphic embedding on the previous example, we would generate the specialised variant of |f (Value head)|. Upon attempting to generate the specialised variant |f (Wrap (Value head))| we would abort, with an embedding.

While homeomorphic embedding is sufficient to obtain defunctionalization in many examples, there do exist examples where it terminates prematurely.

\begin{example}
\begin{code}
main y = f (\x -> x) y
f x y = fst (x, f x y) y
\end{code}

Here we first generate a specialised variant of |f (\x -> x) y|.  If we call the specialised variant |f'|, we have:

\begin{code}
f' y = fst (\x -> x, f' y) y
\end{code}

Note that the recursive call to |f| has also been specialised. We now attempt to generate a specialised variant of |fst|, using the template |fst (\x -> x, f' y) y|. Unfortunately, this template is an embedding of the template we used for |f'|, and we do not specialise. If we permit specialisation, we reduce to:

\begin{code}
f' y = fst' y y
fst' y_1 y_2 = y_2
\end{code}

Note that the homoemorphic embedding has prevented us for making a first-order variant of the program.
\end{example}

While the above example may look slightly obscure, it occurs commonly with the standard desugaring of dictionaries. Often, typeclass dictionaries have default methods, which call other methods in the typeclass. These recursive typeclass calls often pass dictionaries, embedding the original caller -- even though no recursion actually happens.


To aleviate this problem, instead of storing one set $S$, we instead store a sequence of sets, $S_1$ through $S_n$ -- where $n$ is a small positive number, constant for the duration of the program. Instead of adding to the set $S$, we now added to the lowest set $S_i$ where adding the element will not violate the admissible sequence. Each of the sets $S_i$ is still finite, and there are a finite number ($n$) of them, so termination is maintained.

By default the firstification program uses 8 as the number of sets. In the results table given in \S\ref{sec:results}, for each program we have given the minimum possible value of $n$ to remove all lambda expressions. The disadvantage of a higher $n$ is that a program will unroll infinite recursion more times.

\subsection{Termination as a Whole}

We call a function |f| finite if it only applies a finite number of times, given a starting program. We call a function |f| terminating if |fix f| terminates. Given |f +||+ g|, if both |f| and |g| are finite then |f +||+ g| is finite. If one if finite, and the other is terminating, then the entire production is terminating.

We have shown that arity raising, inlining and specialisation are finite. The remaining step, simplification, is terminating. Therefore our entire program is terminating.

\section{Results}
\label{sec:results}

\begin{table}
\caption{Results of defunctionalization on the nofib suite.}
\label{tab:results}

\smallskip

\textbf{Name} is the name of the program;
\textbf{Bound} is the numeric bound use for the termination (see \S\ref{sec:termination_specialisation});
\textbf{HO Create} the number of lambda expressions and under-applied functions, first in the input program and then in the output program;
\textbf{HO Use} the number of application expressions and over-applied functions;
\textbf{Size} the change in the program size.

\smallskip\smallskip

\begin{tabular*}{\linewidth}{lrrrrrrlll}
\textbf{Name} & \textbf{Bound} & \multicolumn{2}{c}{\textbf{HO Create}} & \multicolumn{2}{c}{\textbf{HO Use}} & \textbf{Size} \\
\vspace{-1ex} \\
bernouilli      & 4 & 240 & 0 & 190 & 2 & -32\% \\
digits-of-e1    & 4 & 217 & 0 & 153 & 2 & -35\% \\
digits-of-e2    & 5 & 236 & 0 & 198 & 3 & -32\% \\
exp3\_8         & 4 & 232 & 0 & 154 & 2 & -39\% \\
gen\_regexps    & 7 & 116 & 0 &  69 & 0 & -31\% \\
integrate       & 4 & 348 & 0 & 358 & 2 & -38\% \\
paraffins       & 4 & 360 & 0 & 351 & 2 & -53\% \\
primes          & 4 & 217 & 0 & 148 & 2 & -38\% \\
queens          & 4 & 217 & 0 & 146 & 2 & -38\% \\
rfib            & 4 & 338 & 0 & 355 & 2 & -31\% \\
tak             & 4 & 212 & 0 & 145 & 4 & -40\% \\
wheel-sieve1    & 4 & 224 & 0 & 151 & 2 & -35\% \\
wheel-sieve2    & 4 & 224 & 0 & 162 & 2 & -36\% \\
x2n1            & 4 & 345 & 0 & 385 & 2 & -57\% \\
\multicolumn{8}{l} \ldots{}plus 35 tests from the spectral suite\ldots{} \\
Minimum         & 2 & 60 & 0 & 46 & 0 & -78\% \\
Maximum         & 14 & 437 & 1 & 449 & 100 & 15\% \\
Average         & 4.8 & 237 & 0.06 & 202 & 3.6 & -33\% \\
\hline
\end{tabular*}
\end{table}

We have tested out method with programs drawn from the nofib benchmark \cite{nofib}, and the results are given in Table \ref{tab:results}. We have tested with all 14 tests from the imaginary section (each shown in the table), and 35 out of the 47 tests in the spectral section. The remaining 12 tests in the spectral section do not compile using the Yhc compiler, mainly due to missing or incomplete libraries.

Most analysis operations work on an abstracted view of the program. For example, |readFile| in Yhc is implemented in terms of file handles and pointer operations -- while an analysis tool would probably give an abstraction for the |readFile| function. For the purposes of the firstification, we have worked on unmodified Yhc libraries, including all the low-level detail. Higher-order functions occur in many places, including:

\begin{itemize}
\item Typeclass dictionaries are implemented as tuples of functions.
\item The monadic bind operation is higher-order.
\item The IO data type is implemented as a function.
\item The Haskell |Show| type class uses continuation passing style extensively.
\item List comprehensions in Yhc are desugared to continuations. There are other translations which would probably be preferable if first-order code was desired \cite{wadler:list_comprehensions,coutts:stream_fusion}.
\end{itemize}

One torture test we have used during testing is |show (1 :: Double)|. The underlying implementation of |show| on |Double| makes use of arrays, which are implemented within the |IO| monad. Out of the tests, only 3 programs have residual functional values within them.

The integer program passes functional values to the primitive |seq|, similar to Example \ref{ex:functional_lists}, using the following function:

\begin{code}
seqlist [] = return ()
seqlist (x:xs) = x `seq` seqlist xs
\end{code}

This function is invoked with the IO monad, so the |return ()| expression is a functional value. It is impossible to remove this functional value without having access to the implementation of the |seq| primitive.

The pretty and constraints programs both apply a functional value to an expression that evaluates to |undefined|, similar to Example \ref{ex:undefined_values}. The case in pretty comes from the fragment:

\begin{code}
type Pretty = Int -> Bool -> PrettyRep

ppBesides     :: [Pretty] -> Pretty
ppBesides = foldr1 f nil
\end{code}

Here |ppBesides| may evaluate to |undefined| if the input list to |foldr1| is |[]|. Here the |undefined| value will be of type |Pretty|, and will be given further arguments, which can be functions. In reality, the code ensures that the input list is never |[]|, so the program will never fail with this error.

It would be possible to remove the lambda expression by treating the |undefined| value as a functional value, perhaps using the type system to detect which values have a function type.

The termination bound used on the programs varies from 2 to 11 within the sample programs. If we exclude the integer program, which is complicated by the primitive operations on functional values, the highest bound is 8. Most programs have a termination bound of 4. There is no realation to the size of a program and the termination bound.

On average the size of the resultant program is smaller by 33\%. We measured program size by taking the number of nodes in the abstract syntax tree, and the number of functions. We took the average of the two measures, which were typically very close to each other. Before counting the size we remove all functions which simply call on to other functions passing the same arguments, for example:

\begin{code}
f x y = g x y
\end{code}

Given this, we would replace all invokations of |f| with |g|. The reason for the decrease in program size is due to dictionaries holding references to unnecessary code.

The results show that the termination criteria we have chosen are sufficiently relaxed to enable firstification in most programs.

\section{Related Work}
\label{sec:related}

Reynolds style defunctionalization \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Reynolds method works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f a : map f as
\end{code}

\noindent Now all calls to |map head| are replaced by |map Head|.
\end{example}

Reynold's method works on all programs, Defunctionalized code is still type safe, but type checking would require a dependently typed language. Since the original work, people have proposed variants of Reynold's method that are type safe in the simply typed lambda calculus \cite{bell:type_driven_defunctionalization}, and within a polymorphic type system \cite{pottier:polymorhpic_typed_defunctionaization}.

We are unaware of any simple method for extending Reynold's style defunctionalization to primitives, without changes the primitives to be aware of the |Function| data type. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Reynold's method has been used as a tool in program calculation \cite{danvy:defunctionalization_at_work,hutton:calculating_an_exceptional_machine}, often as a mechanism for removing introduced continuations. Another use of Reynold's method is for optimisation \cite{grin,jhc}, allowing flow control information to be recovered without the complexity of higher-order transformation.

The closest work to ours is that of Chin and Darlington \cite{chin:higher_order_removal}, which itself is similar to that of Nelan \cite{nelan:firstification}. They define a higher-order removal method, with similar goals of removing functional values from a program. Their work shares some of the simplification rules, the arity raising and function specialisation. Despite these commonalities, there are great differences in how their method is structured.

\begin{itemize}
\item Their method makes use of the types of expressions, information that must be maintained and extended to work with additional type systems.
\item Their method does not have an inline step, or any notion of boxed lambdas. Functional values within constructors are ignored, and the authors suggest the use of deforestation \cite{wadler:deforestation} to help remove them. Deforestation is not tailored to this usage, and therefore transforms the program more than necessary, and fails to eliminate many functional values.
\item Their specialisation step does not work with boxed lambdas to functional values.
\item To ensure termination of the specialisation step, they never specialise a recursive function which does have any functional arguments passed identically in all recursive calls. This restriction satisfies higher-order functions such as |map|, but fails in many other cases.
\item By splitting our the simplification rules, our other stages are comparatively simple to theirs.
\end{itemize}

The limitations of their work are entirely understandable, given the nature of programming at the time their work was written. While nowadays people use monads, IO continuations and type classes as a matter of course, these features were much newer when the work was done. Our work can be seen as a successor to theirs, acheiving similar goals but with a greater complexity of input program. Our work acheives most of the aims set out in their future work section.

Some of their observations and extensions apply equally to our work, for example they suggest possible methods of removing accumulating functions such as in Example \ref{ex:functional_lists}. We have tried their examples, and can confirm that all of them are successfully handled by our system.

The specialisation and inlining steps are taken from existing program optimisers, as it the termination strategy of homeomorphic embedding. A lot of program optimisers remove some higher-order functions, such as partial evaluation \cite{jones:partial_evaluation} and supercompilation \cite{supercompilation}. We have certainly benefited from ideas in both these areas in developing our algorithms. Our initial attempt at removing functional values involved modifying a supercompiler \cite{me:supero}. Unfortunately this approach has a number of disadvantages. Firstly, the optimiser is not attempting to preserve correspondence to the original program, so will optimise all aspects of the program equally, instead of focusing on the higher-order elements. The other more serious problem was that the results were poor -- given the restricted setting of only removing functional values, the techniques can be pushed much further.

\section{Conclusions and Future Work}
\label{sec:conclusion}

Higher order functions are very useful to the programmer, but may pose difficulties for certain types of analysis. Using the method we have described, it is possible to remove most functional values from most programs. Our method has already found a practical user within the Catch tool, and we hope it can be of benefit to others.

The implementation presented in this paper is designed for clarity, not speed. By simply tracking which functions have changed, a massive performance could be obtained. The use of a numeric termination bound in the homeomorphic embedding is regretable, but highly practically motivated. We need further research to determine if such a numeric bound is necessary, or if other measures could be used, and what the bound should be.

\section*{Appendix A}

\begin{verbatim}
[x,y,z]
app(lam(x),y)    -> let(y,x)
app(case(x,y),z) -> case(x,app(y,z))
app(let(x,y),z)  -> let(x,app(y,z))
case(let(x,y),z) -> let(x,case(y,z))
case(con(x),y)   -> let(x,y)
case(x,lam(y))   -> lam(case(x,app(lam(y),var)))
let(lam(x),y)    -> lam(let(x,y))
\end{verbatim}


\bibliographystyle{plainnat}

\bibliography



\end{document}
