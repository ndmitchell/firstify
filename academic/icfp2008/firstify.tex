\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}

\include{paper}
%include paper.fmt
%format +|+ = "\mathbin{\ddagger}"
%format Lambda = "\Lambda{}"

%format Lambda a b = \a -> b
%format Fun a b = a \? b
%format Con a b = a \? b
%format App a b = a \? b
%format Var a = a
%format Case a b = case a of b
%format Let a b c = let a = b in c


\begin{document}

\conferenceinfo{ICFP '08}{date, City.} %
\copyrightyear{2008} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Defunctionalisation For Analysis}
\subtitle{ -- transforming away those pesky functional values}

\authorinfo{Neil Mitchell\titlenote{The first author is supported by an EPSRC PhD studentship}}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~ndm/}}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~colin/}}

\maketitle

\begin{abstract}
We describe an automated transformation which takes a higher-order program, and a produces an equivalent program with most functional values removed. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method has a number of advantages over Reynold's style defunctionalisation -- it is type safe within the Hindley-Milner framework, and the results are more amenable to subsequent analysis operations. While our method cannot always succeed in removing all functional values, in practice it usually manages to.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, transformation

\keywords
Haskell, first-order reduction, firstification, defunctionalisation, functional programming

\section{Introduction}

Higher-order functions are well used and much loved within functional programming languages such as Haskell \cite{haskell}. Their use is ubiquitous -- even a simple numeric literal in Haskell typically expands into a the application of a function taking a tuple containing many functional values -- as part of the dictionary transformation for the implementation of type classes \cite{wadler:typeclasses}. While having functions as first-class values leads to more concise code, it often complicates analysis methods, such as checking pattern-match safety \cite{catch} and termination checking \cite{termination_checking}.

\begin{example}
\label{ex:incList}
\begin{code}
incList :: Int -> Int
incList = map (+1)
\end{code}

Consider the above definition of |incList|, containing many higher-order features. The function |(+1)| is passed as a functional argument to |map|. The |incList| value is a functional value, bound to a function taking a list to a list. The use of first-class functions has lead to short code, but we could equally have written:

\begin{code}
incList :: Int -> Int
incList []      = []
incList (x:xs)  = x+1 : incList xs
\end{code}

While the second variant of |incList| is much longer, it may also be more amenable to certain types of analysis -- and indeed may also perform faster at runtime. The transformation pass presented in this paper performs the above transformation automatically.
\end{example}

Our first-order reduction method processes the program as a whole to remove the functional values, without changing the semantics of the program. This idea is not new, indeed as far back as 19** Reynold's gave a solution, known as Reynold's style defunctionalisation \cite{reynolds:defunctionalisation}. Unfortunately, Reynold's method effectively introduces a mini-interpreter within the resultant program, which causes problems for analysis tools. Our method instead tries to produce a program closer to what a human may have written, if not allowed the benefit of functional values.

Our method has been implemented in Haskell, and operates over the Core language from the York Haskell Compiler (Yhc). We have used our transformation within the Catch analysis tool, which checks for potential pattern-match errors in Haskell. In addition, we have made our method available as a library on Hackage\footnote{\url{http://hackage.haskell.org/}} -- and hope others from the termination checking community may be able to make use of it.

\subsection{Contribution}

Our paper makes a number of contributions:

\begin{enumerate}
\item We define three transformation steps to remove functional values.
\item We show how to apply these transformation steps, in particular to maximise the removal of functional values.
\item We define termination criteria to ensure termination of our method.
\item We present results from the nofib suite, including discussions of the limits of our transformation mechanism.
\end{enumerate}

\subsection{Roadmap}

This paper starts with a definition of our Core language \S?, including what we consider to be a higher-order function. Next we present out method in \S?, motivated by a series of examples. Next we classify where out method may fail to remove higher order functions. We then prove that our method is correct in \S?, and terminating in \S?. Finally we give results \S?, review related work \S? and conclude \S?.

\section{Our Core Language}

\begin{figure}
\begin{code}
f (CoreLet a b c) = a


func = f vs_ expr

expr  =  Lambda vs_ x    -- lambda abstraction
      |  Fun f xs_       -- function application
      |  Con c xs_       -- constructor application
      |  App x xs_       -- general application
      |  Var v           -- variable
      |  Case x alts_    -- case expression
      |  Let v x y       -- let expression

alt = c vs_ -> x
\end{code}

We let |v| range over variables, |x| over expressions, |f| over function names and |c| over constructors.
\caption{Core Language.}
\label{fig:core}
\end{figure}

Our Core language is presented in Figure \ref{fig:core} -- and has a number of differences from a standard Core language. A function in our Core language consists of a name, a list of arguments, and a body expression. The arity of a function is the number of arguments associated with it.

The variable, case, let, application and lambda expressions are much as they would be in any Core language. The constructor expression consists of a constructor and a list of expressions, exactly matching the arity of the constructor -- any constructors not given enough arguments can be wrapped within a lambda expression. The function expression consists of a function and a list of expressions -- a number which may be more, equal to, or less than the arity of the function.

A program can be said to be higher-order if at runtime the program generates and manipulates functional values. We define a program to be first-order if we can guarantee that at runtime it will create no functional values. Functional values can only be created in two ways: (1) a lambda expression; (2) an under-saturated function application. We define a first-order program to be one containing neither of these constructs.

In Example \ref{ex:incList}, our program is higher-order because of the under-saturation of both |map| and |(+)|. In the reduced version, the program is first-order.

We do not permit primitives in our language, but explain how to extend our method to them in \S?. We assume a root function, named |main|. We assume that if |main| takes any arguments, they are all first-order values, and that |main| evaluates to a first-order value. Again, we will explain how to loosen these restrictions in \S?.

We use |arity f|, |body f| and |args f| to denote the arguments, body and arity of f.


\section{Our First-Order Reduction Method}

Our method works by combining four separate and well-known transformations:

\begin{description}
\item[Simplification:] We use various standard functional programming simplification rules to perform local transformations within an expression.
\item[Arity Raising:] A function is arity raised \cite{arity_raising} if by local transformations the arity of a function can be changed, without changing its semantics.
\item[Inlining:] Inlining is a standard technique in optimising compilers \cite{spj:inline}, and has been studied at length.
\item[Specialisation:] Specialisation is another standard technique, used to remove type classes \cite{hudak:removing_type_classes} and more recently to specialisation functions to a given constructor \cite{spj:specconstr}.
\end{description}

Each transformation has the possibility of removing some functional values, but the key contribution of this paper is how they can be used together, to remove functional values. One important aspect is that each of these transformations is Hindley-Milner type preserving, and correctness preserving. None of these operations introduce any additional data structures. This is in contrast to Reynold's style defunctionalisation, which is not HM type preserving.

We proceed by first giving a brief flavour of how each operation can be used to remove some functional values. We then discuss in detail how each step can be specified, how they can be combined, and what restrictions are necessary.

\subsection{Simplification}

The simplification step serves to group all the short and simple transformations that most optimising compilers apply, typically as part of a simplification stage \cite{ghc_simplifier}. Some of these steps have the ability to remove functional values, while others simply ensure a normal form for future transformations.

\begin{example}
\begin{code}
one = (\x -> x) 1
\end{code}

The simplification rules transform this function to:

\begin{code}
one = let x = 1 in x
\end{code}
\end{example}

Other rules within the program do not eliminate lambda expressions, but manipulate them -- putting them in a form other stages can remove.

\begin{example}
\begin{code}
even = let one = 1 in \x -> not (odd x)
\end{code}

Here the let expression is surrounding the lambda expression. We can lift the lambda expression out of the let expression.

\begin{code}
even = \x -> let one = 1 in not (odd x)
\end{code}

In general this transformation may cause duplicate computation to be performed, an issue we return to in \S?.
\end{example}


\subsection{Arity Raising}

The arity raising transformation is an attempt to increase the arity of a function.

\begin{example}
\begin{code}
even = \x -> not (odd x)
\end{code}

Here the arity raising transformation changes the body of the function from being a lambda, and lifts the lambda arguments into the explicit arity of the function.

\begin{code}
even x = not (odd x)
\end{code}
\end{example}


\subsection{Inlining}

If a function returns a data constructor containing functional values, one way to remove these functional values is to use inlining.

\begin{example}
\begin{code}
f = case  eqInt of
          (a,b) -> a 1 2

eqInt = (primEqInt, primNeqInt)
\end{code}

In the above example the |eqInt| function evaluates to a tuple, where both |primEqInt| and |primNeqInt| are functional values. We can start to remove these elements by inlining the function |eqInt|:

\begin{code}
f = case  (primEqInt, primNeqInt) of
          (a,b) -> a 1 2
\end{code}

Next if we use the rule that the case scrutinee is a known constructor, we can reduce the program to a first-order variant. This rule is applied by the simplification stage.

\begin{code}
f = case  (primEqInt, primNeqInt) of
          (a,b) -> a 1 2
\end{code}
\end{example}

\subsection{Specialisation}

Specialisation works by replacing a function with a more specialised variant with some of its arguments passed at compile time.

\begin{example}
\begin{code}
map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys

notList xs = map not xs
\end{code}

Here the |map| function takes a functional value as its first argument. In the case of |notList| this functional value is |not|. We can create a variant of |map| specialised to |not| as its first argument:

\begin{code}
map_not x = case  x of
                  []    -> []
                  y:ys  -> not y : map_not ys

notList xs = map_not xs
\end{code}

The recursive call in |map| is replaced by a recursive call to the specialised variant. We have now eliminated all functional values.
\end{example}

\subsection{Goals}

Often these techniques will need to be used in combination, for example the |incList| example required simplification, arity raising and specialisation. The challenge is to apply these methods in a way that removes the most higher order functions.

In combining out methods we define have defined a number of goals, in order of priority.

\paragraph{Correct and terminating.} Like all transformation methods, both correctness and termination are important to make the transformation useful. By making use of three established transformations, correctness is relatively easy to show, and we do so in \S?. The issue of termination is much harder -- both inlining and specialisation can be applied in ways that will diverge. To ensure termination we have to restrict where these two methods can be applied, using termination criteria. We develop a set of termination criteria, along with a proof that they are sufficient, in \S?.

\paragraph{Preserve the type information.} Given an original program that can have a type inferred using the Hindley-Milner type system, we desire that the resultant program has the same property. Each of the stages we have presented have this property independently, so this goal is easy. We note that Reynold's method does not have this property.

\paragraph{Introduce no data values.} Reynold's method introduces a new data type that serves as a representation of functions, then embeds an interpreter for this data type. A natural desire would be to eliminate the higher-order aspects of a program, \textit{without} introducing any new data structures. Unfortunately, such a transformation is not possible.

Given a program, we can remove all data structures by Church encoding \cite{church_encode}. If we then had a transformation which made the program first-order \textit{without} introducing any data, we would end up with a program without data or closures, which is therefore incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

It is therefore clear that we will not be able to remove all functional values, and meet this goal. We choose therefore to not be complete. If a totally first-order program is required, Reynold's method can be always be applied to the result of our transformation.

By composing our transformation out of three existing transformations, none of which introduce data structures, we can easily ensure that our resultant transformation does not.

\paragraph{Correspondence to the original program.} Since our transformation designed to be performed before analysis, it is important that the results of the analysis can be presented in terms of the original program. We provide a method for transforming expressions in the resultant program into equivalent expressions in the original program.

\paragraph{Remove all functional values.} We have already indicated that the removal of all functional values is an impossible goal within our previous constraints. However, we will attempt to remove as many functional values as possible. In \S? we prove that our basic method is capable of removing all functional values, but that once our termination restrictions have been imposed, this is no longer true.

\paragraph{Preserve the space/sharing behaviour of the program.} In the expression |let y = f x in y + y|, according to the rules of lazy evaluation, |f x| will be evaluated at most once. We could inline the let binding to give |f x + f x|, but this expression evaluates |f x| twice. Where possible, we will avoid changing the sharing of the program. Our goals are primarily for analysis of the resultant code, not to compile and execute the result. Because we are not interested in performance, we permit the loss of sharing in computations, if to do so will remove functional values.

\section{Method in Detail}

Our method proceeds in four steps, simplification (|simplify|), arity raising (|arity|), inlining (|inline|) and specialisation (|specialise|). These steps are combined using a fixed point operator.

For the implementation, available at ..., we have implemented our steps within a monadic framework to deal with issues such as obtaining unique free variables and tracking termination constraints. To simplify the presentation, we have ignored these issues -- while these are tedious engineering issues, they do not effect the algorithm.

Our method is written as:

\begin{code}
firstify = simplify +|+ arity +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operator, defined as:

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g x  = fix (fix x . y)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x' then x else fix f x'
    where x' = f x
\end{code}

The |(+||+)| operator applies the first argument until it reaches a fixed point, then applies the second argument. If the second argument changes the value, the first argument is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Idempotent in each function] After the operation has completed, applying either |f| or |g| will not change the value.

\begin{code}
forall f g x `o` let r = (+|+) f g x in f r == r && g r == r
\end{code}

\item[Idempotent] The operation as a whole is idempotent.

\begin{code}
forall f g x `o` let r = (+|+) f g x in r == (+|+) f g r
\end{code}

\item[Function ordering] The function |f| will have reached a fixed point before the function |g| is applied. If a postcondition of |f| implies a precondition of |g|, then we can guarantee |g|'s precondition will always be met. If both |f| and |g| could be applied, |f| will be applied first.
\end{description}

The final property allows us to overlap the application sites between the two arguments, but guarantee the first will always be chosen. The other two properties ensure that when the operation finishes there will be no further sites where application could occur.

The operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((simplify +|+ arity) +|+ inline) +|+ specialise
\end{code}

Within this chain we guarantee that the end result will be idempotent with respect to any of the functions, and before any function is invoked, all those to the left of it will be idempotent.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent on its own, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent in all but the first case, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited. We return to discuss performance improvements in \S?.

We describe each of the stages in the algorithm separately. In all subsequent stages, we assume that all the simplification rules have been applied.


\subsection{Simplification}

\newcommand{\simp}[2]{\vspace{-7mm} #2 & (#1) \\}
\newenvironment{simplify}
    {\begin{flushright}
     \begin{tabular}{p{6.5cm}r}
    }
    {\end{tabular}
     \vspace{-7mm}
     \end{flushright}
    }

\begin{figure}
\begin{simplify}

\simp{app-app}{
\begin{code}
(x xs_) ys_
    => x xs_ ys_
\end{code}}

\simp{fun-app}{
\begin{code}
(f xs_) ys_
    => f xs_ ys_
\end{code}}

\simp{lam-lam}{
\begin{code}
\vs_ -> (\ws_ -> x)
    => \vs_ ws_ -> x
\end{code}}

\simp{lam-app}{
\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\simp{case-con}{
\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\simp{case-app}{
\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\simp{case-let}{
\begin{code}
case (let v = x in y) of alts_
    => let v = x in (case x of alts_)
\end{code}}

\simp{let-app}{
\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\end{simplify}
\caption{Standard Core simplification rules.}
\label{fig:simplify}
\end{figure}


\begin{figure}
\begin{simplify}

\simp{fun}{
\begin{code}
f xs_
    => \v -> f xs_ v
    where arity f > length xs_

\simp{case-lam}{
\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \w -> case x of {... ; c vs_ -> \v -> y ; ...} w
\end{code}}

\simp{bind-lam}{
\begin{code}
let v = \w -> x in y
    => y[v / \w -> x]
\end{code}}

\simp{bind-box}{
\begin{code}
let v = x in y
    => y[v / x]
    where x {-" \text{ is a boxed lambda (see \S?)} "-}
\end{code}}

\simp{let-lam}{
\begin{code}
let v = x in \w -> y
    => \w -> let v = x in y
\end{code}}

\end{simplify}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The simplification stage has the goal of moving lambdas upwards, and introducing lambdas for partially applied functions. This stage makes use of standard simplification rules (Figure \ref{fig:simplify}), which are found in most simplifiers. We also make use of additional rules which deal with lambda expressions, given in Figure \ref{fig:lambda_simplify}.

\subsubsection{Lambda Introduction}

The (fun) rule inserts lambdas where they can be. The original code from the Yhc compiler has been lambda lifted \cite{lambda_lift}, so contains no explicit lambda. This rule removes all occurrences of partial application, replacing them with explicit lambda expressions. The second stage moves these lambda expressions around, attempting to lift them to the top level.

\begin{example}
\begin{code}
even = (.) not odd
\end{code}

Here the functions |(.)|, |not| and |odd| are all partially applied. Lambda expressions can be inserted to saturate these applications.

\begin{code}
even = \x -> (.) (\y -> not y) (z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is applied to a number of arguments equal to its arity. This transformation will enable the arity raising transformation, resulting in:

\begin{code}
even x = (.) (\y -> not y) (z -> odd z) x
\end{code}
\end{example}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. This step trades one form of functional value for another form, but has the advantage of making functional values more explicit, and permitting arity raising.

In general, the insertion of a lambda and application is not semantics preserving:

\begin{code}
x => \v -> x v
\end{code}

Consider for instance |undefined `seq` 1|, which evaluates to |undefined|. Applying the lambda introduction rule we can replace the code with |(\v -> undefined v) `seq` 1|, which evaluates to |1|. Similarly, we could replace |1| with |\v -> 1 v|, which is neither semantics preserving nor type safe. However, a closely allied operation that is both semantics preserving and type safe is:

\begin{code}
\v -> x => \v' -> (\v -> x) v'
\end{code}

The lambda insertion phase can be seen as first transforming the function |f| using the lambda insertion rule, then inlining the outer lambda into the application site. These operations are semantics preserving, and standard within the field of program transformation.


\subsubsection{Lambda Movement}

The rule (case-lam) lifts a lambda out from within a case alternative to outside the case value. The (bind-lam) rule inlines a lambda bound in a let expression. The (bind-box) rule will be discussed as part of the inlining stage, see \S?. The (let-lam) rule is the one responsible for the largest loss of sharing, promoting a lambda expression outside a let, although (case-lam) can be used in a similar manner to reduce the sharing.

\begin{example}
\begin{code}
f x = let i = expensive x
      in \j -> i + j

main xs = map (f 1) xs
\end{code}

In the above example, |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x j = let i = expensive x
        in i + j
\end{code}

Now |expensive| will be recomputed for every element in |xs| -- potentially a very severe speed penalty.
\end{example}

The loss of sharing is not purely theoretical, the Uniplate library \cite{uniplate} makes use of a let within a lambda (see \S\ref{sec:optimise_playdata}) to keep a scoreboard which is computed once. The (let-lam) rule would make the scoreboard mechanism a severe performance penalty.

As we focus on functional value removal at the expense of sharing, we perform these operations when necessary.


\subsection{Arity Raising}

The arity raising step is simple:

\begin{code}
function vs_ = \v -> x
    => function vs_ v = x
\end{code}

Given a body which is a lambda expression, the arguments to the lambda expression can be lifted into the arguments for the function. This transformation requires the body to be a lambda expression, so additional transformations are necessary to introduce and move lambda expressions. If a function has its arity increased, it is likely that this will cause applications of it to become partially saturated, requiring lambda expressions to be reinserted.

In this particular instance, the introduction of lambda expressions changes the arity of |even| from 0 to 1, possibly requiring applications of |even| to become partially applied, causing |lambdas| to be invoked.


\subsection{Inlining}

We use inlining to remove functional values which are stored within a data structure -- for example |Just (\x -> id x)|. We refer to functional values inside data structures as \textit{boxed lambdas}, because they have been placed inside the value. If a boxed lambda is bound in a let expression, we substitute the let binding, using the (bind-box) rule from Figure ?. We only inline a function if two conditions both hold: (1) the function evaluates to a boxed lambda; (2) the function application occurs within a case scrutinee.

\begin{figure}
\begin{code}
isBoxedLambda (Con c xs_) = any isLambda xs_ || any isBoxedLambda xs_
isBoxedLambda (App x xs_) = isBoxedLambda x
isBoxedLambda (Let v x y) = isBoxedLambda y
isBoxedLambda (Case x alts_) = any isBoxedLambda (rhss alts_)
isBoxedLambda (Fun x) = isBoxedLambda (body x)
\end{code}
\caption{The |isBoxedLambda| function.}
\label{fig:boxed_lambda}
\end{figure}

We define an expression to be a boxed lambda if the |isBoxedLambda| check returns true. The |isBoxedLambda| as written may not terminate, however we can ensure it does by keeping a list of followed functions, and assuming False for each one we have already seen. This does not decrease the number of true results, but turns non-termination into False.

\begin{example}
The following examples either are, or are not, boxed lambdas.

\begin{code}
yes0 = [\x -> x]
yes1 = (Just [\x -> x])
yes2 = (let y = 1 in [\x -> x])
yes3 = [Nothing, Just (\x -> x)]
yes4 = yes0
yes5 = [yes0]
no0 = \x -> id x
no1 = [id (\x -> x)]
no2 = id [\x -> x]
\end{code}

In this example, we are permitted to enclose the |yes| expressions within a boxed lambda, but none of the others. In |no0| there is no constructor, so the first condition does not apply. In |no1| there is a constructor, but the lambda expression is not an argument to the constructor. In |no2| there is a lambda expression as an argument to a constructor, but that subexpression is an argument to a function application.

Note that the function |no2| evaluates to a boxed lambda, but that this information is hidden by the |id| function. We do not analyse within the |id| function, but assume that specialisation will deal with any boxed lambdas passed to functions.
\end{example}

We inline if the expression is a boxed lambda, more precisely, we make the following substitution:

\begin{simplify}

\simp{case-boxed}{
\begin{code}
case (f xs_) alts_
    => case (let args f = xs_ in body f) alts_
    where isBoxedLambda body
\end{code}}

\end{simplify}

The |inline| rule deals with situations where functions evaluate to a data structure containing functional values. This situation occurs regularly with the standard dictionary implementation, but rarely in other situations. The inline rule does not actually remove functional values, but can bring their use and creation closer together, and thus helps them be removed.

The inline stage relies on all functional values being represented by a lambda expression, something that the simplification stage ensures.

There may be some loss of sharing with the inline rule, if a CAF is inlined.

\subsection{Specialisation (|special|)}

The original Catch tool \cite{me:catch_tfp} uses specialisation to remove higher-order functions. For each application of a function to functional arguments, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applied where function arguments are partially applied functions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have functional arguments, and generate templates, omitting first-order components.
\item For each template, generate an associated function, specialised to the template.
\item For each subexpression matching a template, replace it with the associated function.
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The specialisation first finds the application of |map| within |main|, and generates the template |map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates the name |map_id| for the template, and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code has no functional values within it.
\end{example}

\subsubsection{Generating Templates}

A template is generated if an expression is an application to a top-level function, whose arguments include a sub-expression which is either a lambda expression or a boxed lambda. The template includes all sub-expressions whose removal would lead to higher-order elements, or which have free variables from a bound variable.

\begin{example}
\begin{code}
id (\x -> x)              => id (\x -> x)
id (Maybe (\x -> x))      => id (Maybe (\x -> x))
id (Maybe (\x -> x + 3))  => id (Maybe (\x -> x + _))
id (Maybe (\x -> x + y))  => id (Maybe (\x -> x + _))
\end{code}

In all three examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, the |3| and |y| are not dependent on variables bound within the lambda, and are left as unspecified.  The |Maybe| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. Each |_| within the template is assigned a free variable, as an argument to the new function, then the body is produced by unfolding the outer function symbol in the template once.

\begin{example}
\label{ex:map_id}
Following the |map (\x -> x) _| template from above, we can generate |v_1| as the unique free variable for the single |_| placeholder, and |map_id| as the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

In the next step, we unfold the definition of map once:

\begin{code}
map_id v_1 = let  f   = \x -> x
                  xs  = v_1
             in   case  xs of
                        []    -> []
                        y:ys  -> f y : map f ys
\end{code}

Now the generation of the specialised variant is complete. To give an idea of how the final function is calculated, after the simplification rules introduced in Figure \ref{fig:lambda_simplify}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{example}

\subsubsection{Using Templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an undecided element is passed as an argument. Continuing with the generated code from Example \ref{ex:map_id}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We have now eliminated all the functional values from within this operation.

\subsection{Extending to Primitives}

The specialisation is the only section that requires extending when working within a language containing primitives. The only restriction is that if a function to be specialised is actually a primitive, no template should be generated. The reason for this is because the generation of code associated with a template requires a one-step unfolding of the function, something which cannot occur with a primitive.

\section{Examples}

To show the boundaries of our method, and the reason certain decisions have been made, we now show several examples.

\begin{examplename}{Dead Code}
\begin{code}
nothing = Nothing

example1 = case  nothing of
                Nothing  -> 1
                Just f   -> f (\x -> x)

example2 = case  Nothing of
                 Nothing  ->  1
                 Just x   ->  x 1
\end{code}

In |example1| the |Nothing| branch will always be taken due to the known case scrutinee. The |Just| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. Our method cannot remove this lambda. In |example2| the value |x| is taken from a data constructor, when we know no value will ever match.
\end{examplename}

\begin{examplename}{Undefined Values}
\begin{code}
example1 = x (\y -> y)
    where x = x

example2 = (error "bang") (\x -> x)

example3 = error "bang" 42

example4 = x 42
    where x = x
\end{code}
 
In |example1| and |example2| the a lambda is applied to |undefined|, so the program will terminate before the lambda is evaluated. In |example3| the |error| primitive is being given two arguments, even though at first glance it appears to only take one. The is possible because the type of error is |String -> alpha|, where |alpha| is completely unconstrained and can be anything, including a functional type. In |example3| the value |42| is being passed to |x|. At runtime |x| will not terminate, and semantically reduces to |undefined|. We note that |error| is a primitive, unless defined using non-termination.
\end{examplename}

\begin{examplename}{Root function returning functional values}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from zero to one. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type |IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> _E alpha)
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to |_E alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{examplename}

\begin{examplename}{Root function taking functional values}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are specialising, we cannot change its interface.
\end{examplename}

\begin{examplename}{Primitives}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type |alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Some primitives not only permit functional values, but actually \textit{require} them. The |primCatch| function within the Yhc standard libraries implements the Haskell exception handling function |catch|. Tye type of |primCatch| is |alpha -> (IOError -> alpha) -> alpha|, taking an exception handler as one of the arguments.
\end{examplename}

\begin{examplename}{Functional Lists}
Sometimes lambda expressions are used to build up lists which can have elements concatenated onto the end. Using Hughes lists (citation from worker/wrapper paper), we can define:

\begin{code}
nil = id
snoc x xs = \ys -> xs (x:ys)
list xs = xs []
\end{code}

This list representation provides |nil| similarly to |[]| in standard lists, but instead of providing a |(:)| or ``cons'' operation, it provides |snoc| which adds a single element on to the end of the list. The function |list| is provided to create an original list. We are unable to first order reduce such a construction, as it stores the values within the lambda, requiring an infinite amount of lambdas. We have seen such constructions in both the lines function of hscolour, and the sort function of Yhc.

However, there is an alternative implementation of these functions:

\begin{code}
nil = []
snoc = (:)
list = reverse
\end{code}

We have benchmarked these operations in a variety of settings and the list based version appears to use approximately 75\% of the memory, and 65\% of the time. We suggest that people using continuations for |snoc| move instead to a list type.
\end{examplename}


\begin{examplename}{Inlining Boxed Lambdas}
Our original program inlined boxed lambdas everywhere they occured. This makes the detection of boxed lambdas much simpler, and does not require looking into external functions. However, it is unable to cope with certain functions:

\begin{code}
main11 xs = app (gen xs)
app = map ($ 1)
gen = map (const x)
\end{code}

Consider the above code. The function |gen| returns a boxed lambda. However, the code is recursive, so if it was inlined repeatedly, it would not terminate. After deciding to restrict the inlining of gen, we are still left with lambdas. However, by first specialising |app| with respect to |gen|, we are able to remove the functional values. The removal of the lambda expressions has effectively lead to a deforesting between the |gen| producer of cons cells, and the |app| consumer -- eliminating the intermediate lambda expressions in the same step.
\end{examplename}



\begin{code}
ascending  b (a:) xs

ascending a as (b:bs)
  | a `cmp` b /= GT = ascending b (\ys -> as (a:ys)) bs
ascending a as bs   = as [a]: sequences bs
\end{code}

After:

\begin{code}
ascending  b [a] xs

ascending a as (b:bs)
  | a `cmp` b /= GT = ascending b (a:as) bs
ascending a as bs   = rev2 as [a] : sequences bs
\end{code}

\subsection{Dictionary Repeating}

\begin{code}
main6_5 y = ma id y
ma x y = fst (x, ma x) y
\end{code}

We tried blurring, but that doesn't work either.

\subsection{Let's interferring with inlining}

\begin{code}
f x = case (let v = 1 in Lambda_Box) of ...
\end{code}

actually occurs in circsim. The case/let rule fixes this.





\section{Proof of Completeness}

\todo{This proof sucks -- even if you remove simplify, the inlining of lambdas ensures you non-terminate. Or rather, the inlining rule assumes there is a context to inline into, which is not the case if you miss simplifications and promote them to the root.}

The algorithm we have presented is complete, in the sense that if it terminates, there will be no remaining lambda expressions within the program, other than those identified bellow. We have already shown that it is impossible to have both completeness and not introduce data structures (which our algorithm does not), therefore it follows that our algorithm is \textit{not} terminating. We shall return to the issue of termination in \S\ref{sec:firstify_terminate}, but assuming that our algorithm does terminate, it is possible to show that there are no remaining functional values.

We originally identified three causes of incompleteness when converting a program to first-order, detailed in \S\ref{sec:first_order_restrictions}. Two of those can be removed by restricting the root function to take no functional arguments and return a first-order value. The only remaining necessary cause is passing a lambda expression to a primitive function. Our method introduces one final instance of incompleteness -- passing a lambda to an expression that evaluates to |undefined|, caused by either a call to the |error| primitive, or by non-termination.

We can prove that, other than the restrictions introduced above, there are no resultant functional values created. We assume for the purposes of this proof that all functions are non-primitive.

\newenvironment{lemma}[1]
    {\paragraph{Lemma:} #1 \textbf{Proof:} }
    {\hfill$\Box$}

\begin{lemma}{There is no partial application in the resultant code.}
After our algorithm terminates, the resultant code must be idempotent with respect to each of the four stages presented. The |lambdas| stage ensures there will be no partial application by inserting explicit lambda expressions.
\end{lemma}

\begin{lemma}{The root expression of each top-level function is not a lambda.}
If a top-level function is bound to a lambda expression, then the arity of the function is increased and the lambda is no longer the root expression. Another way of expressing this property is that all lambda expressions must have a parent expression.
\end{lemma}

\begin{lemma}{The parent of a lambda expression must be an application, applied to a variable.}
In all other cases, one of the rules will transform the expression. To enumerate the possible parent expressions:

\begin{description}
\item[lambda abstraction:] the desugaring rule for lambda abstractions would combine them to one lambda abstraction.
\item[let binding:] the (let-lam) and (bind-lam) rules applied as part of |simplify| ensure these expressions are removed.
\item[case expression:] the (case-lam) rule will remove any lambda abstractions from the alternatives of a case expression. The static typing in the original Core language ensures that a lambda cannot be the scrutinee of a case expression.
\item[application:] the (lam-app) rule ensures that the lambda abstraction must be one of the arguments to the application. We now enumerate all the possible applied values within the application:
    \begin{description}
    \item[constructor:] Either the expression is the child of a function application, in which case the |special| stage will remove it, or it is not, in which case the |inline| stage will remove it.
    \item[function:] The |special| stage will specialise the function with respect to the lambda abstraction.
    \item[application:] The application desugaring rules will combine the application with the current one, removing this situation.
    \item[let binding:] The (let-app) rule applies.
    \item[case expression:] The (case-app) rule applies.
    \end{description}

    The only possibility not covered is that there is an application whose applied expression is a variable.
\end{description}
\end{lemma}

\begin{lemma}{All applications of a variable to a lambda expression evaluate to |undefined|.}
Given an expression |v (\v' -> x)|, the variable |v| must evaluate to |undefined|. It is clear from the type rules of our Core language that the variable |v| must either evaluate to |undefined|, or to a lambda expression. If it evaluates to a lambda expression, there must be a lambda expression within the program, since we are assuming no primitive functions or functional arguments to the root function. If such a lambda exists, it must be itself under an application to a variable, since all other possibilities have been eliminated. Given many such enclosing |v| variables, we must evaluate one first. The first one we evaluate cannot evaluate to a lambda expression, as all lambda expressions must result from first evaluating a variable, and since this is the first variable to be evaluated, that is not possible. The only other case is that the variable evaluates to |undefined|.
\end{lemma}


\section{Proof of Correctness}

To prove the correctness of the whole argument it is sufficient to show the correctness of each step. The simplification rules are all standard within lazy Core languages. The inlining and specialisation stages are also based on well accepted transformations. the only interesting case, from a correctness perspective, is lambda insertion.



\section{Proof of Termination}
\label{sec:firstify_terminate}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.


\subsection{Termination of Inlining}

The standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = (\x -> x) : f
\end{code}

The expression bound to |f| will repeatedly grow in size as inlining is applied, and will remain a candidate for inlining.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. The resultant code would be:

\begin{code}
f = (\x -> x) : (\x -> x) : f
\end{code}

Any future attempts to inline |f| within this function would be disallowed, although |f| could be inlined within other functions. This termination criteria is sound, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}

The termination of specialisation can be achieved by using a homeomorphic embedding \ref{sec:homeomorphic}.

The termination of specialisation is more tricky than inlining. We permit the inlining

A natural extension of specialisation is to take the fixed point, eliminating unsaturated expressions in generated functions. Unfortunately, such an algorithm would not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. We would generate an infinite number of specialisations of |f|.
\end{example}

One simple way to prevent such non-termination is to have a bound on the number of specialisations. Another approach is to use a homeomorphic embedding. All functions relate to some original expression in the Core language, if the expression to be generated was a homeomorphic embedding of an already specialised expression, we can stop.

Using homeomorphic embedding on the previous example, we would generate the following specialised variants of |f (Value head)| and |f (Wrap (Value head))|. Upon attempting to generate the specialised variant |f (Wrap (Wrap (Value head)))| we would abort, with an embedding of |f (Wrap (Value head))|.

\subsection{Termination as a Whole}





\section{Results}

Our preferred method for higher-order function removal is to apply specialisation and inlining interleaved. We have tried our method on the nofib suite, and have the following results.


\section{Reynolds style defunctionalization}

Reynolds style defunctionalization \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Defunctionalization works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f a : map f as
\end{code}

\noindent Now all calls to |map head| are replaced by |map Head|.
\end{example}

This method naturally extends to partial application. To take a more complicated example, where higher-order functions are being used to store information:

\begin{example}
\begin{code}
type Map = String -> Int

new :: Map
new _ = 0

get :: String -> Map -> Int
get key mp = mp key

add :: String -> Int -> Map -> Map
add key val mp s = if s == key then val else get key mp

test = get "foo" (add "bar" 4 (add "baz" 2 new))
\end{code}

\noindent The above code creates a functional map, which uses a higher-order function to store a mapping from |String| to |Int|. The |add| function inserts a new key/value pair into the map. This is transformed with defunctionalization to:

\begin{code}
data Function  =  New
               |  Add3 String Int Function

apply  New                 x = new x
apply  (Add3 y_1 y_2 y_3)  x = add y_1 y_2 y_3 x

new _ = 0

get key mp = apply mp key

add key val mp s = if s == key then val else get key mp

test = get "foo" (Add3 "bar" 4 (Add3 "baz" 2 New))
\end{code}

Here we use the constructor |Add3| to represent the |add| function with three arguments pre-applied. Note that the |Function| data type now serves to store a linked-list of the values with |New| serving a similar role to |[]|, and |Add3| storing one key/value pair along with the remainder of the list.
\end{example}

We are unaware of any simple method for extending Reynold's style defunctionalisation to primitives, without changes the primitives to be aware of the |Function| data type. Defunctionalized code is still type safe, but type checking would require a dependently typed language. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Reynold's method has been used as a tool in program calculation \cite{graham_hutton_calculating_an_exceptional_machine, another_one, another_one}, often as a mechanism for removing introduced continuations. Another use of Reynold's method is for optimisation \cite{grin,jhc}, allowing flow control information to be recovered without the complexity of higher-order transformation.



\section{Conclusions}

We have developed a method for removing functional values from a program. Our method has good results, and has found practical use within the Catch tool.



\bibliographystyle{plainnat}

\bibliography



\end{document}
