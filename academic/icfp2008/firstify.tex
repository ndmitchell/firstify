\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}

\include{paper}

%include paper.fmt
%format +|+ = "\mathbin{\ddagger}"
%format &+ = "\mathbin{\cup}"
%format &* = "\mathbin{\cap}"

%format _a = "\mathit{a}"
%format _b = "\mathit{b}"
%format _e = "\mathit{e}"
%format _s = "\mathit{s}"
%format _l' = "\mathit{l}^\prime{}"
%format _b' = "\mathit{b}^\prime{}"

%format let_ = "\mathsf{let}"
%format case_ = "\mathsf{case}"

%format (Lambda a (b)) = "\ll{}" \a -> b "\rr{}"
%format (Fun a b) = "\ll{}" a \? b "\rr{}"
%format (Con a b) = "\ll{}" a \? b "\rr{}"
%format (App a b) = "\ll{}" a \? b "\rr{}"
%format (Var a) = "\ll{}" a "\rr{}"
%format (Case a b) = "\ll{}\!\!" case a of b "\rr{}"
%format (Let a b c) = "\ll{}\!\!" let a = b in c "\rr{}"

\renewcommand{\ll}{[\![}
\newcommand{\rr}{]\!]}

\newcommand{\simp}[2]{\vspace{-7mm} #2 & (#1) \\}
\newenvironment{simplify}
    {\noindent
     \begin{flushright}
     \begin{tabular}{p{6.5cm}r}
    }
    {\end{tabular}
     \vspace{-7mm}
     \end{flushright}
    }

\newenvironment{definition}
    {\textbf{Definition:}}
    {\noexample}

\begin{comment}
% for paper haskell
\begin{code}
f = undefined
\end{code}
\begin{code}
primNeqInt, primEqInt :: Int -> Int -> Bool
\end{code}
\end{comment}

\begin{document}

\conferenceinfo{ICFP '08}{date, City.} %
\copyrightyear{2008} %
\copyrightdata{[to be supplied]}

\titlebanner{\today{} - \currenttime{}}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Losing functions without gaining data}
\subtitle{ -- a new method for defunctionalisation}

\authorinfo{Neil Mitchell\titlenote{The first author is supported by an EPSRC PhD studentship}}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~ndm/}}
\authorinfo{Colin Runciman}
           {University of York, UK}
           {\url{http://www.cs.york.ac.uk/~colin/}}

\maketitle

\begin{abstract}
We describe an automated transformation which takes a higher-order program, and a produces an equivalent first-order program. Unlike Reynolds style defunctionalisation it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method cannot always succeed in removing \textit{all} functional values, but in practice it is remarkably successful. We have evaluated our method on programs from the nofib benchmark, where only 3 out of 49 programs remain higher-order.
\end{abstract}

\category{D.3}{Software}{Programming Languages}

\terms
languages, transformation

\keywords
Haskell, firstification, defunctionalisation

\section{Introduction}

Higher-order functions are widely used in functional programming languages.  Having functions as first-class values leads to more concise code, but it often complicates analysis methods, such as those for checking pattern-match safety \cite{me:catch_icfp} or termination \cite{sereni:higher_order_termination}.

\begin{example}
\label{ex:incList}

Consider this definition of |incList|:

\begin{code}
incList :: [Int] -> [Int]
incList = map (+1)

map :: (alpha -> beta) -> [alpha] -> [beta]
map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

The definition of |incList| has higher-order features. The function |(+1)| is passed as a functional argument to |map|. The |incList| value is a functional value, bound to a function taking a list to a list. The use of first-class functions has led to short code, but we could equally have written:

\begin{code}
incList :: [Int] -> [Int]
incList []      = []
incList (x:xs)  = x+1 : incList xs
\end{code}

Although this first-order variant of |incList| is much longer, it is also more amenable to certain types of analysis -- and indeed may also perform faster at runtime. The method presented in this paper transforms the higher-order definition into the first-order one automatically.
\end{example}

Our defunctionalisation method processes the whole program to remove functional values, without changing the semantics of the program. This idea is not new. As far back as 1972 Reynolds gave a solution, now known as \textit{Reynolds style defunctionalisation} \cite{reynolds:defunc}. Unfortunately, Reynolds method effectively introduces a mini-interpreter, which causes problems for analysis tools. Our method produces a program closer to what a human might have written, if denied the use of functional values.

Our method has been implemented in Haskell \cite{haskell}, and operates over the Core language from the York Haskell Compiler \cite{me:yhc_core}. We have used our transformation within the Catch analysis tool \cite{me:catch_icfp}, which checks for potential pattern-match errors in Haskell. We have made our defunctionalisation method available as a library on Hackage\footnote{\url{http://hackage.haskell.org/}, under ``firstify''}.

\subsection{Contributions}

Our paper makes the following contributions:

\begin{itemize}
\item We define a defunctionalisation method which, unlike some previous work, does not introduce new types.
\item Our method can deal well with the complexities of a language like Haskell, including type classes, continuations and monads.
\item Our method makes use of standard transformation steps, but combined in a novel way.
\item We identify restrictions required to guarantee termination, without restricting defunctionalisation.
\item We have implemented our method, and present measured results for much of the nofib benchmark suite.
\end{itemize}

\subsection{Roadmap}

The sections following begin with a definition of our Core language (\S\ref{sec:core}), including what we consider to be a first-order program. Next we present an overview of our method (\S\ref{sec:overview}), followed by a more detailed account (\S\ref{sec:detailed}), along with a number of examples (\S\ref{sec:examples}). We classify where functional values may remain in a resultant program (\S\ref{sec:completeness}) and show how to modify our method to guarantee termination (\S\ref{sec:termination}). Finally we give results (\S\ref{sec:results}), review related work (\S\ref{sec:related}) and conclude (\S\ref{sec:conclusion}).

\section{Core Language}
\label{sec:core}

\begin{figure}
\ignore\begin{code}
prog   =  [func]           {-"\text{\hspace{1cm}program definition}"-}

func   =  f vs_ expr       {-"\text{\hspace{1cm}function definition}"-}

expr   =  Lambda vs_ x     {-"\text{\hspace{1cm}lambda abstraction}"-}
       |  Fun f xs_        {-"\text{\hspace{1cm}function application}"-}
       |  Con c xs_        {-"\text{\hspace{1cm}constructor application}"-}
       |  App x xs_        {-"\text{\hspace{1cm}general application}"-}
       |  Var v            {-"\text{\hspace{1cm}variable}"-}
       |  Let v x y        {-"\text{\hspace{1cm}let expression}"-}
       |  Case x alts_     {-"\text{\hspace{1cm}case expression}"-}

alt    =  c vs_ -> x       {-"\text{\hspace{1cm}case alternative}"-}
\end{code}
\begin{comment}
% for paper haskell
\begin{code}
data Expr = Lambda [String] Expr
          | Fun String [Expr]
          | Con String [Expr]
          | App Expr [Expr]
          | Var String
          | Case Expr [Alt]
          | Let String Expr Expr

data Alt = Alt String [String] Expr

arity :: String -> Int
body :: String -> Expr
args :: String -> [String]
rhs :: Alt -> Expr
\end{code}
\end{comment}

We let |v| range over variables, |x| over expressions, |f| over function names and |c| over constructors.
\caption{Core Language.}
\label{fig:core}
\end{figure}

Our Core language is presented in Figure \ref{fig:core}. A program is a set of functions, with a root function named |main|. A function definition consists of a name, a list of argument variables, and a body expression. The arity of a function is the number of arguments in its definition. We initially assume there are no primitive functions in our language, but explain how to extend our method to deal with them in \S\ref{sec:primitives}.

The variable, case, let, application and lambda expressions are much as they would be in any Core language. The constructor expression consists of a constructor and a list of expressions, exactly matching the arity of the constructor. (Any partially applied constructor can be represented as a lambda expression.) A named application consists of a function name and a list of argument expressions -- a number which may be less, equal to, or more than the arity of the function. If a function is given too few arguments, we refer to it as \textit{partially-applied}, matching the arity is \textit{fully-applied} and more than the arity is \textit{over-applied}. We use the meta functions |arity f|, |body f| and |args f| to denote the arguments, body and arity of |f|. We use the function |rhs| to extract the expression on the right of a case alternative.

Informally, a program is higher-order if at runtime the program generates and manipulates functional values. As functional values can only be created in two ways; (1) a lambda expression and (2) a partially-applied function application, we make the following definition:

\begin{definition}
A program which contains no lambda expressions and no partially-applied functions is first-order.
\end{definition}

\begin{examplerevisit}{\ref{ex:incList}}
The original program is higher-order because of the under-saturation of both |map| and |(+)|. In the defunctionalised version, the program is first-order.
\end{examplerevisit}

\todo{Clarify use vs creation of functional values} In general, we want to assume that general application is first-order, even though the first argument must be a functional value. Refer to this later in the results.

\section{Our First-Order Reduction Method}
\label{sec:overview}

Our method works by combining three separate and well-known transformations. Each transformation on its own is correctness preserving, and none introduce any additional data types. Additionally we use general simplification rules, most of which may be found in any optimising compiler \cite{spj:transformation}.

\begin{description}
\item[Arity Raising:] A function can be arity raised if the body of the function is a lambda expression. In this situation, the variables within the lambda expression can be added as arguments to the function.
\item[Inlining:] Inlining is a standard technique in optimising compilers \cite{spj:inlining}, and has been studied in depth.
\item[Specialisation:] Specialisation is another standard technique, used to remove type classes \cite{jones:dictionary_free} and more recently to specialise functions to a given constructor \cite{spj:specconstr}.
\end{description}

Each transformation has the possibility of removing some functional values, but the key contribution of this paper is \textit{how they can be used together}. Using the fixed point operator |(+||+)| introduced in \S\ref{sec:detailed}, their combination is:

\begin{code}
firstify = simplify +|+ arity +|+ inline +|+ specialise
\end{code}

We proceed by first giving a brief flavour of how these transformations used in isolation may remove some functional values. We then discuss them in detail in \S\ref{sec:detailed}, including how they can be combined.

\todo{If possible, come up with a running example -- hard to find one which hits them all and isn't too complex.}

\subsection{Simplification}

Simplification serves to group several simple transformations that most optimising compilers apply. Some of these steps have the ability to remove functional values; others simply ensure a normal form for future transformations.

\begin{example}
\begin{code}
one = (\x -> x) 1
\end{code}

One of our simplification rules transforms this function to:

\begin{code}
one = let x = 1 in x
\end{code}
\end{example}

Other rules do not eliminate lambda expressions, but put them into a form that other stages can remove.

\begin{example}
\begin{code}
even =  let one = 1
        in \x -> not (odd x)
\end{code}

A simplification rule lifts the lambda outside of the let expression.

\begin{code}
even = \x ->  let one = 1
              in not (odd x)
\end{code}

In general this transformation may cause duplicate computation to be performed, an issue we return to in \S\ref{sec:sharing}.
\end{example}


\subsection{Arity Raising}

The arity raising transformation increases the definition arity of functions with lambdas as bodies.

\begin{example}
\begin{code}
even = \x -> not (odd x)
\end{code}

Here the arity raising transformation lifts the argument to the lambda into a definition-level argument, increasing the arity.

\begin{code}
even x = not (odd x)
\end{code}
\end{example}


\subsection{Inlining}

We use inlining to remove functions which return data constructors containing functional values. A frequent cause of data constructors containing functional values is due to the dictionary implementation of type classes \cite{wadler:type_classes}.

\begin{example}
\begin{code}
main = case  eqInt of
             (a,b) -> a 1 2

eqInt = (primEqInt, primNeqInt)
\end{code}

Both components of the |eqInt| tuple, |primEqInt| and |primNeqInt|, are functional values. We can start to remove these functional values by inlining |eqInt|:

\begin{code}
main = case  (primEqInt, primNeqInt) of
             (a,b) -> a 1 2
\end{code}

The simplification stage can now turn the program into a first-order variant, using a rule dealing with a case scrutinising a known constructor.

\begin{code}
main = primEqInt 1 2
\end{code}
\end{example}

\subsection{Specialisation}

Specialisation works by replacing a function application with a specialised variant. In effect, at least one argument is passed at transformation time.

\begin{example}
\begin{code}
notList xs = map not xs
\end{code}

Here the |map| function takes the functional value |not| as its first argument. We can create a variant of |map| specialised to this argument:

\begin{code}
map_not x = case  x of
                  []    -> []
                  y:ys  -> not y : map_not ys

notList xs = map_not xs
\end{code}

The recursive call in |map| is replaced by a recursive call to the specialised variant. We have eliminated all functional values.
\end{example}

\subsection{Goals}
\label{sec:goals}

We have defined a number of goals: some \textit{essential}, and others \textit{desirable}. If essential goals make desirable goals unachievable, we still aim to do the best we can.

\subsubsection*{Essential}

\paragraph{Preserve the result computed by the program.} By making use of three established transformations, correctness is relatively easy to show.

\paragraph{Ensure the transformation terminates.} The issue of termination is much harder. Both inlining and specialisation could be applied in ways that diverge. In \S\ref{sec:termination} we develop a set of criteria to ensure termination.

\paragraph{Correspondence to the original program.} Our transformation is designed to be performed before analysis. It is important that the results of the analysis can be presented in terms of the original program. We need a method for transforming expressions in the resultant program into equivalent expressions in the original program.

\paragraph{Introduce no data types.} Reynolds method introduces a new data type that serves as a representation of functions, then embeds an interpreter for this data type into the program. We aim to eliminate the higher-order aspects of a program, \textit{without} introducing any new data types. By composing our transformation out of existing transformations, none of which introduce data types, we can easily ensure that our resultant transformation does not introduce data types.


\subsubsection*{Desirable}

\paragraph{Remove all functional values.} Given a program, we can remove all data types by encoding them as functions, as described in \citet{naylor:reduceron}. If we then had a transformation which made the program first-order \textit{without} introducing any data types, we would end up with a program without data or closures, which is incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

We aim to remove as many functional values as possible. In \S\ref{sec:completeness} we make precise where functional values may appear in the resultant programs. If a totally first-order program is required, Reynolds method can be always be applied after our transformation.

\paragraph{Preserve the space/sharing behaviour of the program.} In the expression |let y = f x in y + y|, according to the rules of lazy evaluation, |f x| will be evaluated at most once. We could inline the let binding to give |f x + f x|, but this expression evaluates |f x| twice. Where possible, we will avoid changing the sharing of the program. Our goals are primarily for analysis of the resultant code, not to compile and execute the result. Because we are not interested in performance, we permit the loss of sharing in computations, if to do so will remove functional values.

\paragraph{Minimize the size of the program.} Previous defunctionalisation attempts have shown a concern about code size increase \cite{chin:higher_order_removal}. A smaller resultant program would be desirable, but not at the cost of clarity.

\paragraph{Make the transformation fast.} The implementation must be fast enough to permit proper evaluation.


\section{Method in Detail}
\label{sec:detailed}

\begin{comment}
% paper haskell
\begin{code}
data Prog = Prog deriving Eq
simplify,arity,inline,specialise :: Prog -> Prog
\end{code}
\end{comment}

Our method proceeds in four iteratively nested steps, simplification (|simplify|), arity raising (|arity|), inlining (|inline|) and specialisation (|specialise|). Our goal is to combine these steps to remove as many functional values as possible. For example, the initial |incList| example requires simplification, arity raising and specialisation.

We have implemented our steps in a monadic framework to deal with issues such as obtaining unique free variables and tracking termination constraints. But to simplify the presentation, we have ignored these issues -- they are mostly tedious engineering concerns, and do not effect the underlying algorithm.

Our method is written as:

\begin{code}
firstify = simplify +|+ arity +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operator, defined as follows.

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g = fix (g . fix f)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x' then x else fix f x'
    where x' = f x
\end{code}

The expression |f +||+ g| applies |f| to an input until it reaches a fixed point, then applies |g|. If |g| changes the value, then |f| is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Joint fixpoint] After the operation has completed, applying either |f| or |g| does not change the value.

\begin{code}
propFix f g x = let r = (+|+) f g x in (f r == r) && (g r == r)
\end{code}

\item[Overall idempotence] The operation as a whole achieves is idempotent.

\begin{code}
propIdempotent f g x = let op = f +|+ g in op (op x) == op x
\end{code}

\item[Function ordering] The function |f| reaches a fixed point before the function |g| is applied. If a postcondition of |f| implies a precondition of |g|, then we can guarantee |g|'s precondition is always met.
\end{description}

These properties allow us to separate the separate the individual transformations from the overall application strategy. The first two properties ensure that the method terminates only once no transformation is applicable. The function ordering allows us to overlap the application sites of two stages, but prefer one stage over another.

The |(+||+)| operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((simplify +|+ arity) +|+ inline) +|+ specialise
\end{code}

Within this chain we can guarantee that the end result will be a fixed point with any of the transformations. Additionally, before each transformation is applied, those to the left will have reached fixed points.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent if it is the result of |(+||+)|, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited.

We describe each of the stages in the algorithm separately. In all subsequent stages, we assume that all the simplification rules have been applied.


\subsection{Simplification}

\begin{figure}
\begin{simplify}

\simp{app-app}{
\ignore\begin{code}
(x xs_) ys_
    => x xs_ ys_
\end{code}}

\simp{fun-app}{
\ignore\begin{code}
(f xs_) ys_
    => f xs_ ys_
\end{code}}

\simp{lam-app}{
\ignore\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\simp{let-app}{
\ignore\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\simp{case-app}{
\ignore\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\simp{case-con}{
\ignore\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\simp{case-let}{
\ignore\begin{code}
case (let v = x in y) of alts_
    => let v = x in (case y of alts_)
\end{code}}

\simp{case-case}{
\ignore\begin{code}
case (case x of {... ; c vs_ -> y ; ...}) of alts_
    => case x of {... ; c vs_ -> case y of alts_ ; ...}
\end{code}}

\simp{case-lam}{
\ignore\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \z -> case x of {... ; c vs_ -> (\v -> y) z ; ...} z
\end{code}}

\simp{eta}{
\ignore\begin{code}
f xs_
    => \v -> f xs_ v
    where arity f > length xs_
\end{code}}

\end{simplify}
\caption{Standard Core simplification rules.}
\label{fig:simplify}
\end{figure}


\begin{figure}
\begin{simplify}

\simp{bind-lam}{
\ignore\begin{code}
let v = (\w -> x) in y
    => y[v / \w -> x]
\end{code}}

\simp{bind-box}{
\ignore\begin{code}
let v = x in y
    => y[v / x]
    where x {-" \text{ is a boxed lambda (see \S\ref{sec:inlining})} "-}
\end{code}}

\simp{let-lam}{
\ignore\begin{code}
let v = x in \w -> y
    => \w -> let v = x in y
\end{code}}

\end{simplify}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The simplification stage has the goal of moving lambda expressions upwards, and introducing lambdas for partially applied functions. This stage makes use of standard simplification rules given in Figure \ref{fig:simplify}, which are found in most simplifiers, such as GHC \cite{spj:transformation}. We also make use of additional rules which deal specifically with lambda expressions, given in Figure \ref{fig:lambda_simplify}. All of the simplification rules are correct individually. The rules are applied to any subexpression, as long as any rule matches. \todo{Confluence?}

\subsubsection{Lambda Introduction}

The (eta) rule inserts lambdas where possible, using $\eta$-expansion. If the input program has been generated by the Yhc compiler it has been lambda lifted \cite{lambda_lift}, replacing lambda expressions with partial application. This rule removes all occurrences of partial-application, replacing them with explicit lambda expressions. The simplification rules move these lambda expressions around, attempting to lift them to the top level.

\begin{example}
\begin{code}
even = (.) not odd
\end{code}

Here the functions |(.)|, |not| and |odd| are all partially applied. Lambda expressions can be inserted to saturate these applications.

\begin{code}
even = \x -> (.) (\y -> not y) (\z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is fully-applied. This transformation enables the arity raising transformation, resulting in:

\begin{code}
even x = (.) (\y -> not y) (\z -> odd z) x
\end{code}
\end{example}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. This step trades one form of functional value for another form, but has the advantage of making functional values more explicit, and permitting arity raising.

\subsubsection{Lambda Movement}
\label{sec:sharing}

The (case-lam) rule lifts a lambda within a case alternative outside the case expression. The (bind-lam) rule inlines a lambda bound in a let expression. The (bind-box) rule will be discussed as part of the inlining stage, see \S\label{sec:inlining}. The (let-lam) rule can be responsible for a reduction in sharing:

\begin{example}
\begin{code}
f x =  let i = expensive x
       in \j -> i + j

main xs = map (f 1) xs
\end{code}

Here |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x =  \j ->  let i = expensive x
              in i + j
\end{code}

Now |expensive| is recomputed for every element in |xs|. We include this rule in our simplifier, focusing on functional value removal at the expense of sharing.
\end{example}


\subsection{Arity Raising}

\begin{definition}
The arity raising step is:

\ignore\begin{code}
function vs_ = \v -> x
    => function vs_ v = x
\end{code}
\end{definition}

Given a body which is a lambda expression, the arguments to the lambda expression can be lifted into the definition-level arguments for the function. This transformation requires the body to be a lambda expression, so additional transformations are necessary to introduce and move lambda expressions. If a function has its arity increased, fully-applied uses become partially-applied, causing the (eta) simplification rule to fire.


\subsection{Inlining}
\label{sec:inlining}

We use inlining as the first stage in the removal of functional values stored within a data value -- for example |Just (\x -> x)|. We refer to functional values inside data values as \textit{boxed lambdas}, because they are contained within a data value. If a boxed lambda is bound in a let expression, we substitute the let binding, using the (bind-box) rule from Figure \ref{fig:lambda_simplify}. We only inline a function if two conditions both hold: (1) the function's body is a boxed lambda; (2) the function application occurs within a case scrutinee.

\begin{figure}
\begin{code}
isBox (Con c xs_     )  =  any isLambda xs_ ||  any isBox xs_
isBox (Let v x y     )  =  isBox y
isBox (Case x alts_  )  =  any (isBox . rhs) alts_
isBox (Fun f xs_     )  =  isBox (body f)
isBox _                 =  False

isLambda (Lambda vs_ x)  = True
isLambda _               = False
\end{code}
\caption{The |isBox| function, to test if an expression is a boxed lambda.}
\label{fig:boxed_lambda}
\end{figure}

\begin{definition}
An expression |e| is a boxed lambda if |isBox e == True|, where |isBox| is defined as in Figure \ref{fig:boxed_lambda}.
\end{definition}

The |isBox| as presented may not terminate, but by simply keeping a list of followed functions, we can assume the result is false in any duplicate call. This modification does not change the result of any previously terminating evaluations.

\begin{example}
Recalling that |[e]| is shorthand for |(:) e []|, where |(:)| is the cons constructor, the following expressions are boxed lambdas:

\ignore\begin{code}
[\x -> x]
(Just [\x -> x])
(let y = 1 in [\x -> x])
[Nothing, Just (\x -> x)]
\end{code}

The following are \textit{not} boxed lambdas:

\ignore\begin{code}
\x -> id x
[id (\x -> x)]
id [\x -> x]
\end{code}

The final expression \textit{evaluates to} a boxed lambda, but this information is hidden by the |id| function. We assume that specialisation will deal with any boxed lambdas passed to functions.
\end{example}

\begin{definition}
The inlining transformation is specified by:

\ignore\begin{code}
case (f xs_) alts_
    => case (let args f = xs_ in body f) alts_
    where isBox (body f)
\end{code}
\end{definition}

As with the simplification stage, there may be some loss of sharing if the definition being inlined has an arity of 0, known as a constant applicative form (CAF). A Haskell implementation computes these expressions only once, and reuses their value as necessary. If they are inlined, this sharing will be lost.

\subsection{Specialisation (|specialise|)}

For each application of a function to arguments containing lambdas, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applies where function arguments are lambda expressions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have arguments containing lambdas, and generate templates, omitting first-order components (see \S\ref{sec:generate_templates}).
\item For each template, generate a function specialised to that template (see \S\ref{sec:generate_functions}).
\item For each subexpression matching a template, replace it with the generated function (see \S\ref{sec:use_templates}).
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

Specialisation first finds the application of |map| in |main|, and generates the template \ignore|map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates a unique name for the template (we choose |map_id|), and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
\label{ex:map_id}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code is first-order.
\end{example}

\todo{Not clear here}
The specialisation stage is the only one which introduces new function names. In order to recover an expression in the input language, it is sufficient to replace all generated functions with their associated template, supplying all necessary variables.

\subsubsection{Generating Templates}
\label{sec:generate_templates}

%format underscore = _

\begin{figure}
\begin{comment}
\begin{code}
underscore = undefined :: Expr
\end{code}
\end{comment}
\begin{code}
template :: Expr -> Expr
template (Fun f xs_)  = map (t []) xs_ & Fun f
template _            = underscore

t :: [String] -> Expr -> Expr
t seen y =  let t' = t seen in case y of
    Lambda vs_ x  ->  Lambda vs_ (t (vs_++seen) x)
    Fun f xs_     |   isBox y -> Fun f (map t' xs_)
    Var v         ->  if v `elem` seen then Var v else underscore
    Fun f xs_     ->  map t' xs_ & Fun f
    Con c xs_     ->  map t' xs_ & Con c
    App x xs_     ->  map t' (x:xs_) & \(x:xs_) -> App x xs_
    Let v x y     ->  [x,y] &
                      \[x,_] -> Let v x (t (v:seen) y)
    Case x alts_  ->  (t' x : map (t' . rhs) alts_) &
                      \(x:_) -> Case x (map g alts_)
        where g (Alt c vs_ x) = Alt c vs_ (f (vs_++seen) x)

(&) :: [Expr] -> ([Expr] -> Expr) -> Expr
(&) xs f = if all (== underscore) xs then underscore else f xs
\end{code}
\caption{Template generation function.}
\label{fig:template_generation}
\end{figure}

A template is generated if an expression is an application of a top-level function, whose arguments include a sub-expression which is either a lambda expression or a boxed lambda -- as calculated by the |template| function in Figure \ref{fig:template_generation}. The template additionally includes all sub-expressions whose removal would lead to functional values in |underscore| positions, and all free variables bound within the template.

\begin{example}
\noindent\begin{tabular}{ll}
Expression & Template \\
|id (\x -> x)|                & |id (\x -> x)| \\
|id (Just (\x -> x))|         & |id (Just (\x -> x))| \\
|id (\x -> let y = 12 in 4)|  & |id (\x -> underscore)| \\
|id (\x -> let y = 12 in x)|  & |id (\x -> let y = underscore in x)| \\
\end{tabular}

In all examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, there are subexpressions which do not depend on variables bound within the lambda -- these have been removed. The |Just| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}
\label{sec:generate_functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. For each |_| template a fresh argument free variable is assigned. The body is produced by unfolding the outer function symbol in the template once.

\begin{examplerevisit}{\ref{ex:map_id}}
Consider the \ignore|map (\x -> x) _|. Let |v_1| be the fresh argument variable for the single |_| placeholder, and |map_id| be the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

\noindent We unfold the definition of |map| once:

\begin{code}
map_id v_1 =  let  f   = \x -> x
                   xs  = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> f y : map f ys
\end{code}

\noindent After the simplification rules from Figure \ref{fig:lambda_simplify}, we obtain:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{examplerevisit}

\subsubsection{Using Templates}
\label{sec:use_templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an |underscore| is passed as an argument.

\begin{exampleany}{\ref{ex:map_id} (continued)}
\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We now have a first-order definition.
\end{exampleany}

\subsection{Extending the method to allow primitive functions}
\label{sec:primitives}

Primitive functions are like normal functions, but do not have an associated body, and therefore cannot be examined or inlined. We make two just simple changes to support primitives.

\begin{enumerate}
\item We define that a primitive application is \textit{not} a boxed lambda.
\item We restrict specialisation so that if a function to be specialised is actually a primitive, no template is generated. The reason for this restriction is that the generation of code associated with a template requires a one-step unfolding of the function, something which cannot occur with a primitive.
\end{enumerate}

\section{Examples}
\label{sec:examples}

We now give a series of examples. We have particularly focused on where our method leaves residual functional values, or where a small alteration to the method would harm the effectiveness.

\begin{examplename}{Dead Code}
\begin{code}
nothing = Nothing

main = case  nothing of
             Nothing  -> 1
             Just f   -> f (\x -> x)
\end{code}

In |main| the |Nothing| branch will always be taken. The |Just| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. Our method cannot remove this lambda.
\end{examplename}

\begin{examplename}{Undefined Values}
\label{ex:undefined_values}
\begin{code}
bottom = bottom

main = bottom (\y -> y)
\end{code}

In |main| a lambda is applied to |undefined|, so the program will non-terminate before the lambda is evaluated. The |bottom| function could also have been implemented as a call to the |error| primitive, with similar results.
\end{examplename}

\begin{examplename}{Root function returning functional values}
\label{ex:root_function_functional}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor, i.e. a boxed lambda. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from 0 to 1. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type \ignore|IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> One alpha)
data One alpha = One alpha
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to \ignore|One alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{examplename}

\begin{examplename}{Root function taking functional values}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are transforming, we cannot remove the functional value.
\end{examplename}

\begin{examplename}{Primitives}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type \ignore|alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Some primitives not only permit functional values, but actually \textit{require} them. The |primCatch| function within the Yhc standard libraries implements the Haskell exception handling function |catch|. The type of |primCatch| is \ignore|alpha -> (IOError -> alpha) -> alpha|, taking an exception handler as one of the arguments.
\end{examplename}

\begin{examplename}{Functional Lists}
\label{ex:functional_lists}
Sometimes lambda expressions are used to build up lists which can have elements concatenated onto the end. Using Hughes lists \cite{hughes:lists}, we can define:

\begin{code}
nil = id
snoc x xs = \ys -> xs (x:ys)
list xs = xs []
\end{code}

This list representation provides |nil| similarly to |[]| in standard lists, but instead of providing a |(:)| or ``cons'' operation, it provides |snoc| which adds a single element on to the end of the list. The function |list| is provided to create a standard list. We are unable to defunctionalise such a construction, as it stores unbounded information within closures. We have seen such constructions in both the |lines| function of the HsColour program, and the |sort| function of Yhc.

However, there is an alternative implementation of these functions:

\begin{code}
nil = []
snoc = (:)
list = reverse
\end{code}

We have benchmarked these operations in a variety of settings and the list based version appears to use approximately 75\% of the memory, and 65\% of the time. We suggest that people using continuations for |snoc| move instead to a list type.
\end{examplename}


\begin{examplename}{Inlining Boxed Lambdas}
\label{ex:inlining_boxed_lambdas}
Our original program inlined boxed lambdas everywhere they occurred. This makes the detection of boxed lambdas much simpler, and does not require looking into external functions. However, it is unable to cope with certain programs:

\begin{code}
main xs = app (gen xs)
app = map ($ 1)
gen = map (const x)
\end{code}

The function |gen| returns a boxed lambda. However, the code is recursive, so if it was inlined repeatedly, it would not terminate. After deciding to restrict the inlining of gen, we are still left with lambdas. However, by first specialising |app| with respect to |gen|, we are able to remove the functional values. The removal of the lambda expressions has effectively lead to a deforesting between the |gen| producer of |(:)| nodes, and the |app| consumer -- eliminating the intermediate functional values in the same step.
\end{examplename}



\section{Classification of Completeness}
\label{sec:completeness}

Our method would be complete if it removed all lambda expressions from a program. Completeness is not obtainable given the termination restriction (see \S\ref{sec:goals}). We have two causes of incompleteness: those that come from the fundamental algorithm, and those that are added by the termination criteria in \S\ref{sec:termination}. Here we explore causes arising from our algorithm. We wish to classify where a lambda expression may reside in a program after the application of our firstification method.

To examine where lambda expressions may occur, we model our Core expression language as a context free grammar. We define the start symbol |_s|, and require that the bodies of all functions in our language be defined by the grammar. First we map all expressions onto symbols in our grammar:

\ignore\begin{code}
\vs_ -> x         => lam x
f xs_             => fun (body f) xs_
c xs_             => con xs_
x ys_             => app x ys_
v                 => var
case x of alts_   => case_ x (map rhs alts_)
let v = x in y    => let_ x y
\end{code}

We have abstracted away variable names and patterns in expressions such as case, let and lambda. Some expressions, such as |app|, have multiple arguments. We have abstracted this away, and use \ignore|(app _a _b)| to denote that the first argument to an application is define by |_a|, and that all subsequent arguments are defined by |_b|.

In the case of function application, the first argument represents the possible expressions of the body of the function, and the second expression represents the arguments applied to the function. We compose grammars using \ignore|&+|, \ignore|&*| and \ignore|-| as grammar subtraction.

First we start out with the complete grammar:

% _ = L f C a v c l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app _s _s &+ var &+
       case_ _s _s &+ let_ _s _s
\end{code}

We know our original program is type safe. This means that the scrutinee of a case may not evaluate to a functional value, and therefore we can discard the |lam| production from the first argument of |case_|. Likewise, we know that all constructor expressions are saturated, so will evaluate to a data value, and cannot be the first argument of an application. Modifying our grammar to take account of this, we have:

% _ = L f C a(!C, _) v c(!L, _) l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app (_s - con _s) _s &+ var &+
       case_ (_s - lam _s) _s &+ let_ _s _s
\end{code}

Our simplification rules given in Figure \ref{fig:simplify} are applied until a fixed point is found, meaning that the left hand side of any rule cannot occur in the output. Accounting for these restrictions in the grammar we are left with:

% _ = L f C a(v, _) v c(f a v, _) l
\ignore\begin{code}
_s  =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
       case_ (fun _s _s &+ app var _s &+ var) _s &+ let_ _s _s
\end{code}

Next we apply the lambda rules from Figure \ref{fig:lambda_simplify}. As \ignore|(_s - lam _s)| is a common production we have factored it out as |_l'|.

% _ = L f C a(v, _) v c(f a v, !L) l(!L, !L)
\ignore\begin{code}
_l'  =  _s - lam _s
_s   =  lam _s &+ fun _s _s &+ con _s &+ app var _s &+ var &+
        case_(fun _s _s &+ app var _s &+ var) _l' &+ let_ _l' _l'
\end{code}

We can now use the results of arity raising, which ensures that every function's root may not be a lambda expression.

% _ = L f(!L, _) C a(v, _) v c(f a v, !L) l(!L, !L)
\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun _l' _e &+ app var _e &+ var) _l' &+ let_ _l' _l'
\end{code}

We now wish to work with lambda boxes, as defined by the function |isBox|, from Figure \ref{fig:boxed_lambda}. Within our grammar rules it is not possible to define that an expression is a lambda box. The first expression defined as a lambda box is a constructor where one of the arguments is a lambda, and the other arguments are unrestricted. Using our grammar, we have merged all the arguments to a constructor, and cannot define the property that a term is a lambda box in our grammar. We can however define the property that something is \textit{not} a lambda box, which we define with the production:

%
\ignore\begin{code}
_b'  =  lam _e &+ fun _b' _e &+ con (_b' - lam _e) &+ app _e _e &+ var &+
        case_ _e _b' &+ let_ _e _b'
\end{code}

Now if an expression meets |_e|, but not |_b'|, then we know it is a lambda box. We can now make use of the inlining rule and the (bind-box) rules to write:

\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' _e &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') _e &+ app var _e &+ var) _l' &+ let_ (_l' &* _b') _l'
\end{code}

Now we can make use of the specialise rule, which removes all lambdas and boxed lambdas from the arguments of function applications:

\ignore\begin{code}
_s   = _l'
_l' = _e - lam _e
_e =  lam _e &+ fun _l' (_l' &* _b') &+ con _e &+ app var _e &+ var &+
      case_ (fun (_l' &* _b') (_l' &* _b') &+ app var _e &+ var) _l' &+
      let_ (_l' &* _b') _l'
\end{code}

Having applied all the rules, we now classify what the parent expressions of a lambda may be. A lambda may not be the root of a function body, because |lam| is not a production in |_s|. If we restrict our attention only to expressions which permit a lambda expression as a direct child, and add explicit \ignore|lam _e| productions where we can, we are left with:

\ignore\begin{code}
_e =  lam (lam _e &+ _e) &+ con (lam _e &+ _e) &+ app var (lam _e &+ _e) &+
      -- productions not permitting a lambda as a direct child
\end{code}

A lambda may therefore occur as the child of a lambda expression, as an argument to a constructor, or as an argument to an application. However, a constructor containing a lambda is a lambda box, and therefore is not permitted anywhere |_b'| is intersected with the expression. We can now classify where a lambda box may occur, denoting all expressions as either |_b| to denote that a box may occur, or |_b'| to denote that it may not:

\ignore\begin{code}
lam _b &+ fun _b _b' &+ con _b &+ app _b' _b &+ case_ _b' _b &+ let _b' _b
\end{code}

Therefore a constructor containing a lambda may only occur at the root of a function (since |_s| permits boxed lambdas), or in one of the positions outlined above. Assuming no unreachable functions, either the root function of the program must evaluate to a boxed lambda, or there must be an expression which is not an unboxed lambda containing a boxed lambda. Of the above productions, most are boxed lambdas by definition, so the only non-boxed lambdas containing a boxed lambda are:

\ignore\begin{code}
lam _b &+ app _b' _b
\end{code}

Therefore the root function may be a boxed lambda, with a lambda inside the box. Or a lambda or boxed lambda may occur as inside a lambda, or as the second argument to an application. Given this situation, we can be sure that the variable does not bind to a lambda, as the lambda cannot escape. This leads us to the following situations:

\ignore\begin{code}
case v of
    Just w -> w (\x -> x)
    Nothing -> 1

bottom (\x -> x)
    where bottom = bottom
\end{code}

In the first case there must be no corresponding |Just| value containing a functional value, so the first branch must be dead code. In the second example, a non-terminating function the first argument of an application, and contains a lambda as an argument to it. Provided the |main| function is first-order, and the termination criteria have not applied, if a program has any residual lambda expressions, there must be at least one in one of these two situations.


\section{Proof of Termination}
\label{sec:termination}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.

\subsection{Termination of Simplification}
\label{sec:termination_simplification}

In order to check the termination of the simplifier we have used the AProVE system \cite{aprove}, to model our rules as term rewrites, and check termination thereon. A simple encoding of our rules is given in Appendix A, which checks basic termination. We have proven termination using both this simple formulation, which considers all constructors to have one alternative of exactly arity one, and a more complex encoding. In both cases, the system is able to report success.

The encoding of the (bind-box) and (bind-lam) rules is excluded. Given these rules, there are non terminating sequences. For example:

\ignore\begin{code}
(\x -> x x) (\x -> x x)
   => -- (lam-app) rule
let x = \x -> x x in x x
   => -- (bind-lam) rule
(\x -> x x) (\x -> x x)
\end{code}

These type of expressions are a problem for GHC, and can cause the compiler to non-terminate if encoded as data structures \cite{spj:inlining}. Other transformation systems \cite{chin:higher_order_removal} are able to make use of type annotations to ensure these reductions terminate. In practice the above situation does not occur, but in order to guarantee termination, we only perform $n$ (bind-lam) or (bind-box) rules upon an expression. If the expression is altered by other stages, we reset the count. Currently we have set $n$ to 1000, and have never had this limit reached.

\subsection{Termination of Arity Raising}

\todo{What makes arity raising terminate. Does it terminate?}

\subsection{Termination of Inlining}

One standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions, such as Example \ref{ex:inlining_boxed_lambdas}. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = case  f of
          One _ -> One (\x -> x)
\end{code}

The |f| inside the case is a candidate for inlining:

\ignore\begin{code}
case f of One _ -> One (\x -> x)
    => -- inlining rule
case (case f of One _ -> One (\x -> x)) of One _ -> One (\x -> x)
    => -- (case-case) rule
case f of One _ -> case One (\x -> x) of One _ -> One (\x -> x)
    => -- (case-con) rule
case f of One _ -> One (\x -> x)
\end{code}

This expression could cause non-termination.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. Any future attempts to inline |f| within this function would be disallowed, although |f| could still be inlined within other function bodies. This termination criteria is sufficient, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}
\label{sec:termination_specialisation}

The specialisation method, left unrestricted, does not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. Specialisation would generate an infinite number of specialisations of |f|.
\end{example}

\begin{figure}
\[\frac{s \unlhd t_i \text{ for some } |i|}{s \unlhd \sigma(t_1,\ldots,t_n)} \]

\[\frac{\sigma_1 \sim \sigma_2,
        s_1 \unlhd t_1, \ldots , s_n \unlhd t_n}
       {\sigma_1 (s_1,\ldots,s_n) \unlhd \sigma_2 (t_1,\ldots,t_n)} \]
\caption{Homeomorphic embedding relation.}
\label{fig:homeomorphic}
\end{figure}

To ensure we only specialise a finite number of times we use a homeomorphic embedding \cite{leuschel:homeomorphic}, given in Figure \ref{fig:homeomorphic}. The homeomorphic embedding has been used for termination of Supercompilation \cite{sorensen:supercompilation}, which makes use of some similar transformations. The homeomorphic embedding $\unlhd$ is a well-binary relation, meaning there are no infinite admissible sequences. A sequence $s_1,s_2 \ldots$ is admissible if there are no $i < j$ such that $s_i \unlhd s_j$. This property only holds provided all elements are expressions over a finite alphabet.

For each function, we associate a set $S$, of expressions. After generating a template $t$, we only specialise with that template if $\forall s \in S \bullet \neg(s \unlhd t)$. If $S$ is already admissible, then the sequence $S$ with $t$ added at the end is still admissible. After specialising with a template we add that template to the set $S$ associated with that expression. When we create a new function based on a template, we copy the $S$ associated with the function in which the specialisation is performed.

One of the conditions for termination of homeomorphic embedding is that there is only a finite alphabet. To ensure this condition, we consider all variables and literals to be equal. However, this is not sufficient. During the process of specialisation we create new functions, and these new functions are new symbols in our language. Because the homeomorphic embedding is being used to ensure that infinite functions are not created, we cannot assume this property when proving it. Instead we only use function names from the original input program. Every template has a correspondence with an expression in the original program. We perform the homeomorphic embedding after transforming all templates into their original equivalent expression.

Using homeomorphic embedding on the previous example, we would generate the specialised variant of |f (Value head)|. Upon attempting to generate the specialised variant |f (Wrap (Value head))| we would abort, with an embedding.

While homeomorphic embedding is sufficient to obtain defunctionalisation in most simple examples, there do exist examples where it terminates prematurely.

\begin{example}
\begin{code}
main y = f (\x -> x) y
f x y = fst (x, f x y) y
\end{code}

Here we first generate a specialised variant of |f (\x -> x) y|.  If we call the specialised variant |f'|, we have:

\begin{code}
f' y = fst (\x -> x, f' y) y
\end{code}

Note that the recursive call to |f| has also been specialised. We now attempt to generate a specialised variant of |fst|, using the template |fst (\x -> x, f' y) y|. Unfortunately, this template is an embedding of the template we used for |f'|, and we do not specialise. If we permit specialisation, we reduce to:

\begin{code}
f' y = fst' y y
fst' y_1 y_2 = y_2
\end{code}

Note that the homeomorphic embedding has prevented us for making a first-order variant of the program.
\end{example}

While the above example may look slightly obscure, it occurs commonly with the standard translation of dictionaries. Often, classes have default methods, which call other methods in the same class. These recursive class calls often pass dictionaries, embedding the original caller -- even though no recursion actually happens.

To alleviate this problem, instead of storing one set $S$, we instead store a sequence of sets, $S_1$ through $S_n$ -- where $n$ is a small positive number, constant for the duration of the program. Instead of adding to the set $S$, we now add to the lowest set $S_i$ where adding the element will not violate the admissible sequence. Each of the sets $S_i$ is still finite, and there are a finite number ($n$) of them, so termination is maintained.

By default the firstification program uses 8 as the number of sets. In the results table given in \S\ref{sec:results}, we have given the minimum possible value of $n$ to remove all lambda expressions within each program. The disadvantage of a higher $n$ is that a program will unroll infinite recursion more times.

\subsection{Termination as a Whole}

We call a function |f| finite if it only applies a finite number of times, given a starting program. We call a function |f| terminating if |fix f| terminates. Given |f +||+ g|, if both |f| and |g| are finite then |f +||+ g| is finite. If one if finite, and the other is terminating, then the entire production is terminating.

We have shown that arity raising, inlining and specialisation are finite. The remaining step, simplification, is terminating. Therefore our entire program is terminating.

\section{Results}
\label{sec:results}

\begin{table}
\caption{Results of defunctionalisation on the nofib suite.}
\label{tab:results}

\smallskip

\textbf{Name} is the name of the program;
\textbf{Bound} is the numeric bound use for the termination (see \S\ref{sec:termination_specialisation});
\textbf{HO Create} the number of lambda expressions and under-applied functions, first in the input program and then in the output program;
\textbf{HO Use} the number of application expressions and over-applied functions;
\textbf{Size} the change in the program size.

\smallskip\smallskip

\begin{tabular*}{\linewidth}{lrrrrrrlll}
\textbf{Name} & \textbf{Bound} & \multicolumn{2}{c}{\textbf{HO Create}} & \multicolumn{2}{c}{\textbf{HO Use}} & \textbf{Size} \\
\vspace{-1ex} \\
bernouilli      & 4 & 240 & 0 & 190 & 2 & -32\% \\
digits-of-e1    & 4 & 217 & 0 & 153 & 2 & -35\% \\
digits-of-e2    & 5 & 236 & 0 & 198 & 3 & -32\% \\
exp3\_8         & 4 & 232 & 0 & 154 & 2 & -39\% \\
gen\_regexps    & 7 & 116 & 0 &  69 & 0 & -31\% \\
integrate       & 4 & 348 & 0 & 358 & 2 & -38\% \\
paraffins       & 4 & 360 & 0 & 351 & 2 & -53\% \\
primes          & 4 & 217 & 0 & 148 & 2 & -38\% \\
queens          & 4 & 217 & 0 & 146 & 2 & -38\% \\
rfib            & 4 & 338 & 0 & 355 & 2 & -31\% \\
tak             & 4 & 212 & 0 & 145 & 4 & -40\% \\
wheel-sieve1    & 4 & 224 & 0 & 151 & 2 & -35\% \\
wheel-sieve2    & 4 & 224 & 0 & 162 & 2 & -36\% \\
x2n1            & 4 & 345 & 0 & 385 & 2 & -57\% \\
\multicolumn{8}{l} \ldots{}plus 35 tests from the spectral suite\ldots{} \\
Minimum         & 2 & 60 & 0 & 46 & 0 & -78\% \\
Maximum         & 14 & 437 & 1 & 449 & 100 & 15\% \\
Average         & 4.8 & 237 & 0.06 & 202 & 3.6 & -33\% \\
\hline
\end{tabular*}
\end{table}

We have tested out method with programs drawn from the nofib benchmark \cite{nofib}, and the results are given in Table \ref{tab:results}. We have tested with all 14 tests from the imaginary section (each shown in the table), and 35 out of the 47 tests in the spectral section. The remaining 12 tests in the spectral section do not compile using the Yhc compiler, mainly due to missing or incomplete libraries.

Most analysis operations work on an abstracted view of the program. For example, |readFile| in Yhc is implemented in terms of file handles and pointer operations -- while an analysis tool would probably give an abstraction for the |readFile| function. For the purposes of defunctionalisation, we have worked on unmodified Yhc libraries, including all the low-level detail. Higher-order functions occur in many places, including:

\begin{itemize}
\item Type classes create dictionaries which are implemented as tuples of functions.
\item The monadic bind operation is higher-order.
\item The IO data type is implemented as a function.
\item The Haskell |Show| type class uses continuation passing style extensively.
\item List comprehensions in Yhc are desugared to continuations. There are other translations which require less functional value manipulations \cite{wadler:list_comprehensions,coutts:stream_fusion}.
\end{itemize}

One torture test we have used during testing is \ignore|show (1 :: Double)|. The underlying implementation of |show| on |Double| makes use of arrays, which are implemented within the |IO| monad. Out of the tests, only 3 programs have residual functional values within them.

The integer program passes functional values to the primitive |seq|, similar to Example \ref{ex:functional_lists}, using the following function:

\begin{code}
seqlist [] = return ()
seqlist (x:xs) = x `seq` seqlist xs
\end{code}

This function is invoked with the IO monad, so the |return ()| expression is a functional value. It is impossible to remove this functional value without having access to the implementation of the |seq| primitive.

The pretty and constraints programs both apply a functional value to an expression that evaluates to |undefined|, similar to Example \ref{ex:undefined_values}. The case in pretty comes from the fragment:

\begin{comment}
% for paper haskell
\begin{code}
data PrettyRep = PrettyRep
\end{code}
\end{comment}
\begin{code}
type Pretty = Int -> Bool -> PrettyRep

ppBesides     :: [Pretty] -> Pretty
ppBesides = foldr1 f nil
\end{code}

Here |ppBesides| may evaluate to |undefined| if the input list to |foldr1| is |[]|. The |undefined| value will be of type |Pretty|, and will be given further arguments, which can be functions. In reality, the code ensures that the input list is never |[]|, so the program will never fail with this error.

While very few programs have functional values, a substantial number make use of applications to a non-function/non-constructor, and use over-application of functions. In most cases these result from supplying |error| calls with additional arguments, typically related to the desugaring of |do| notation and pattern matching within Yhc. We do not consider the use of a small number of higher-order applications to be a problem.

The termination bound used on the programs varies from 2 to 11 within the sample programs. If we exclude the integer program, which is complicated by the primitive operations on functional values, the highest bound is 8. Most programs have a termination bound of 4. There is no relation to the size of a program and the termination bound.

On average the size of the resultant program is smaller by 33\%. We measured program size by taking the number of nodes in the abstract syntax tree, and the number of functions. We took the average of the two measures, which were typically very close to each other. Before counting the size we remove all functions which simply call on to other functions passing the same arguments, for example:

\begin{code}
f x y = g x y
\end{code}

Given this, we would replace all invocations of |f| with |g|. The reason for the decrease in program size is due to dictionaries holding references to unnecessary code.

\section{Related Work}
\label{sec:related}

Reynolds style defunctionalisation \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Reynolds method works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\ignore\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f y : map f ys
\end{code}

\noindent Now all calls to |map head| are replaced by \ignore|map Head|.
\end{example}

Reynold's method works on all programs, defunctionalised code is still type safe, but type checking would require a dependently typed language. Since the original work, people have proposed variants of Reynold's method that are type safe in the simply typed lambda calculus \cite{bell:type_driven_defunctionalization}, and within a polymorphic type system \cite{pottier:polymorhpic_typed_defunctionaization}.

We are unaware of any simple method for extending Reynold's style defunctionalisation to primitives, without changes the primitives to be aware of the \ignore|Function| data type. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Reynold's method has been used as a tool in program calculation \cite{danvy:defunctionalization_at_work,hutton:calculating_an_exceptional_machine}, often as a mechanism for removing introduced continuations. Another use of Reynold's method is for optimisation \cite{grin,jhc}, allowing flow control information to be recovered without the complexity of higher-order transformation.

The closest work to ours is \citet{chin:higher_order_removal}, which itself is similar to that of Nelan \cite{nelan:firstification}. They define a higher-order removal method, with similar goals of removing functional values from a program. Their work shares some of the simplification rules, the arity raising and function specialisation. Despite these commonalities, there are great differences in how their method is structured.

\begin{itemize}
\item Their method makes use of the types of expressions, information that must be maintained and extended to work with additional type systems.
\item Their method does not have an inline step, or any notion of boxed lambdas. Functional values within constructors are ignored, and the authors suggest the use of deforestation \cite{wadler:deforestation} to help remove them. Deforestation is not tailored to this usage, and therefore transforms the program more than necessary, and fails to eliminate many functional values.
\item Their specialisation step does not work with boxed lambdas to functional values.
\item To ensure termination of the specialisation step, they never specialise a recursive function which does have any functional arguments passed identically in all recursive calls. This restriction satisfies higher-order functions such as |map|, but fails in many other cases.
\item By splitting our the simplification rules, our other stages are comparatively simple to theirs.
\end{itemize}

The limitations of their work are entirely understandable, given the nature of programming at the time their work was written. While nowadays people use monads, IO continuations and type classes as a matter of course, these features were still experimental when their work was done. Our work can be seen as a successor to theirs, achieving similar goals but with a greater complexity of input program. Our work achieves most of the aims set out in their future work section.

We have tried their examples, and can confirm that all of them are successfully handled by our system. None of the nofib programs could be reduced by their transformations. Some of their observations and extensions apply equally to our work, for example they suggest possible methods of removing accumulating functions such as in Example \ref{ex:functional_lists}.

The specialisation and inlining steps are taken from existing program optimisers, as is the termination strategy of homeomorphic embedding. A lot of program optimisers remove some higher-order functions, such as partial evaluation \cite{jones:partial_evaluation} and supercompilation \cite{supercompilation}. We have certainly benefited from ideas in both these areas in developing our algorithms. Our initial attempt at removing functional values involved modifying a supercompiler \cite{me:supero}. Unfortunately this approach has a number of disadvantages. Firstly, the optimiser is not attempting to preserve correspondence to the original program, so will optimise all aspects of the program equally, instead of focusing on the higher-order elements. The other more serious problem was that the results were poor -- given the restricted setting of only removing functional values, the techniques can be pushed much further.

\section{Conclusions and Future Work}
\label{sec:conclusion}

Higher order functions are very useful to the programmer, but may pose difficulties for certain types of analysis. Using the method we have described, it is possible to remove most functional values from most programs. Our method has already found practical use within the Catch tool, and we hope it can be of benefit to others.

The implementation presented in this paper is designed for clarity, not speed. By simply tracking which functions have changed, a massive performance could be obtained. The use of a numeric termination bound in the homeomorphic embedding is regrettable, but practically motivated. We need further research to determine if such a numeric bound is necessary, or if other measures could be used, and what the bound should be.

Many analysis methods, in fields such as strictness analysis and termination analysis, start out first-order and are gradually extended to work in a higher-order language. Instead of modifying each analysis method, we hope that instead we can transform the functional values away, enabling more analysis methods to work on a greater range of programs.

\section*{Appendix A}

The following script can be supplied to the AProVe system \cite{aprove} to check termination of our simplification rules, as described in \S\ref{sec:termination_simplification}.

\begin{verbatim}
[x,y,z]
app(lam(x),y)    -> let(y,x)
app(case(x,y),z) -> case(x,app(y,z))
app(let(x,y),z)  -> let(x,app(y,z))
case(let(x,y),z) -> let(x,case(y,z))
case(con(x),y)   -> let(x,y)
case(x,lam(y))   -> lam(case(x,app(lam(y),var)))
let(lam(x),y)    -> lam(let(x,y))
\end{verbatim}


\bibliographystyle{plainnat}

\bibliography



\end{document}
